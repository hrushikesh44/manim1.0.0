{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 13500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 6.225846767425537,
      "learning_rate": 0.0001999111111111111,
      "loss": 10.1395,
      "step": 10
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 4.225636959075928,
      "learning_rate": 0.00019976296296296298,
      "loss": 9.1283,
      "step": 20
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.4864975214004517,
      "learning_rate": 0.00019961481481481483,
      "loss": 7.7224,
      "step": 30
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 1.652529239654541,
      "learning_rate": 0.00019946666666666667,
      "loss": 6.8205,
      "step": 40
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 2.579355239868164,
      "learning_rate": 0.00019931851851851852,
      "loss": 6.163,
      "step": 50
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.8612302541732788,
      "learning_rate": 0.0001991703703703704,
      "loss": 5.7051,
      "step": 60
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 1.4752705097198486,
      "learning_rate": 0.0001990222222222222,
      "loss": 5.4,
      "step": 70
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.364428162574768,
      "learning_rate": 0.00019887407407407408,
      "loss": 5.2438,
      "step": 80
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.771734893321991,
      "learning_rate": 0.00019872592592592593,
      "loss": 5.0789,
      "step": 90
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.6452211737632751,
      "learning_rate": 0.0001985777777777778,
      "loss": 5.0787,
      "step": 100
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 1.1411465406417847,
      "learning_rate": 0.00019842962962962962,
      "loss": 5.0332,
      "step": 110
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.6597524285316467,
      "learning_rate": 0.0001982814814814815,
      "loss": 4.97,
      "step": 120
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 0.48184919357299805,
      "learning_rate": 0.00019813333333333334,
      "loss": 4.9535,
      "step": 130
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 1.1146373748779297,
      "learning_rate": 0.0001979851851851852,
      "loss": 4.9555,
      "step": 140
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3618299663066864,
      "learning_rate": 0.00019783703703703704,
      "loss": 4.9256,
      "step": 150
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.40859565138816833,
      "learning_rate": 0.0001976888888888889,
      "loss": 4.9245,
      "step": 160
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 0.40305325388908386,
      "learning_rate": 0.00019754074074074075,
      "loss": 4.9214,
      "step": 170
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.3685086667537689,
      "learning_rate": 0.0001973925925925926,
      "loss": 4.8996,
      "step": 180
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 0.4647376835346222,
      "learning_rate": 0.00019724444444444445,
      "loss": 4.9315,
      "step": 190
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.4640187621116638,
      "learning_rate": 0.00019709629629629632,
      "loss": 4.914,
      "step": 200
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.4320927560329437,
      "learning_rate": 0.00019694814814814814,
      "loss": 4.9306,
      "step": 210
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 0.2586047053337097,
      "learning_rate": 0.0001968,
      "loss": 4.8955,
      "step": 220
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 0.37456658482551575,
      "learning_rate": 0.00019665185185185186,
      "loss": 4.9392,
      "step": 230
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.23325705528259277,
      "learning_rate": 0.00019650370370370373,
      "loss": 4.8518,
      "step": 240
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.5212029218673706,
      "learning_rate": 0.00019635555555555555,
      "loss": 4.8612,
      "step": 250
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 0.6987076997756958,
      "learning_rate": 0.00019620740740740743,
      "loss": 4.8812,
      "step": 260
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.39385896921157837,
      "learning_rate": 0.00019605925925925927,
      "loss": 4.8125,
      "step": 270
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 0.1816035658121109,
      "learning_rate": 0.00019591111111111112,
      "loss": 4.8842,
      "step": 280
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 0.22836747765541077,
      "learning_rate": 0.00019576296296296296,
      "loss": 4.8934,
      "step": 290
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.37063872814178467,
      "learning_rate": 0.00019561481481481484,
      "loss": 4.8959,
      "step": 300
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 0.26113709807395935,
      "learning_rate": 0.00019546666666666668,
      "loss": 4.8563,
      "step": 310
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.4359486699104309,
      "learning_rate": 0.00019531851851851853,
      "loss": 4.8754,
      "step": 320
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.35568204522132874,
      "learning_rate": 0.00019517037037037038,
      "loss": 4.8734,
      "step": 330
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 0.41173622012138367,
      "learning_rate": 0.00019502222222222225,
      "loss": 4.8785,
      "step": 340
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 0.5408313870429993,
      "learning_rate": 0.00019487407407407407,
      "loss": 4.8375,
      "step": 350
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.7096452116966248,
      "learning_rate": 0.00019472592592592594,
      "loss": 4.8731,
      "step": 360
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 0.5935046076774597,
      "learning_rate": 0.0001945777777777778,
      "loss": 4.8385,
      "step": 370
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 0.2911219298839569,
      "learning_rate": 0.00019442962962962963,
      "loss": 4.8834,
      "step": 380
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.39248591661453247,
      "learning_rate": 0.00019428148148148148,
      "loss": 4.9184,
      "step": 390
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.3067822754383087,
      "learning_rate": 0.00019413333333333335,
      "loss": 4.8557,
      "step": 400
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 0.2045900970697403,
      "learning_rate": 0.0001939851851851852,
      "loss": 4.8343,
      "step": 410
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.22982415556907654,
      "learning_rate": 0.00019383703703703705,
      "loss": 4.8365,
      "step": 420
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 0.15578646957874298,
      "learning_rate": 0.0001936888888888889,
      "loss": 4.8555,
      "step": 430
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.2889108657836914,
      "learning_rate": 0.00019354074074074077,
      "loss": 4.8753,
      "step": 440
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.14415167272090912,
      "learning_rate": 0.00019339259259259259,
      "loss": 4.8203,
      "step": 450
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 0.35246098041534424,
      "learning_rate": 0.00019324444444444446,
      "loss": 4.8741,
      "step": 460
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 0.49699148535728455,
      "learning_rate": 0.0001930962962962963,
      "loss": 4.8456,
      "step": 470
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.18623782694339752,
      "learning_rate": 0.00019294814814814818,
      "loss": 4.8145,
      "step": 480
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 0.1282183676958084,
      "learning_rate": 0.0001928,
      "loss": 4.8736,
      "step": 490
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.12000176310539246,
      "learning_rate": 0.00019265185185185187,
      "loss": 4.838,
      "step": 500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2719639241695404,
      "learning_rate": 0.00019250370370370372,
      "loss": 4.8135,
      "step": 510
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 0.1470160186290741,
      "learning_rate": 0.00019235555555555556,
      "loss": 4.8576,
      "step": 520
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 0.3521769940853119,
      "learning_rate": 0.0001922074074074074,
      "loss": 4.8298,
      "step": 530
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.261871874332428,
      "learning_rate": 0.00019205925925925928,
      "loss": 4.8592,
      "step": 540
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 0.20727117359638214,
      "learning_rate": 0.00019191111111111113,
      "loss": 4.8603,
      "step": 550
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.5540192127227783,
      "learning_rate": 0.00019176296296296298,
      "loss": 4.8674,
      "step": 560
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.14969080686569214,
      "learning_rate": 0.00019161481481481482,
      "loss": 4.8552,
      "step": 570
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 0.2017495036125183,
      "learning_rate": 0.0001914666666666667,
      "loss": 4.8381,
      "step": 580
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 0.23540093004703522,
      "learning_rate": 0.00019131851851851851,
      "loss": 4.8453,
      "step": 590
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.11461997032165527,
      "learning_rate": 0.0001911703703703704,
      "loss": 4.8461,
      "step": 600
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 0.4314859211444855,
      "learning_rate": 0.00019102222222222223,
      "loss": 4.8552,
      "step": 610
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 0.2765265107154846,
      "learning_rate": 0.0001908740740740741,
      "loss": 4.8576,
      "step": 620
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.19034814834594727,
      "learning_rate": 0.00019072592592592593,
      "loss": 4.8385,
      "step": 630
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": NaN,
      "learning_rate": 0.0001905777777777778,
      "loss": 4.9411,
      "step": 640
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.1758766621351242,
      "learning_rate": 0.00019044444444444444,
      "loss": 4.8719,
      "step": 650
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.22520118951797485,
      "learning_rate": 0.00019029629629629632,
      "loss": 4.8389,
      "step": 660
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 0.2431105524301529,
      "learning_rate": 0.00019014814814814816,
      "loss": 4.879,
      "step": 670
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.09286356717348099,
      "learning_rate": 0.00019,
      "loss": 4.8446,
      "step": 680
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.15588346123695374,
      "learning_rate": 0.00018985185185185186,
      "loss": 4.885,
      "step": 690
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.0809885561466217,
      "learning_rate": 0.00018970370370370373,
      "loss": 4.909,
      "step": 700
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 0.1147589385509491,
      "learning_rate": 0.00018955555555555558,
      "loss": 4.9048,
      "step": 710
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.089451864361763,
      "learning_rate": 0.00018940740740740742,
      "loss": 4.8481,
      "step": 720
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 0.1127135157585144,
      "learning_rate": 0.00018925925925925927,
      "loss": 4.8291,
      "step": 730
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 0.2691830098628998,
      "learning_rate": 0.00018911111111111112,
      "loss": 4.8429,
      "step": 740
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.6335787177085876,
      "learning_rate": 0.00018896296296296296,
      "loss": 4.7833,
      "step": 750
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.07316829264163971,
      "learning_rate": 0.00018881481481481483,
      "loss": 4.8824,
      "step": 760
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 0.42868587374687195,
      "learning_rate": 0.00018866666666666668,
      "loss": 4.8487,
      "step": 770
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.23224855959415436,
      "learning_rate": 0.00018851851851851853,
      "loss": 4.823,
      "step": 780
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 0.26916834712028503,
      "learning_rate": 0.00018837037037037037,
      "loss": 4.8186,
      "step": 790
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.1982819139957428,
      "learning_rate": 0.00018822222222222222,
      "loss": 4.8339,
      "step": 800
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.15245532989501953,
      "learning_rate": 0.0001880740740740741,
      "loss": 4.8191,
      "step": 810
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.36395251750946045,
      "learning_rate": 0.00018792592592592594,
      "loss": 4.8373,
      "step": 820
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 0.3229612708091736,
      "learning_rate": 0.00018777777777777779,
      "loss": 4.8741,
      "step": 830
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.11741460859775543,
      "learning_rate": 0.00018762962962962963,
      "loss": 4.8519,
      "step": 840
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.12124843895435333,
      "learning_rate": 0.00018748148148148148,
      "loss": 4.8212,
      "step": 850
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 0.0715753436088562,
      "learning_rate": 0.00018733333333333335,
      "loss": 4.8651,
      "step": 860
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.2490977793931961,
      "learning_rate": 0.0001871851851851852,
      "loss": 4.84,
      "step": 870
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 0.043998729437589645,
      "learning_rate": 0.00018703703703703704,
      "loss": 4.8428,
      "step": 880
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 0.20516645908355713,
      "learning_rate": 0.0001868888888888889,
      "loss": 4.8314,
      "step": 890
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2136959433555603,
      "learning_rate": 0.00018674074074074074,
      "loss": 4.819,
      "step": 900
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 0.10561199486255646,
      "learning_rate": 0.0001865925925925926,
      "loss": 4.8507,
      "step": 910
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.35473087430000305,
      "learning_rate": 0.00018644444444444446,
      "loss": 4.8739,
      "step": 920
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.1472129076719284,
      "learning_rate": 0.0001862962962962963,
      "loss": 4.8269,
      "step": 930
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 0.4117239713668823,
      "learning_rate": 0.00018614814814814815,
      "loss": 4.8498,
      "step": 940
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 0.05495855584740639,
      "learning_rate": 0.00018600000000000002,
      "loss": 4.8102,
      "step": 950
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06877665221691132,
      "learning_rate": 0.00018585185185185187,
      "loss": 4.8637,
      "step": 960
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 0.16302725672721863,
      "learning_rate": 0.00018570370370370371,
      "loss": 4.865,
      "step": 970
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.2345418632030487,
      "learning_rate": 0.00018555555555555556,
      "loss": 4.8285,
      "step": 980
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.1729120910167694,
      "learning_rate": 0.0001854074074074074,
      "loss": 4.8293,
      "step": 990
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.11146701872348785,
      "learning_rate": 0.00018525925925925925,
      "loss": 4.8355,
      "step": 1000
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 0.2214905321598053,
      "learning_rate": 0.00018511111111111113,
      "loss": 4.8148,
      "step": 1010
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.1841714233160019,
      "learning_rate": 0.00018496296296296297,
      "loss": 4.8191,
      "step": 1020
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 0.10226409882307053,
      "learning_rate": 0.00018481481481481482,
      "loss": 4.816,
      "step": 1030
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.17372289299964905,
      "learning_rate": 0.00018466666666666666,
      "loss": 4.8827,
      "step": 1040
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 3.3862955570220947,
      "learning_rate": 0.00018451851851851854,
      "loss": 4.8101,
      "step": 1050
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 0.16539266705513,
      "learning_rate": 0.00018437037037037038,
      "loss": 4.7907,
      "step": 1060
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 0.08927718549966812,
      "learning_rate": 0.00018422222222222223,
      "loss": 4.8297,
      "step": 1070
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.29183894395828247,
      "learning_rate": 0.00018407407407407408,
      "loss": 4.8969,
      "step": 1080
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 0.16662229597568512,
      "learning_rate": 0.00018392592592592592,
      "loss": 4.8217,
      "step": 1090
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.3479580283164978,
      "learning_rate": 0.00018377777777777777,
      "loss": 4.8299,
      "step": 1100
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.10477970540523529,
      "learning_rate": 0.00018362962962962964,
      "loss": 4.8605,
      "step": 1110
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.11046769469976425,
      "learning_rate": 0.0001834814814814815,
      "loss": 4.7804,
      "step": 1120
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 0.06127776950597763,
      "learning_rate": 0.00018333333333333334,
      "loss": 4.8645,
      "step": 1130
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.09219725430011749,
      "learning_rate": 0.00018318518518518518,
      "loss": 4.8399,
      "step": 1140
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.10261193662881851,
      "learning_rate": 0.00018303703703703705,
      "loss": 4.8078,
      "step": 1150
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.22127504646778107,
      "learning_rate": 0.00018288888888888887,
      "loss": 4.8222,
      "step": 1160
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.07819434255361557,
      "learning_rate": 0.00018274074074074075,
      "loss": 4.8256,
      "step": 1170
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.2039748579263687,
      "learning_rate": 0.0001825925925925926,
      "loss": 4.7943,
      "step": 1180
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 0.7903993725776672,
      "learning_rate": 0.00018244444444444447,
      "loss": 4.8048,
      "step": 1190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.0804237127304077,
      "learning_rate": 0.00018229629629629629,
      "loss": 4.8742,
      "step": 1200
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 0.07221407443284988,
      "learning_rate": 0.00018214814814814816,
      "loss": 4.8183,
      "step": 1210
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 0.20203007757663727,
      "learning_rate": 0.000182,
      "loss": 4.8093,
      "step": 1220
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.10640046745538712,
      "learning_rate": 0.00018185185185185185,
      "loss": 4.8083,
      "step": 1230
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.08235148340463638,
      "learning_rate": 0.0001817037037037037,
      "loss": 4.8277,
      "step": 1240
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.08674979954957962,
      "learning_rate": 0.00018155555555555557,
      "loss": 4.7807,
      "step": 1250
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.07111555337905884,
      "learning_rate": 0.00018140740740740742,
      "loss": 4.8097,
      "step": 1260
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 0.03507561236619949,
      "learning_rate": 0.00018125925925925926,
      "loss": 4.7956,
      "step": 1270
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.05117575079202652,
      "learning_rate": 0.0001811111111111111,
      "loss": 4.8366,
      "step": 1280
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.09060432016849518,
      "learning_rate": 0.00018096296296296298,
      "loss": 4.7954,
      "step": 1290
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.1350524127483368,
      "learning_rate": 0.0001808148148148148,
      "loss": 4.8007,
      "step": 1300
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 0.3557530641555786,
      "learning_rate": 0.00018066666666666668,
      "loss": 4.8659,
      "step": 1310
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.08523300290107727,
      "learning_rate": 0.00018051851851851852,
      "loss": 4.8382,
      "step": 1320
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 0.07186221331357956,
      "learning_rate": 0.0001803703703703704,
      "loss": 4.7904,
      "step": 1330
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 0.03733265399932861,
      "learning_rate": 0.00018022222222222221,
      "loss": 4.8151,
      "step": 1340
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.06911791861057281,
      "learning_rate": 0.0001800740740740741,
      "loss": 4.874,
      "step": 1350
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 3.320232629776001,
      "learning_rate": 0.00017992592592592593,
      "loss": 4.8332,
      "step": 1360
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 0.2257617563009262,
      "learning_rate": 0.00017977777777777778,
      "loss": 4.8269,
      "step": 1370
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.04689428210258484,
      "learning_rate": 0.00017962962962962963,
      "loss": 4.8531,
      "step": 1380
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 0.30755850672721863,
      "learning_rate": 0.0001794814814814815,
      "loss": 4.8152,
      "step": 1390
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.0688280239701271,
      "learning_rate": 0.00017933333333333332,
      "loss": 4.7822,
      "step": 1400
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.07995197176933289,
      "learning_rate": 0.0001791851851851852,
      "loss": 4.7778,
      "step": 1410
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 0.32423603534698486,
      "learning_rate": 0.00017903703703703704,
      "loss": 4.7847,
      "step": 1420
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 0.08564295619726181,
      "learning_rate": 0.0001788888888888889,
      "loss": 4.791,
      "step": 1430
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.06000882387161255,
      "learning_rate": 0.00017874074074074073,
      "loss": 4.7949,
      "step": 1440
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 15.912732124328613,
      "learning_rate": 0.00017860740740740743,
      "loss": 4.9082,
      "step": 1450
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 0.3114972710609436,
      "learning_rate": 0.00017847407407407408,
      "loss": 5.0546,
      "step": 1460
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.3143916428089142,
      "learning_rate": 0.00017832592592592595,
      "loss": 4.8511,
      "step": 1470
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 1.1049848794937134,
      "learning_rate": 0.00017817777777777777,
      "loss": 4.8039,
      "step": 1480
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 0.11288613080978394,
      "learning_rate": 0.00017802962962962964,
      "loss": 4.7715,
      "step": 1490
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.037048663944005966,
      "learning_rate": 0.0001778814814814815,
      "loss": 4.8199,
      "step": 1500
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 0.07504619657993317,
      "learning_rate": 0.00017773333333333336,
      "loss": 4.8672,
      "step": 1510
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.12516652047634125,
      "learning_rate": 0.00017758518518518518,
      "loss": 4.7971,
      "step": 1520
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07029396295547485,
      "learning_rate": 0.00017743703703703705,
      "loss": 4.8247,
      "step": 1530
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 0.03191877156496048,
      "learning_rate": 0.0001772888888888889,
      "loss": 4.8328,
      "step": 1540
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.1010226309299469,
      "learning_rate": 0.00017714074074074075,
      "loss": 4.7859,
      "step": 1550
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.33741921186447144,
      "learning_rate": 0.0001769925925925926,
      "loss": 4.8315,
      "step": 1560
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 0.21538840234279633,
      "learning_rate": 0.00017684444444444447,
      "loss": 4.8381,
      "step": 1570
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 0.035015348345041275,
      "learning_rate": 0.0001766962962962963,
      "loss": 4.7954,
      "step": 1580
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.3413379192352295,
      "learning_rate": 0.00017654814814814816,
      "loss": 4.8276,
      "step": 1590
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.09113043546676636,
      "learning_rate": 0.0001764,
      "loss": 4.8392,
      "step": 1600
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 0.201874241232872,
      "learning_rate": 0.00017625185185185188,
      "loss": 4.7693,
      "step": 1610
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.05491882562637329,
      "learning_rate": 0.0001761037037037037,
      "loss": 4.8264,
      "step": 1620
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 0.05842265486717224,
      "learning_rate": 0.00017595555555555557,
      "loss": 4.7852,
      "step": 1630
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.04852772504091263,
      "learning_rate": 0.00017580740740740742,
      "loss": 4.8419,
      "step": 1640
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.054340530186891556,
      "learning_rate": 0.00017565925925925926,
      "loss": 4.8349,
      "step": 1650
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 0.264435350894928,
      "learning_rate": 0.0001755111111111111,
      "loss": 4.7926,
      "step": 1660
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 0.12340620160102844,
      "learning_rate": 0.00017536296296296298,
      "loss": 4.817,
      "step": 1670
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.0568724200129509,
      "learning_rate": 0.00017521481481481483,
      "loss": 4.7946,
      "step": 1680
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 0.10169481486082077,
      "learning_rate": 0.00017506666666666668,
      "loss": 4.8356,
      "step": 1690
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 1.0170198678970337,
      "learning_rate": 0.00017491851851851852,
      "loss": 4.826,
      "step": 1700
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.1736062616109848,
      "learning_rate": 0.0001747703703703704,
      "loss": 4.8071,
      "step": 1710
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.06196598708629608,
      "learning_rate": 0.00017462222222222221,
      "loss": 4.743,
      "step": 1720
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 0.04486741125583649,
      "learning_rate": 0.0001744740740740741,
      "loss": 4.8424,
      "step": 1730
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.045939262956380844,
      "learning_rate": 0.00017432592592592593,
      "loss": 4.8313,
      "step": 1740
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.027429169043898582,
      "learning_rate": 0.0001741777777777778,
      "loss": 4.7999,
      "step": 1750
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.04754316061735153,
      "learning_rate": 0.00017402962962962963,
      "loss": 4.8012,
      "step": 1760
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.1826382726430893,
      "learning_rate": 0.0001738814814814815,
      "loss": 4.7772,
      "step": 1770
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 0.03608357161283493,
      "learning_rate": 0.00017373333333333335,
      "loss": 4.8255,
      "step": 1780
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 0.0551888681948185,
      "learning_rate": 0.0001735851851851852,
      "loss": 4.8024,
      "step": 1790
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.21744665503501892,
      "learning_rate": 0.00017343703703703704,
      "loss": 4.7796,
      "step": 1800
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 0.036127861589193344,
      "learning_rate": 0.0001732888888888889,
      "loss": 4.8535,
      "step": 1810
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.035309772938489914,
      "learning_rate": 0.00017314074074074076,
      "loss": 4.8471,
      "step": 1820
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.09789478778839111,
      "learning_rate": 0.0001729925925925926,
      "loss": 4.8584,
      "step": 1830
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.08884906023740768,
      "learning_rate": 0.00017284444444444445,
      "loss": 4.7995,
      "step": 1840
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.02342410758137703,
      "learning_rate": 0.00017269629629629632,
      "loss": 4.7509,
      "step": 1850
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.11575110256671906,
      "learning_rate": 0.00017254814814814814,
      "loss": 4.8008,
      "step": 1860
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 0.09495913237333298,
      "learning_rate": 0.00017240000000000002,
      "loss": 4.7875,
      "step": 1870
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.06455512344837189,
      "learning_rate": 0.00017225185185185186,
      "loss": 4.8014,
      "step": 1880
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.0464775450527668,
      "learning_rate": 0.00017210370370370374,
      "loss": 4.8193,
      "step": 1890
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.22632479667663574,
      "learning_rate": 0.00017195555555555556,
      "loss": 4.8217,
      "step": 1900
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 0.05434798076748848,
      "learning_rate": 0.00017180740740740743,
      "loss": 4.8897,
      "step": 1910
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.02603461965918541,
      "learning_rate": 0.00017165925925925927,
      "loss": 4.802,
      "step": 1920
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 0.02507615275681019,
      "learning_rate": 0.00017151111111111112,
      "loss": 4.7948,
      "step": 1930
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 0.11721260100603104,
      "learning_rate": 0.00017136296296296297,
      "loss": 4.815,
      "step": 1940
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.5435838103294373,
      "learning_rate": 0.00017121481481481484,
      "loss": 4.8385,
      "step": 1950
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.18674038350582123,
      "learning_rate": 0.00017106666666666666,
      "loss": 4.8546,
      "step": 1960
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 0.03779018297791481,
      "learning_rate": 0.00017091851851851853,
      "loss": 4.8485,
      "step": 1970
    },
    {
      "epoch": 0.44,
      "grad_norm": 34.14052200317383,
      "learning_rate": 0.00017077037037037038,
      "loss": 4.8256,
      "step": 1980
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 0.04346787557005882,
      "learning_rate": 0.00017062222222222223,
      "loss": 4.8328,
      "step": 1990
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.13076938688755035,
      "learning_rate": 0.00017047407407407407,
      "loss": 4.7986,
      "step": 2000
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.402369886636734,
      "learning_rate": 0.00017034074074074074,
      "loss": 4.8459,
      "step": 2010
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.02312617190182209,
      "learning_rate": 0.0001701925925925926,
      "loss": 4.8068,
      "step": 2020
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 0.06896031647920609,
      "learning_rate": 0.00017004444444444446,
      "loss": 4.8437,
      "step": 2030
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.07789898663759232,
      "learning_rate": 0.0001698962962962963,
      "loss": 4.821,
      "step": 2040
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.4466211497783661,
      "learning_rate": 0.00016974814814814816,
      "loss": 4.7937,
      "step": 2050
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.05345480889081955,
      "learning_rate": 0.0001696,
      "loss": 4.7825,
      "step": 2060
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.08653102070093155,
      "learning_rate": 0.00016945185185185185,
      "loss": 4.8123,
      "step": 2070
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.10900440067052841,
      "learning_rate": 0.00016930370370370372,
      "loss": 4.7588,
      "step": 2080
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 0.03177042677998543,
      "learning_rate": 0.00016915555555555557,
      "loss": 4.7803,
      "step": 2090
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.03538152575492859,
      "learning_rate": 0.00016900740740740741,
      "loss": 4.831,
      "step": 2100
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 0.03634023666381836,
      "learning_rate": 0.00016885925925925926,
      "loss": 4.8306,
      "step": 2110
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.06607925146818161,
      "learning_rate": 0.0001687111111111111,
      "loss": 4.834,
      "step": 2120
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.02397308498620987,
      "learning_rate": 0.00016856296296296298,
      "loss": 4.7839,
      "step": 2130
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 0.25402572751045227,
      "learning_rate": 0.00016841481481481483,
      "loss": 4.7887,
      "step": 2140
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.04983317852020264,
      "learning_rate": 0.00016826666666666667,
      "loss": 4.8392,
      "step": 2150
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.0893629640340805,
      "learning_rate": 0.00016811851851851852,
      "loss": 4.814,
      "step": 2160
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 0.22175899147987366,
      "learning_rate": 0.00016797037037037037,
      "loss": 4.7824,
      "step": 2170
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 0.06697045266628265,
      "learning_rate": 0.00016782222222222224,
      "loss": 4.8242,
      "step": 2180
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.057267703115940094,
      "learning_rate": 0.00016767407407407408,
      "loss": 4.8639,
      "step": 2190
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.07415208220481873,
      "learning_rate": 0.00016752592592592593,
      "loss": 4.8266,
      "step": 2200
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 0.6119957566261292,
      "learning_rate": 0.00016737777777777778,
      "loss": 4.8179,
      "step": 2210
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.2886906862258911,
      "learning_rate": 0.00016722962962962965,
      "loss": 4.7874,
      "step": 2220
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 0.0805521160364151,
      "learning_rate": 0.00016708148148148147,
      "loss": 4.8066,
      "step": 2230
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.05512950196862221,
      "learning_rate": 0.00016693333333333334,
      "loss": 4.8144,
      "step": 2240
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.05888549983501434,
      "learning_rate": 0.0001667851851851852,
      "loss": 4.815,
      "step": 2250
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.028582138940691948,
      "learning_rate": 0.00016663703703703704,
      "loss": 4.819,
      "step": 2260
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 3.8194327354431152,
      "learning_rate": 0.00016648888888888888,
      "loss": 4.8496,
      "step": 2270
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.03565702214837074,
      "learning_rate": 0.00016634074074074076,
      "loss": 4.8311,
      "step": 2280
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 0.045316241681575775,
      "learning_rate": 0.0001661925925925926,
      "loss": 4.792,
      "step": 2290
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.45526930689811707,
      "learning_rate": 0.00016604444444444445,
      "loss": 4.8741,
      "step": 2300
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.2710990607738495,
      "learning_rate": 0.0001658962962962963,
      "loss": 4.7478,
      "step": 2310
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.03241879492998123,
      "learning_rate": 0.00016574814814814817,
      "loss": 4.8018,
      "step": 2320
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 0.1902332603931427,
      "learning_rate": 0.0001656,
      "loss": 4.8263,
      "step": 2330
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.08178194612264633,
      "learning_rate": 0.00016545185185185186,
      "loss": 4.8098,
      "step": 2340
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.3877515196800232,
      "learning_rate": 0.0001653037037037037,
      "loss": 4.8623,
      "step": 2350
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.12245471030473709,
      "learning_rate": 0.00016515555555555558,
      "loss": 4.8385,
      "step": 2360
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.04057373106479645,
      "learning_rate": 0.0001650074074074074,
      "loss": 4.7685,
      "step": 2370
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 0.08279513567686081,
      "learning_rate": 0.00016485925925925927,
      "loss": 4.7981,
      "step": 2380
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 0.19693955779075623,
      "learning_rate": 0.00016471111111111112,
      "loss": 4.8019,
      "step": 2390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.04385685920715332,
      "learning_rate": 0.00016456296296296296,
      "loss": 4.8518,
      "step": 2400
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 0.05642789974808693,
      "learning_rate": 0.0001644148148148148,
      "loss": 4.8138,
      "step": 2410
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.08452783524990082,
      "learning_rate": 0.00016426666666666668,
      "loss": 4.8674,
      "step": 2420
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.020838331431150436,
      "learning_rate": 0.0001641185185185185,
      "loss": 4.816,
      "step": 2430
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.03530994802713394,
      "learning_rate": 0.00016397037037037038,
      "loss": 4.7694,
      "step": 2440
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.07283475250005722,
      "learning_rate": 0.00016382222222222222,
      "loss": 4.7976,
      "step": 2450
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.10329196602106094,
      "learning_rate": 0.0001636740740740741,
      "loss": 4.7605,
      "step": 2460
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 0.15117445588111877,
      "learning_rate": 0.00016352592592592592,
      "loss": 4.8116,
      "step": 2470
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.04708515852689743,
      "learning_rate": 0.0001633777777777778,
      "loss": 4.8072,
      "step": 2480
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.023962868377566338,
      "learning_rate": 0.00016322962962962963,
      "loss": 4.8451,
      "step": 2490
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.02796465903520584,
      "learning_rate": 0.00016308148148148148,
      "loss": 4.7772,
      "step": 2500
    },
    {
      "epoch": 0.5577777777777778,
      "grad_norm": 0.09123159199953079,
      "learning_rate": 0.00016293333333333333,
      "loss": 4.7863,
      "step": 2510
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.22173824906349182,
      "learning_rate": 0.0001627851851851852,
      "loss": 4.7899,
      "step": 2520
    },
    {
      "epoch": 0.5622222222222222,
      "grad_norm": 0.06814306974411011,
      "learning_rate": 0.00016263703703703705,
      "loss": 4.825,
      "step": 2530
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.07562677562236786,
      "learning_rate": 0.0001624888888888889,
      "loss": 4.7791,
      "step": 2540
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.039130281656980515,
      "learning_rate": 0.00016234074074074074,
      "loss": 4.7907,
      "step": 2550
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.024463443085551262,
      "learning_rate": 0.0001621925925925926,
      "loss": 4.7994,
      "step": 2560
    },
    {
      "epoch": 0.5711111111111111,
      "grad_norm": 0.025398829951882362,
      "learning_rate": 0.00016204444444444443,
      "loss": 4.8044,
      "step": 2570
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.06839799880981445,
      "learning_rate": 0.0001618962962962963,
      "loss": 4.7922,
      "step": 2580
    },
    {
      "epoch": 0.5755555555555556,
      "grad_norm": 0.46191322803497314,
      "learning_rate": 0.00016174814814814815,
      "loss": 4.8475,
      "step": 2590
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.8350647687911987,
      "learning_rate": 0.00016160000000000002,
      "loss": 4.7586,
      "step": 2600
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.16785189509391785,
      "learning_rate": 0.00016145185185185184,
      "loss": 4.7913,
      "step": 2610
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 0.08069221675395966,
      "learning_rate": 0.00016130370370370372,
      "loss": 4.7924,
      "step": 2620
    },
    {
      "epoch": 0.5844444444444444,
      "grad_norm": 0.021457675844430923,
      "learning_rate": 0.00016115555555555556,
      "loss": 4.7934,
      "step": 2630
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.12207910418510437,
      "learning_rate": 0.0001610074074074074,
      "loss": 4.7906,
      "step": 2640
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.03473467007279396,
      "learning_rate": 0.00016085925925925926,
      "loss": 4.7768,
      "step": 2650
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 0.595558762550354,
      "learning_rate": 0.00016071111111111113,
      "loss": 4.8214,
      "step": 2660
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.051868222653865814,
      "learning_rate": 0.00016056296296296298,
      "loss": 4.8332,
      "step": 2670
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.12981002032756805,
      "learning_rate": 0.00016041481481481482,
      "loss": 4.7715,
      "step": 2680
    },
    {
      "epoch": 0.5977777777777777,
      "grad_norm": 0.05930769443511963,
      "learning_rate": 0.00016026666666666667,
      "loss": 4.7894,
      "step": 2690
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01983875408768654,
      "learning_rate": 0.00016011851851851854,
      "loss": 4.7918,
      "step": 2700
    },
    {
      "epoch": 0.6022222222222222,
      "grad_norm": 0.04878874495625496,
      "learning_rate": 0.00015997037037037036,
      "loss": 4.8316,
      "step": 2710
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.31241539120674133,
      "learning_rate": 0.00015982222222222223,
      "loss": 4.7602,
      "step": 2720
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.02190900221467018,
      "learning_rate": 0.00015967407407407408,
      "loss": 4.7977,
      "step": 2730
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.025188762694597244,
      "learning_rate": 0.00015952592592592593,
      "loss": 4.8047,
      "step": 2740
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.03448328748345375,
      "learning_rate": 0.00015937777777777777,
      "loss": 4.7763,
      "step": 2750
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.09709175676107407,
      "learning_rate": 0.00015922962962962965,
      "loss": 4.8538,
      "step": 2760
    },
    {
      "epoch": 0.6155555555555555,
      "grad_norm": 0.08132929354906082,
      "learning_rate": 0.0001590814814814815,
      "loss": 4.8268,
      "step": 2770
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 0.05931726470589638,
      "learning_rate": 0.00015893333333333334,
      "loss": 4.8351,
      "step": 2780
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.3755083084106445,
      "learning_rate": 0.00015878518518518518,
      "loss": 4.7895,
      "step": 2790
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.025575334206223488,
      "learning_rate": 0.00015863703703703706,
      "loss": 4.7844,
      "step": 2800
    },
    {
      "epoch": 0.6244444444444445,
      "grad_norm": 0.025471709668636322,
      "learning_rate": 0.00015848888888888888,
      "loss": 4.7852,
      "step": 2810
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.01796501688659191,
      "learning_rate": 0.00015834074074074075,
      "loss": 4.7849,
      "step": 2820
    },
    {
      "epoch": 0.6288888888888889,
      "grad_norm": 0.048570871353149414,
      "learning_rate": 0.0001581925925925926,
      "loss": 4.79,
      "step": 2830
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.021736647933721542,
      "learning_rate": 0.00015804444444444447,
      "loss": 4.7862,
      "step": 2840
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.027525577694177628,
      "learning_rate": 0.0001578962962962963,
      "loss": 4.8399,
      "step": 2850
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 0.05499672144651413,
      "learning_rate": 0.00015774814814814816,
      "loss": 4.7586,
      "step": 2860
    },
    {
      "epoch": 0.6377777777777778,
      "grad_norm": 0.04042093828320503,
      "learning_rate": 0.0001576,
      "loss": 4.7781,
      "step": 2870
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.06387072801589966,
      "learning_rate": 0.00015745185185185186,
      "loss": 4.7031,
      "step": 2880
    },
    {
      "epoch": 0.6422222222222222,
      "grad_norm": 0.04832005500793457,
      "learning_rate": 0.0001573037037037037,
      "loss": 4.8716,
      "step": 2890
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.2567991316318512,
      "learning_rate": 0.00015715555555555557,
      "loss": 4.7927,
      "step": 2900
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.02953645959496498,
      "learning_rate": 0.00015700740740740742,
      "loss": 4.8001,
      "step": 2910
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.04758251830935478,
      "learning_rate": 0.00015685925925925927,
      "loss": 4.7915,
      "step": 2920
    },
    {
      "epoch": 0.6511111111111111,
      "grad_norm": 0.12686872482299805,
      "learning_rate": 0.0001567111111111111,
      "loss": 4.8512,
      "step": 2930
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.02003282867372036,
      "learning_rate": 0.000156562962962963,
      "loss": 4.8142,
      "step": 2940
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.05091472715139389,
      "learning_rate": 0.0001564148148148148,
      "loss": 4.8068,
      "step": 2950
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.08855864405632019,
      "learning_rate": 0.00015626666666666668,
      "loss": 4.8325,
      "step": 2960
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.04448270425200462,
      "learning_rate": 0.00015611851851851853,
      "loss": 4.7891,
      "step": 2970
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.04784185066819191,
      "learning_rate": 0.0001559703703703704,
      "loss": 4.8306,
      "step": 2980
    },
    {
      "epoch": 0.6644444444444444,
      "grad_norm": 0.1011221781373024,
      "learning_rate": 0.00015582222222222222,
      "loss": 4.7458,
      "step": 2990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.04285053908824921,
      "learning_rate": 0.0001556740740740741,
      "loss": 4.7933,
      "step": 3000
    },
    {
      "epoch": 0.6688888888888889,
      "grad_norm": 0.13265873491764069,
      "learning_rate": 0.00015552592592592594,
      "loss": 4.8112,
      "step": 3010
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 0.028287548571825027,
      "learning_rate": 0.00015537777777777778,
      "loss": 4.8242,
      "step": 3020
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.03308146819472313,
      "learning_rate": 0.00015522962962962963,
      "loss": 4.8608,
      "step": 3030
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.05597485229372978,
      "learning_rate": 0.0001550814814814815,
      "loss": 4.8075,
      "step": 3040
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.06473412364721298,
      "learning_rate": 0.00015493333333333332,
      "loss": 4.8239,
      "step": 3050
    },
    {
      "epoch": 0.68,
      "grad_norm": 5.453907489776611,
      "learning_rate": 0.0001547851851851852,
      "loss": 4.9119,
      "step": 3060
    },
    {
      "epoch": 0.6822222222222222,
      "grad_norm": 0.09795532375574112,
      "learning_rate": 0.00015463703703703704,
      "loss": 4.8935,
      "step": 3070
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.19963125884532928,
      "learning_rate": 0.00015448888888888892,
      "loss": 4.842,
      "step": 3080
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.48394325375556946,
      "learning_rate": 0.00015434074074074073,
      "loss": 4.9079,
      "step": 3090
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.2704543173313141,
      "learning_rate": 0.0001541925925925926,
      "loss": 4.8056,
      "step": 3100
    },
    {
      "epoch": 0.6911111111111111,
      "grad_norm": 0.24303196370601654,
      "learning_rate": 0.00015404444444444445,
      "loss": 4.8226,
      "step": 3110
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.05319428816437721,
      "learning_rate": 0.0001538962962962963,
      "loss": 4.8039,
      "step": 3120
    },
    {
      "epoch": 0.6955555555555556,
      "grad_norm": 0.03242554888129234,
      "learning_rate": 0.00015374814814814815,
      "loss": 4.8396,
      "step": 3130
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 0.03477591648697853,
      "learning_rate": 0.00015360000000000002,
      "loss": 4.7811,
      "step": 3140
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04443289712071419,
      "learning_rate": 0.00015345185185185187,
      "loss": 4.8457,
      "step": 3150
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 22.163921356201172,
      "learning_rate": 0.0001533037037037037,
      "loss": 4.7535,
      "step": 3160
    },
    {
      "epoch": 0.7044444444444444,
      "grad_norm": 0.028610048815608025,
      "learning_rate": 0.00015315555555555556,
      "loss": 4.8491,
      "step": 3170
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.03832579404115677,
      "learning_rate": 0.00015300740740740743,
      "loss": 4.7938,
      "step": 3180
    },
    {
      "epoch": 0.7088888888888889,
      "grad_norm": 0.05353081598877907,
      "learning_rate": 0.00015285925925925925,
      "loss": 4.7668,
      "step": 3190
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.17786644399166107,
      "learning_rate": 0.00015271111111111112,
      "loss": 4.7995,
      "step": 3200
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.13153599202632904,
      "learning_rate": 0.00015256296296296297,
      "loss": 4.8011,
      "step": 3210
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.04630366712808609,
      "learning_rate": 0.00015241481481481484,
      "loss": 4.8315,
      "step": 3220
    },
    {
      "epoch": 0.7177777777777777,
      "grad_norm": 0.180622860789299,
      "learning_rate": 0.00015226666666666666,
      "loss": 4.7944,
      "step": 3230
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.14740556478500366,
      "learning_rate": 0.00015211851851851854,
      "loss": 4.7645,
      "step": 3240
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.0676250234246254,
      "learning_rate": 0.00015197037037037038,
      "loss": 4.8171,
      "step": 3250
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 1.061899185180664,
      "learning_rate": 0.00015182222222222223,
      "loss": 4.8499,
      "step": 3260
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.03788484260439873,
      "learning_rate": 0.00015167407407407408,
      "loss": 4.892,
      "step": 3270
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.022214988246560097,
      "learning_rate": 0.00015152592592592595,
      "loss": 4.7369,
      "step": 3280
    },
    {
      "epoch": 0.7311111111111112,
      "grad_norm": 0.07346963882446289,
      "learning_rate": 0.0001513777777777778,
      "loss": 4.8326,
      "step": 3290
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.030612295493483543,
      "learning_rate": 0.00015122962962962964,
      "loss": 4.837,
      "step": 3300
    },
    {
      "epoch": 0.7355555555555555,
      "grad_norm": 0.0907873660326004,
      "learning_rate": 0.0001510814814814815,
      "loss": 4.738,
      "step": 3310
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.057164162397384644,
      "learning_rate": 0.00015093333333333336,
      "loss": 4.807,
      "step": 3320
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.7134498953819275,
      "learning_rate": 0.00015078518518518518,
      "loss": 4.7882,
      "step": 3330
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.08121614903211594,
      "learning_rate": 0.00015063703703703705,
      "loss": 4.7829,
      "step": 3340
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.05506924167275429,
      "learning_rate": 0.0001504888888888889,
      "loss": 4.7671,
      "step": 3350
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.03904391825199127,
      "learning_rate": 0.00015034074074074075,
      "loss": 4.8073,
      "step": 3360
    },
    {
      "epoch": 0.7488888888888889,
      "grad_norm": 0.06783798336982727,
      "learning_rate": 0.0001501925925925926,
      "loss": 4.8331,
      "step": 3370
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.06048039346933365,
      "learning_rate": 0.00015004444444444447,
      "loss": 4.7749,
      "step": 3380
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06031946465373039,
      "learning_rate": 0.0001498962962962963,
      "loss": 4.8174,
      "step": 3390
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.05767936632037163,
      "learning_rate": 0.00014974814814814816,
      "loss": 4.7826,
      "step": 3400
    },
    {
      "epoch": 0.7577777777777778,
      "grad_norm": 0.06567662209272385,
      "learning_rate": 0.0001496,
      "loss": 4.7699,
      "step": 3410
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.12990343570709229,
      "learning_rate": 0.00014945185185185188,
      "loss": 4.8346,
      "step": 3420
    },
    {
      "epoch": 0.7622222222222222,
      "grad_norm": 0.5535753965377808,
      "learning_rate": 0.0001493037037037037,
      "loss": 4.7643,
      "step": 3430
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.12236610800027847,
      "learning_rate": 0.00014915555555555557,
      "loss": 4.8038,
      "step": 3440
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.47755059599876404,
      "learning_rate": 0.00014900740740740742,
      "loss": 4.7985,
      "step": 3450
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.11376135796308517,
      "learning_rate": 0.00014885925925925926,
      "loss": 4.7938,
      "step": 3460
    },
    {
      "epoch": 0.7711111111111111,
      "grad_norm": 0.22976437211036682,
      "learning_rate": 0.0001487111111111111,
      "loss": 4.8302,
      "step": 3470
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.07534478604793549,
      "learning_rate": 0.00014856296296296298,
      "loss": 4.8528,
      "step": 3480
    },
    {
      "epoch": 0.7755555555555556,
      "grad_norm": 3.7993428707122803,
      "learning_rate": 0.00014841481481481483,
      "loss": 5.0331,
      "step": 3490
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.21897488832473755,
      "learning_rate": 0.00014826666666666667,
      "loss": 4.849,
      "step": 3500
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.0291218757629395,
      "learning_rate": 0.00014811851851851852,
      "loss": 4.8051,
      "step": 3510
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 1.901645302772522,
      "learning_rate": 0.0001479703703703704,
      "loss": 4.8031,
      "step": 3520
    },
    {
      "epoch": 0.7844444444444445,
      "grad_norm": 0.045075349509716034,
      "learning_rate": 0.00014782222222222224,
      "loss": 4.8492,
      "step": 3530
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.15294213593006134,
      "learning_rate": 0.0001476740740740741,
      "loss": 4.8198,
      "step": 3540
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.16552050411701202,
      "learning_rate": 0.00014752592592592593,
      "loss": 4.7689,
      "step": 3550
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.5107570290565491,
      "learning_rate": 0.00014737777777777778,
      "loss": 4.7896,
      "step": 3560
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.7163199782371521,
      "learning_rate": 0.00014722962962962963,
      "loss": 4.8114,
      "step": 3570
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.037423014640808105,
      "learning_rate": 0.0001470814814814815,
      "loss": 4.7893,
      "step": 3580
    },
    {
      "epoch": 0.7977777777777778,
      "grad_norm": 0.08286458998918533,
      "learning_rate": 0.00014693333333333335,
      "loss": 4.8119,
      "step": 3590
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.04053176939487457,
      "learning_rate": 0.0001467851851851852,
      "loss": 4.7717,
      "step": 3600
    },
    {
      "epoch": 0.8022222222222222,
      "grad_norm": 0.39993616938591003,
      "learning_rate": 0.00014663703703703704,
      "loss": 4.8255,
      "step": 3610
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.05636029690504074,
      "learning_rate": 0.0001464888888888889,
      "loss": 4.8224,
      "step": 3620
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.03807814419269562,
      "learning_rate": 0.00014634074074074076,
      "loss": 4.76,
      "step": 3630
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.04478556662797928,
      "learning_rate": 0.0001461925925925926,
      "loss": 4.7873,
      "step": 3640
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.036233775317668915,
      "learning_rate": 0.00014604444444444445,
      "loss": 4.7974,
      "step": 3650
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.03000890649855137,
      "learning_rate": 0.0001458962962962963,
      "loss": 4.7535,
      "step": 3660
    },
    {
      "epoch": 0.8155555555555556,
      "grad_norm": 0.02313845418393612,
      "learning_rate": 0.00014574814814814814,
      "loss": 4.8096,
      "step": 3670
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.04449488967657089,
      "learning_rate": 0.00014560000000000002,
      "loss": 4.7773,
      "step": 3680
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.029727432876825333,
      "learning_rate": 0.00014545185185185186,
      "loss": 4.8153,
      "step": 3690
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.02713426947593689,
      "learning_rate": 0.0001453037037037037,
      "loss": 4.7919,
      "step": 3700
    },
    {
      "epoch": 0.8244444444444444,
      "grad_norm": 0.029725555330514908,
      "learning_rate": 0.00014515555555555555,
      "loss": 4.7962,
      "step": 3710
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.1477232426404953,
      "learning_rate": 0.00014500740740740743,
      "loss": 4.8185,
      "step": 3720
    },
    {
      "epoch": 0.8288888888888889,
      "grad_norm": 0.04521794989705086,
      "learning_rate": 0.00014485925925925927,
      "loss": 4.8099,
      "step": 3730
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.02922739088535309,
      "learning_rate": 0.00014471111111111112,
      "loss": 4.7805,
      "step": 3740
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04356227442622185,
      "learning_rate": 0.00014456296296296297,
      "loss": 4.7764,
      "step": 3750
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.032142411917448044,
      "learning_rate": 0.0001444148148148148,
      "loss": 4.8059,
      "step": 3760
    },
    {
      "epoch": 0.8377777777777777,
      "grad_norm": 0.13950306177139282,
      "learning_rate": 0.00014426666666666669,
      "loss": 4.753,
      "step": 3770
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.042966004461050034,
      "learning_rate": 0.00014411851851851853,
      "loss": 4.8356,
      "step": 3780
    },
    {
      "epoch": 0.8422222222222222,
      "grad_norm": 0.04302078112959862,
      "learning_rate": 0.00014397037037037038,
      "loss": 4.776,
      "step": 3790
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.13220183551311493,
      "learning_rate": 0.00014382222222222222,
      "loss": 4.7825,
      "step": 3800
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.07489722222089767,
      "learning_rate": 0.00014367407407407407,
      "loss": 4.7864,
      "step": 3810
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.03115803748369217,
      "learning_rate": 0.00014352592592592592,
      "loss": 4.8118,
      "step": 3820
    },
    {
      "epoch": 0.8511111111111112,
      "grad_norm": 0.03724142909049988,
      "learning_rate": 0.0001433777777777778,
      "loss": 4.7593,
      "step": 3830
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.09584537148475647,
      "learning_rate": 0.00014322962962962964,
      "loss": 4.7389,
      "step": 3840
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.10730807483196259,
      "learning_rate": 0.00014308148148148148,
      "loss": 4.7845,
      "step": 3850
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 0.5785430669784546,
      "learning_rate": 0.00014293333333333333,
      "loss": 4.8164,
      "step": 3860
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.2936428189277649,
      "learning_rate": 0.0001427851851851852,
      "loss": 4.8085,
      "step": 3870
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.03274869546294212,
      "learning_rate": 0.00014263703703703705,
      "loss": 4.796,
      "step": 3880
    },
    {
      "epoch": 0.8644444444444445,
      "grad_norm": 0.031509190797805786,
      "learning_rate": 0.0001424888888888889,
      "loss": 4.8138,
      "step": 3890
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.0474981851875782,
      "learning_rate": 0.00014234074074074074,
      "loss": 4.8202,
      "step": 3900
    },
    {
      "epoch": 0.8688888888888889,
      "grad_norm": 0.02456037513911724,
      "learning_rate": 0.0001421925925925926,
      "loss": 4.7615,
      "step": 3910
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.05099480226635933,
      "learning_rate": 0.00014204444444444443,
      "loss": 4.7777,
      "step": 3920
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.06268300861120224,
      "learning_rate": 0.0001418962962962963,
      "loss": 4.8179,
      "step": 3930
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 0.05342372879385948,
      "learning_rate": 0.00014174814814814815,
      "loss": 4.8284,
      "step": 3940
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.1927410066127777,
      "learning_rate": 0.0001416,
      "loss": 4.785,
      "step": 3950
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.02663915976881981,
      "learning_rate": 0.00014145185185185185,
      "loss": 4.814,
      "step": 3960
    },
    {
      "epoch": 0.8822222222222222,
      "grad_norm": 0.3225458860397339,
      "learning_rate": 0.00014130370370370372,
      "loss": 4.7597,
      "step": 3970
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.09178091585636139,
      "learning_rate": 0.00014115555555555557,
      "loss": 4.7666,
      "step": 3980
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.02773629128932953,
      "learning_rate": 0.0001410074074074074,
      "loss": 4.8184,
      "step": 3990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.4953054189682007,
      "learning_rate": 0.00014085925925925926,
      "loss": 4.7947,
      "step": 4000
    },
    {
      "epoch": 0.8911111111111111,
      "grad_norm": 0.050142280757427216,
      "learning_rate": 0.00014071111111111113,
      "loss": 4.7638,
      "step": 4010
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.10711908340454102,
      "learning_rate": 0.00014056296296296295,
      "loss": 4.7475,
      "step": 4020
    },
    {
      "epoch": 0.8955555555555555,
      "grad_norm": 0.07102419435977936,
      "learning_rate": 0.00014041481481481482,
      "loss": 4.8006,
      "step": 4030
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.024793591350317,
      "learning_rate": 0.00014026666666666667,
      "loss": 4.8652,
      "step": 4040
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.03574278578162193,
      "learning_rate": 0.00014011851851851852,
      "loss": 4.7674,
      "step": 4050
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 0.046670813113451004,
      "learning_rate": 0.00013997037037037036,
      "loss": 4.8236,
      "step": 4060
    },
    {
      "epoch": 0.9044444444444445,
      "grad_norm": 0.05954406037926674,
      "learning_rate": 0.00013982222222222224,
      "loss": 4.8388,
      "step": 4070
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.08055663853883743,
      "learning_rate": 0.00013967407407407408,
      "loss": 4.8023,
      "step": 4080
    },
    {
      "epoch": 0.9088888888888889,
      "grad_norm": 0.046899646520614624,
      "learning_rate": 0.00013952592592592593,
      "loss": 4.7932,
      "step": 4090
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.0761643722653389,
      "learning_rate": 0.00013937777777777777,
      "loss": 4.7828,
      "step": 4100
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.06197596713900566,
      "learning_rate": 0.00013922962962962965,
      "loss": 4.7864,
      "step": 4110
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.055811867117881775,
      "learning_rate": 0.00013908148148148147,
      "loss": 4.7674,
      "step": 4120
    },
    {
      "epoch": 0.9177777777777778,
      "grad_norm": 0.04831898584961891,
      "learning_rate": 0.00013893333333333334,
      "loss": 4.8014,
      "step": 4130
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.07409906387329102,
      "learning_rate": 0.0001387851851851852,
      "loss": 4.7763,
      "step": 4140
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.32686784863471985,
      "learning_rate": 0.00013863703703703706,
      "loss": 4.7619,
      "step": 4150
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.023378504440188408,
      "learning_rate": 0.00013848888888888888,
      "loss": 4.8268,
      "step": 4160
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.062322117388248444,
      "learning_rate": 0.00013834074074074075,
      "loss": 4.8068,
      "step": 4170
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.0839676484465599,
      "learning_rate": 0.0001381925925925926,
      "loss": 4.7955,
      "step": 4180
    },
    {
      "epoch": 0.9311111111111111,
      "grad_norm": 0.04062415659427643,
      "learning_rate": 0.00013804444444444444,
      "loss": 4.7626,
      "step": 4190
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04688778519630432,
      "learning_rate": 0.0001378962962962963,
      "loss": 4.8049,
      "step": 4200
    },
    {
      "epoch": 0.9355555555555556,
      "grad_norm": 0.24838438630104065,
      "learning_rate": 0.00013774814814814816,
      "loss": 4.7733,
      "step": 4210
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 0.07659061998128891,
      "learning_rate": 0.00013759999999999998,
      "loss": 4.8165,
      "step": 4220
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.07646184414625168,
      "learning_rate": 0.00013745185185185186,
      "loss": 4.8201,
      "step": 4230
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.2703281044960022,
      "learning_rate": 0.0001373037037037037,
      "loss": 4.7717,
      "step": 4240
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.06751704216003418,
      "learning_rate": 0.00013715555555555558,
      "loss": 4.7736,
      "step": 4250
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.04046843945980072,
      "learning_rate": 0.0001370074074074074,
      "loss": 4.8091,
      "step": 4260
    },
    {
      "epoch": 0.9488888888888889,
      "grad_norm": 0.16692186892032623,
      "learning_rate": 0.00013685925925925927,
      "loss": 4.732,
      "step": 4270
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.04203793779015541,
      "learning_rate": 0.00013671111111111112,
      "loss": 4.81,
      "step": 4280
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.02649099938571453,
      "learning_rate": 0.00013656296296296296,
      "loss": 4.7595,
      "step": 4290
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.04465778172016144,
      "learning_rate": 0.0001364148148148148,
      "loss": 4.7939,
      "step": 4300
    },
    {
      "epoch": 0.9577777777777777,
      "grad_norm": 0.13342531025409698,
      "learning_rate": 0.00013626666666666668,
      "loss": 4.8107,
      "step": 4310
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06086326390504837,
      "learning_rate": 0.00013611851851851853,
      "loss": 4.7589,
      "step": 4320
    },
    {
      "epoch": 0.9622222222222222,
      "grad_norm": 0.046362631022930145,
      "learning_rate": 0.00013597037037037037,
      "loss": 4.7834,
      "step": 4330
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 0.16470949351787567,
      "learning_rate": 0.00013582222222222222,
      "loss": 4.732,
      "step": 4340
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.019804544746875763,
      "learning_rate": 0.0001356740740740741,
      "loss": 4.8275,
      "step": 4350
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.02984030917286873,
      "learning_rate": 0.0001355259259259259,
      "loss": 4.8063,
      "step": 4360
    },
    {
      "epoch": 0.9711111111111111,
      "grad_norm": 0.194355770945549,
      "learning_rate": 0.00013537777777777779,
      "loss": 4.8323,
      "step": 4370
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.10787688195705414,
      "learning_rate": 0.00013522962962962963,
      "loss": 4.7858,
      "step": 4380
    },
    {
      "epoch": 0.9755555555555555,
      "grad_norm": 0.017161691561341286,
      "learning_rate": 0.0001350814814814815,
      "loss": 4.7597,
      "step": 4390
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.023063194006681442,
      "learning_rate": 0.00013493333333333332,
      "loss": 4.7589,
      "step": 4400
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05855024605989456,
      "learning_rate": 0.0001347851851851852,
      "loss": 4.7853,
      "step": 4410
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.07282644510269165,
      "learning_rate": 0.00013463703703703704,
      "loss": 4.7669,
      "step": 4420
    },
    {
      "epoch": 0.9844444444444445,
      "grad_norm": 0.02776503935456276,
      "learning_rate": 0.0001344888888888889,
      "loss": 4.7556,
      "step": 4430
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08908132463693619,
      "learning_rate": 0.00013434074074074074,
      "loss": 4.7721,
      "step": 4440
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.01535737980157137,
      "learning_rate": 0.0001341925925925926,
      "loss": 4.7564,
      "step": 4450
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.023407433182001114,
      "learning_rate": 0.00013404444444444446,
      "loss": 4.7565,
      "step": 4460
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.05652699992060661,
      "learning_rate": 0.0001338962962962963,
      "loss": 4.7552,
      "step": 4470
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.021533146500587463,
      "learning_rate": 0.00013374814814814815,
      "loss": 4.8032,
      "step": 4480
    },
    {
      "epoch": 0.9977777777777778,
      "grad_norm": 0.08728024363517761,
      "learning_rate": 0.00013360000000000002,
      "loss": 4.813,
      "step": 4490
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04888789355754852,
      "learning_rate": 0.00013345185185185184,
      "loss": 4.8142,
      "step": 4500
    },
    {
      "epoch": 1.0022222222222221,
      "grad_norm": 0.08452571928501129,
      "learning_rate": 0.00013330370370370371,
      "loss": 4.7804,
      "step": 4510
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.028259960934519768,
      "learning_rate": 0.00013315555555555556,
      "loss": 4.7564,
      "step": 4520
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.1106945350766182,
      "learning_rate": 0.0001330074074074074,
      "loss": 4.8262,
      "step": 4530
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 0.21127821505069733,
      "learning_rate": 0.00013285925925925925,
      "loss": 4.785,
      "step": 4540
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.22567860782146454,
      "learning_rate": 0.00013271111111111113,
      "loss": 4.8113,
      "step": 4550
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.030399782583117485,
      "learning_rate": 0.00013256296296296297,
      "loss": 4.8022,
      "step": 4560
    },
    {
      "epoch": 1.0155555555555555,
      "grad_norm": 0.06662391126155853,
      "learning_rate": 0.00013241481481481482,
      "loss": 4.7924,
      "step": 4570
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 0.04036082699894905,
      "learning_rate": 0.00013226666666666667,
      "loss": 4.8308,
      "step": 4580
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.06272118538618088,
      "learning_rate": 0.00013211851851851854,
      "loss": 4.772,
      "step": 4590
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.4230833649635315,
      "learning_rate": 0.00013197037037037036,
      "loss": 4.7998,
      "step": 4600
    },
    {
      "epoch": 1.0244444444444445,
      "grad_norm": 0.242519810795784,
      "learning_rate": 0.00013182222222222223,
      "loss": 4.8297,
      "step": 4610
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.18198652565479279,
      "learning_rate": 0.00013167407407407408,
      "loss": 4.7944,
      "step": 4620
    },
    {
      "epoch": 1.028888888888889,
      "grad_norm": 0.05433490127325058,
      "learning_rate": 0.00013152592592592595,
      "loss": 4.8036,
      "step": 4630
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.0678962990641594,
      "learning_rate": 0.00013137777777777777,
      "loss": 4.752,
      "step": 4640
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.032778237015008926,
      "learning_rate": 0.00013122962962962964,
      "loss": 4.7393,
      "step": 4650
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.03913460299372673,
      "learning_rate": 0.0001310814814814815,
      "loss": 4.7979,
      "step": 4660
    },
    {
      "epoch": 1.0377777777777777,
      "grad_norm": 0.05847787857055664,
      "learning_rate": 0.00013093333333333334,
      "loss": 4.7379,
      "step": 4670
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.03265945240855217,
      "learning_rate": 0.00013078518518518518,
      "loss": 4.776,
      "step": 4680
    },
    {
      "epoch": 1.0422222222222222,
      "grad_norm": 0.09606794267892838,
      "learning_rate": 0.00013063703703703706,
      "loss": 4.7956,
      "step": 4690
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 19.039653778076172,
      "learning_rate": 0.0001304888888888889,
      "loss": 4.82,
      "step": 4700
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.2836056053638458,
      "learning_rate": 0.00013034074074074075,
      "loss": 4.8022,
      "step": 4710
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 0.0209143478423357,
      "learning_rate": 0.0001301925925925926,
      "loss": 4.7884,
      "step": 4720
    },
    {
      "epoch": 1.051111111111111,
      "grad_norm": 0.02102196030318737,
      "learning_rate": 0.00013004444444444447,
      "loss": 4.7406,
      "step": 4730
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.020810887217521667,
      "learning_rate": 0.00012989629629629629,
      "loss": 4.775,
      "step": 4740
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.05707002431154251,
      "learning_rate": 0.00012974814814814816,
      "loss": 4.8475,
      "step": 4750
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.025925656780600548,
      "learning_rate": 0.0001296,
      "loss": 4.7876,
      "step": 4760
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.07592500746250153,
      "learning_rate": 0.00012945185185185188,
      "loss": 4.8099,
      "step": 4770
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 0.029528193175792694,
      "learning_rate": 0.0001293037037037037,
      "loss": 4.7722,
      "step": 4780
    },
    {
      "epoch": 1.0644444444444445,
      "grad_norm": 0.024207063019275665,
      "learning_rate": 0.00012915555555555557,
      "loss": 4.8261,
      "step": 4790
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.021839218214154243,
      "learning_rate": 0.00012900740740740742,
      "loss": 4.7699,
      "step": 4800
    },
    {
      "epoch": 1.068888888888889,
      "grad_norm": 0.15723957121372223,
      "learning_rate": 0.00012885925925925926,
      "loss": 4.7664,
      "step": 4810
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.3295145332813263,
      "learning_rate": 0.0001287111111111111,
      "loss": 4.8142,
      "step": 4820
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.20102110505104065,
      "learning_rate": 0.00012856296296296298,
      "loss": 4.7797,
      "step": 4830
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.06332946568727493,
      "learning_rate": 0.0001284148148148148,
      "loss": 4.8113,
      "step": 4840
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.04120546206831932,
      "learning_rate": 0.00012826666666666668,
      "loss": 4.7177,
      "step": 4850
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.021369364112615585,
      "learning_rate": 0.00012811851851851852,
      "loss": 4.7875,
      "step": 4860
    },
    {
      "epoch": 1.0822222222222222,
      "grad_norm": 0.028867194429039955,
      "learning_rate": 0.0001279703703703704,
      "loss": 4.7943,
      "step": 4870
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.02713795006275177,
      "learning_rate": 0.00012782222222222222,
      "loss": 4.7882,
      "step": 4880
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.025911038741469383,
      "learning_rate": 0.0001276740740740741,
      "loss": 4.8239,
      "step": 4890
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.47331324219703674,
      "learning_rate": 0.00012752592592592593,
      "loss": 4.7602,
      "step": 4900
    },
    {
      "epoch": 1.0911111111111111,
      "grad_norm": 0.024547653272747993,
      "learning_rate": 0.00012737777777777778,
      "loss": 4.7958,
      "step": 4910
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.0297675933688879,
      "learning_rate": 0.00012722962962962963,
      "loss": 4.7968,
      "step": 4920
    },
    {
      "epoch": 1.0955555555555556,
      "grad_norm": 5.166498184204102,
      "learning_rate": 0.0001270814814814815,
      "loss": 4.746,
      "step": 4930
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 0.1047884002327919,
      "learning_rate": 0.00012693333333333335,
      "loss": 4.7678,
      "step": 4940
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.041804756969213486,
      "learning_rate": 0.0001267851851851852,
      "loss": 4.8086,
      "step": 4950
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.4727305471897125,
      "learning_rate": 0.00012663703703703704,
      "loss": 4.8167,
      "step": 4960
    },
    {
      "epoch": 1.1044444444444443,
      "grad_norm": 0.024291785433888435,
      "learning_rate": 0.0001264888888888889,
      "loss": 4.8538,
      "step": 4970
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.05105869844555855,
      "learning_rate": 0.00012634074074074073,
      "loss": 4.8012,
      "step": 4980
    },
    {
      "epoch": 1.1088888888888888,
      "grad_norm": 0.01768808625638485,
      "learning_rate": 0.0001261925925925926,
      "loss": 4.7535,
      "step": 4990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.024806179106235504,
      "learning_rate": 0.00012604444444444445,
      "loss": 4.7626,
      "step": 5000
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.08845096081495285,
      "learning_rate": 0.00012589629629629632,
      "loss": 4.7829,
      "step": 5010
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 0.03240024298429489,
      "learning_rate": 0.00012574814814814814,
      "loss": 4.7862,
      "step": 5020
    },
    {
      "epoch": 1.1177777777777778,
      "grad_norm": 0.02122502587735653,
      "learning_rate": 0.00012560000000000002,
      "loss": 4.7757,
      "step": 5030
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.03196129575371742,
      "learning_rate": 0.00012545185185185186,
      "loss": 4.7755,
      "step": 5040
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.017105702310800552,
      "learning_rate": 0.0001253037037037037,
      "loss": 4.8158,
      "step": 5050
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 3.234753131866455,
      "learning_rate": 0.00012515555555555556,
      "loss": 4.7681,
      "step": 5060
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.05565845966339111,
      "learning_rate": 0.00012500740740740743,
      "loss": 4.7925,
      "step": 5070
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 0.10281645506620407,
      "learning_rate": 0.00012485925925925928,
      "loss": 4.7764,
      "step": 5080
    },
    {
      "epoch": 1.1311111111111112,
      "grad_norm": 0.21317817270755768,
      "learning_rate": 0.00012471111111111112,
      "loss": 4.7902,
      "step": 5090
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.04531840234994888,
      "learning_rate": 0.00012456296296296297,
      "loss": 4.7451,
      "step": 5100
    },
    {
      "epoch": 1.1355555555555557,
      "grad_norm": 0.056993912905454636,
      "learning_rate": 0.00012441481481481484,
      "loss": 4.7776,
      "step": 5110
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 0.06353379786014557,
      "learning_rate": 0.00012426666666666666,
      "loss": 4.851,
      "step": 5120
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.8964985013008118,
      "learning_rate": 0.00012411851851851853,
      "loss": 4.7787,
      "step": 5130
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.049453191459178925,
      "learning_rate": 0.00012397037037037038,
      "loss": 4.8101,
      "step": 5140
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.08007173985242844,
      "learning_rate": 0.00012382222222222223,
      "loss": 4.7696,
      "step": 5150
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.056016575545072556,
      "learning_rate": 0.00012367407407407407,
      "loss": 4.8285,
      "step": 5160
    },
    {
      "epoch": 1.1488888888888888,
      "grad_norm": 0.15244045853614807,
      "learning_rate": 0.00012352592592592595,
      "loss": 4.8284,
      "step": 5170
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 0.02495133876800537,
      "learning_rate": 0.0001233777777777778,
      "loss": 4.7897,
      "step": 5180
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.09863270074129105,
      "learning_rate": 0.00012322962962962964,
      "loss": 4.7859,
      "step": 5190
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.185833141207695,
      "learning_rate": 0.00012308148148148148,
      "loss": 4.7741,
      "step": 5200
    },
    {
      "epoch": 1.1577777777777778,
      "grad_norm": 0.18404529988765717,
      "learning_rate": 0.00012293333333333336,
      "loss": 4.826,
      "step": 5210
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.10715100169181824,
      "learning_rate": 0.00012278518518518518,
      "loss": 4.7846,
      "step": 5220
    },
    {
      "epoch": 1.1622222222222223,
      "grad_norm": 0.05356592684984207,
      "learning_rate": 0.00012263703703703705,
      "loss": 4.8121,
      "step": 5230
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 0.0484338104724884,
      "learning_rate": 0.0001224888888888889,
      "loss": 4.7642,
      "step": 5240
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.04250757023692131,
      "learning_rate": 0.00012234074074074074,
      "loss": 4.7976,
      "step": 5250
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 0.23741847276687622,
      "learning_rate": 0.0001221925925925926,
      "loss": 4.7912,
      "step": 5260
    },
    {
      "epoch": 1.1711111111111112,
      "grad_norm": 0.4945729672908783,
      "learning_rate": 0.00012204444444444445,
      "loss": 4.8087,
      "step": 5270
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.06573821604251862,
      "learning_rate": 0.00012189629629629631,
      "loss": 4.7624,
      "step": 5280
    },
    {
      "epoch": 1.1755555555555555,
      "grad_norm": 0.059608060866594315,
      "learning_rate": 0.00012174814814814814,
      "loss": 4.8177,
      "step": 5290
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.05558706447482109,
      "learning_rate": 0.0001216,
      "loss": 4.8001,
      "step": 5300
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.062052465975284576,
      "learning_rate": 0.00012145185185185186,
      "loss": 4.8091,
      "step": 5310
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 0.26766347885131836,
      "learning_rate": 0.00012130370370370372,
      "loss": 4.7277,
      "step": 5320
    },
    {
      "epoch": 1.1844444444444444,
      "grad_norm": 0.05976512283086777,
      "learning_rate": 0.00012115555555555555,
      "loss": 4.8036,
      "step": 5330
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.034882426261901855,
      "learning_rate": 0.00012100740740740741,
      "loss": 4.8763,
      "step": 5340
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 0.04071861878037453,
      "learning_rate": 0.00012085925925925927,
      "loss": 4.7472,
      "step": 5350
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 0.02249228022992611,
      "learning_rate": 0.0001207111111111111,
      "loss": 4.8194,
      "step": 5360
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.13052605092525482,
      "learning_rate": 0.00012056296296296297,
      "loss": 4.7996,
      "step": 5370
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.04134427011013031,
      "learning_rate": 0.00012041481481481483,
      "loss": 4.7955,
      "step": 5380
    },
    {
      "epoch": 1.1977777777777778,
      "grad_norm": 0.034597668796777725,
      "learning_rate": 0.00012026666666666669,
      "loss": 4.8017,
      "step": 5390
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.05450793728232384,
      "learning_rate": 0.00012011851851851852,
      "loss": 4.7675,
      "step": 5400
    },
    {
      "epoch": 1.2022222222222223,
      "grad_norm": 0.11891671270132065,
      "learning_rate": 0.00011997037037037038,
      "loss": 4.8302,
      "step": 5410
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 0.03354271501302719,
      "learning_rate": 0.00011982222222222224,
      "loss": 4.7787,
      "step": 5420
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.3007601797580719,
      "learning_rate": 0.00011967407407407407,
      "loss": 4.735,
      "step": 5430
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 0.0628771036863327,
      "learning_rate": 0.00011952592592592593,
      "loss": 4.845,
      "step": 5440
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 0.015806399285793304,
      "learning_rate": 0.00011937777777777779,
      "loss": 4.8268,
      "step": 5450
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.017702218145132065,
      "learning_rate": 0.00011922962962962962,
      "loss": 4.7527,
      "step": 5460
    },
    {
      "epoch": 1.2155555555555555,
      "grad_norm": 0.026119399815797806,
      "learning_rate": 0.00011908148148148148,
      "loss": 4.8106,
      "step": 5470
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 0.2901439666748047,
      "learning_rate": 0.00011893333333333334,
      "loss": 4.7585,
      "step": 5480
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.026482991874217987,
      "learning_rate": 0.0001187851851851852,
      "loss": 4.8004,
      "step": 5490
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.21608060598373413,
      "learning_rate": 0.00011863703703703703,
      "loss": 4.8445,
      "step": 5500
    },
    {
      "epoch": 1.2244444444444444,
      "grad_norm": 0.06541945785284042,
      "learning_rate": 0.0001184888888888889,
      "loss": 4.8128,
      "step": 5510
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.19849471747875214,
      "learning_rate": 0.00011834074074074075,
      "loss": 4.8142,
      "step": 5520
    },
    {
      "epoch": 1.228888888888889,
      "grad_norm": 0.10124270617961884,
      "learning_rate": 0.00011819259259259259,
      "loss": 4.7935,
      "step": 5530
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.021340027451515198,
      "learning_rate": 0.00011804444444444445,
      "loss": 4.8236,
      "step": 5540
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.023249106481671333,
      "learning_rate": 0.0001178962962962963,
      "loss": 4.8146,
      "step": 5550
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 0.15315869450569153,
      "learning_rate": 0.00011774814814814817,
      "loss": 4.7883,
      "step": 5560
    },
    {
      "epoch": 1.2377777777777779,
      "grad_norm": 0.02900037169456482,
      "learning_rate": 0.0001176,
      "loss": 4.819,
      "step": 5570
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.0111347436904907,
      "learning_rate": 0.00011745185185185186,
      "loss": 4.8296,
      "step": 5580
    },
    {
      "epoch": 1.2422222222222223,
      "grad_norm": 0.07447566092014313,
      "learning_rate": 0.00011730370370370372,
      "loss": 4.8052,
      "step": 5590
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 1.0860793590545654,
      "learning_rate": 0.00011715555555555555,
      "loss": 4.8223,
      "step": 5600
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.382079541683197,
      "learning_rate": 0.00011700740740740741,
      "loss": 4.7766,
      "step": 5610
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.043336499482393265,
      "learning_rate": 0.00011687407407407407,
      "loss": 4.795,
      "step": 5620
    },
    {
      "epoch": 1.251111111111111,
      "grad_norm": 0.050542302429676056,
      "learning_rate": 0.00011672592592592593,
      "loss": 4.804,
      "step": 5630
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.06133347377181053,
      "learning_rate": 0.00011657777777777779,
      "loss": 4.7927,
      "step": 5640
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.05230249837040901,
      "learning_rate": 0.00011642962962962965,
      "loss": 4.7611,
      "step": 5650
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 0.02220809832215309,
      "learning_rate": 0.00011628148148148148,
      "loss": 4.7879,
      "step": 5660
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.024447977542877197,
      "learning_rate": 0.00011613333333333334,
      "loss": 4.8162,
      "step": 5670
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 0.019945818930864334,
      "learning_rate": 0.0001159851851851852,
      "loss": 4.8378,
      "step": 5680
    },
    {
      "epoch": 1.2644444444444445,
      "grad_norm": 0.13786359131336212,
      "learning_rate": 0.00011583703703703703,
      "loss": 4.7637,
      "step": 5690
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 18.78807830810547,
      "learning_rate": 0.0001156888888888889,
      "loss": 4.8312,
      "step": 5700
    },
    {
      "epoch": 1.268888888888889,
      "grad_norm": 0.059430260211229324,
      "learning_rate": 0.00011554074074074075,
      "loss": 4.8444,
      "step": 5710
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 0.09854090213775635,
      "learning_rate": 0.00011539259259259261,
      "loss": 4.8571,
      "step": 5720
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.024914080277085304,
      "learning_rate": 0.00011524444444444445,
      "loss": 4.7732,
      "step": 5730
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 0.13818904757499695,
      "learning_rate": 0.0001150962962962963,
      "loss": 4.7916,
      "step": 5740
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.020959006622433662,
      "learning_rate": 0.00011494814814814817,
      "loss": 4.7624,
      "step": 5750
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.026929831132292747,
      "learning_rate": 0.0001148,
      "loss": 4.7908,
      "step": 5760
    },
    {
      "epoch": 1.2822222222222222,
      "grad_norm": 1.695643424987793,
      "learning_rate": 0.00011465185185185186,
      "loss": 4.7854,
      "step": 5770
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 0.05035816505551338,
      "learning_rate": 0.00011450370370370372,
      "loss": 4.844,
      "step": 5780
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.07391200214624405,
      "learning_rate": 0.00011435555555555558,
      "loss": 4.799,
      "step": 5790
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.12064913660287857,
      "learning_rate": 0.00011420740740740741,
      "loss": 4.8549,
      "step": 5800
    },
    {
      "epoch": 1.291111111111111,
      "grad_norm": 0.07925701886415482,
      "learning_rate": 0.00011405925925925927,
      "loss": 4.7213,
      "step": 5810
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.09980065375566483,
      "learning_rate": 0.00011391111111111113,
      "loss": 4.7567,
      "step": 5820
    },
    {
      "epoch": 1.2955555555555556,
      "grad_norm": 0.024438371881842613,
      "learning_rate": 0.00011376296296296296,
      "loss": 4.7982,
      "step": 5830
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 0.1777978241443634,
      "learning_rate": 0.00011361481481481482,
      "loss": 4.7913,
      "step": 5840
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.015350492671132088,
      "learning_rate": 0.00011346666666666668,
      "loss": 4.7792,
      "step": 5850
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.7490591406822205,
      "learning_rate": 0.00011331851851851853,
      "loss": 4.7738,
      "step": 5860
    },
    {
      "epoch": 1.3044444444444445,
      "grad_norm": 0.021689895540475845,
      "learning_rate": 0.00011317037037037038,
      "loss": 4.7388,
      "step": 5870
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.051843516528606415,
      "learning_rate": 0.00011302222222222223,
      "loss": 4.8206,
      "step": 5880
    },
    {
      "epoch": 1.3088888888888888,
      "grad_norm": 0.032256096601486206,
      "learning_rate": 0.00011287407407407408,
      "loss": 4.7544,
      "step": 5890
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.018711233511567116,
      "learning_rate": 0.00011272592592592593,
      "loss": 4.8085,
      "step": 5900
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.015077993273735046,
      "learning_rate": 0.00011257777777777779,
      "loss": 4.774,
      "step": 5910
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 0.038186293095350266,
      "learning_rate": 0.00011242962962962965,
      "loss": 4.7232,
      "step": 5920
    },
    {
      "epoch": 1.3177777777777777,
      "grad_norm": 0.018883811309933662,
      "learning_rate": 0.00011228148148148148,
      "loss": 4.7899,
      "step": 5930
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.2901497185230255,
      "learning_rate": 0.00011213333333333334,
      "loss": 4.7165,
      "step": 5940
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.02490781433880329,
      "learning_rate": 0.0001119851851851852,
      "loss": 4.7698,
      "step": 5950
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 0.04333404079079628,
      "learning_rate": 0.00011183703703703705,
      "loss": 4.7744,
      "step": 5960
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.026569534093141556,
      "learning_rate": 0.00011168888888888889,
      "loss": 4.7832,
      "step": 5970
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 0.037562962621450424,
      "learning_rate": 0.00011154074074074075,
      "loss": 4.7609,
      "step": 5980
    },
    {
      "epoch": 1.3311111111111111,
      "grad_norm": 0.017963560298085213,
      "learning_rate": 0.0001113925925925926,
      "loss": 4.7923,
      "step": 5990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.01403115876019001,
      "learning_rate": 0.00011124444444444444,
      "loss": 4.7725,
      "step": 6000
    },
    {
      "epoch": 1.3355555555555556,
      "grad_norm": 0.025428345426917076,
      "learning_rate": 0.0001110962962962963,
      "loss": 4.7818,
      "step": 6010
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.025194944813847542,
      "learning_rate": 0.00011094814814814815,
      "loss": 4.8173,
      "step": 6020
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.1221214011311531,
      "learning_rate": 0.00011080000000000001,
      "loss": 4.7842,
      "step": 6030
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 0.31436076760292053,
      "learning_rate": 0.00011065185185185186,
      "loss": 4.8142,
      "step": 6040
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.31793665885925293,
      "learning_rate": 0.00011050370370370372,
      "loss": 4.8233,
      "step": 6050
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.022506343200802803,
      "learning_rate": 0.00011035555555555556,
      "loss": 4.8276,
      "step": 6060
    },
    {
      "epoch": 1.3488888888888888,
      "grad_norm": 0.02493954636156559,
      "learning_rate": 0.00011020740740740741,
      "loss": 4.7399,
      "step": 6070
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 0.3295588195323944,
      "learning_rate": 0.00011005925925925927,
      "loss": 4.7631,
      "step": 6080
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.021865839138627052,
      "learning_rate": 0.00010991111111111111,
      "loss": 4.7696,
      "step": 6090
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.33869171142578125,
      "learning_rate": 0.00010976296296296297,
      "loss": 4.7862,
      "step": 6100
    },
    {
      "epoch": 1.3577777777777778,
      "grad_norm": 0.04637591540813446,
      "learning_rate": 0.00010961481481481482,
      "loss": 4.7539,
      "step": 6110
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.0219959057867527,
      "learning_rate": 0.00010946666666666667,
      "loss": 4.7512,
      "step": 6120
    },
    {
      "epoch": 1.3622222222222222,
      "grad_norm": 0.020389938727021217,
      "learning_rate": 0.00010931851851851853,
      "loss": 4.7762,
      "step": 6130
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 0.032535143196582794,
      "learning_rate": 0.00010917037037037037,
      "loss": 4.7509,
      "step": 6140
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 3.90012788772583,
      "learning_rate": 0.00010902222222222222,
      "loss": 4.7802,
      "step": 6150
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 0.07463262975215912,
      "learning_rate": 0.00010887407407407408,
      "loss": 4.8607,
      "step": 6160
    },
    {
      "epoch": 1.3711111111111112,
      "grad_norm": 0.1461130976676941,
      "learning_rate": 0.00010872592592592594,
      "loss": 4.7926,
      "step": 6170
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.08509598672389984,
      "learning_rate": 0.00010857777777777778,
      "loss": 4.8081,
      "step": 6180
    },
    {
      "epoch": 1.3755555555555556,
      "grad_norm": 0.05934100225567818,
      "learning_rate": 0.00010842962962962963,
      "loss": 4.8072,
      "step": 6190
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.024265199899673462,
      "learning_rate": 0.00010828148148148149,
      "loss": 4.8484,
      "step": 6200
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.05614480748772621,
      "learning_rate": 0.00010813333333333334,
      "loss": 4.7632,
      "step": 6210
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 0.01911739632487297,
      "learning_rate": 0.00010798518518518518,
      "loss": 4.7502,
      "step": 6220
    },
    {
      "epoch": 1.3844444444444444,
      "grad_norm": 0.18608489632606506,
      "learning_rate": 0.00010783703703703704,
      "loss": 4.7953,
      "step": 6230
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.4179264008998871,
      "learning_rate": 0.00010768888888888889,
      "loss": 4.7527,
      "step": 6240
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.26628148555755615,
      "learning_rate": 0.00010754074074074074,
      "loss": 4.8138,
      "step": 6250
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.01920265331864357,
      "learning_rate": 0.0001073925925925926,
      "loss": 4.7841,
      "step": 6260
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.028864948078989983,
      "learning_rate": 0.00010724444444444446,
      "loss": 4.7849,
      "step": 6270
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 0.015252499841153622,
      "learning_rate": 0.00010709629629629629,
      "loss": 4.7712,
      "step": 6280
    },
    {
      "epoch": 1.3977777777777778,
      "grad_norm": 0.03415839001536369,
      "learning_rate": 0.00010694814814814815,
      "loss": 4.8505,
      "step": 6290
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.20028972625732422,
      "learning_rate": 0.00010680000000000001,
      "loss": 4.7784,
      "step": 6300
    },
    {
      "epoch": 1.4022222222222223,
      "grad_norm": 0.01495153084397316,
      "learning_rate": 0.00010665185185185185,
      "loss": 4.8238,
      "step": 6310
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 0.06947348266839981,
      "learning_rate": 0.0001065037037037037,
      "loss": 4.8065,
      "step": 6320
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.07321708649396896,
      "learning_rate": 0.00010635555555555556,
      "loss": 4.7975,
      "step": 6330
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.040074337273836136,
      "learning_rate": 0.00010620740740740742,
      "loss": 4.7687,
      "step": 6340
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 2.988091230392456,
      "learning_rate": 0.00010605925925925925,
      "loss": 4.7884,
      "step": 6350
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.04384012520313263,
      "learning_rate": 0.00010591111111111111,
      "loss": 4.8083,
      "step": 6360
    },
    {
      "epoch": 1.4155555555555557,
      "grad_norm": 0.07444912940263748,
      "learning_rate": 0.00010576296296296297,
      "loss": 4.8088,
      "step": 6370
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 0.18638065457344055,
      "learning_rate": 0.0001056148148148148,
      "loss": 4.7631,
      "step": 6380
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.1641968935728073,
      "learning_rate": 0.00010546666666666666,
      "loss": 4.7983,
      "step": 6390
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.04386325553059578,
      "learning_rate": 0.00010531851851851852,
      "loss": 4.7645,
      "step": 6400
    },
    {
      "epoch": 1.4244444444444444,
      "grad_norm": 0.019170956686139107,
      "learning_rate": 0.00010517037037037038,
      "loss": 4.7693,
      "step": 6410
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.044605325907468796,
      "learning_rate": 0.00010502222222222222,
      "loss": 4.7997,
      "step": 6420
    },
    {
      "epoch": 1.4288888888888889,
      "grad_norm": 0.026318492367863655,
      "learning_rate": 0.00010487407407407408,
      "loss": 4.7765,
      "step": 6430
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 0.03252828121185303,
      "learning_rate": 0.00010472592592592594,
      "loss": 4.7688,
      "step": 6440
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.017913861200213432,
      "learning_rate": 0.00010457777777777777,
      "loss": 4.8043,
      "step": 6450
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 0.09654092788696289,
      "learning_rate": 0.00010442962962962963,
      "loss": 4.8,
      "step": 6460
    },
    {
      "epoch": 1.4377777777777778,
      "grad_norm": 0.16987952589988708,
      "learning_rate": 0.00010428148148148149,
      "loss": 4.7772,
      "step": 6470
    },
    {
      "epoch": 1.44,
      "grad_norm": 17.719528198242188,
      "learning_rate": 0.00010413333333333335,
      "loss": 4.7734,
      "step": 6480
    },
    {
      "epoch": 1.4422222222222223,
      "grad_norm": 0.020666705444455147,
      "learning_rate": 0.00010398518518518518,
      "loss": 4.7647,
      "step": 6490
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.026988042518496513,
      "learning_rate": 0.00010383703703703704,
      "loss": 4.8032,
      "step": 6500
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.020031923428177834,
      "learning_rate": 0.0001036888888888889,
      "loss": 4.7798,
      "step": 6510
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 0.022354431450366974,
      "learning_rate": 0.00010354074074074073,
      "loss": 4.7703,
      "step": 6520
    },
    {
      "epoch": 1.451111111111111,
      "grad_norm": 0.19942891597747803,
      "learning_rate": 0.00010339259259259259,
      "loss": 4.725,
      "step": 6530
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.01963507942855358,
      "learning_rate": 0.00010324444444444445,
      "loss": 4.8269,
      "step": 6540
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.025747833773493767,
      "learning_rate": 0.00010309629629629629,
      "loss": 4.8167,
      "step": 6550
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 0.027106989175081253,
      "learning_rate": 0.00010294814814814815,
      "loss": 4.8,
      "step": 6560
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.023052098229527473,
      "learning_rate": 0.0001028,
      "loss": 4.7513,
      "step": 6570
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.021009517833590508,
      "learning_rate": 0.00010265185185185186,
      "loss": 4.8356,
      "step": 6580
    },
    {
      "epoch": 1.4644444444444444,
      "grad_norm": 0.09725311398506165,
      "learning_rate": 0.0001025037037037037,
      "loss": 4.8127,
      "step": 6590
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.029716521501541138,
      "learning_rate": 0.00010235555555555556,
      "loss": 4.8102,
      "step": 6600
    },
    {
      "epoch": 1.468888888888889,
      "grad_norm": 0.04157308116555214,
      "learning_rate": 0.00010220740740740742,
      "loss": 4.8003,
      "step": 6610
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 0.0246648658066988,
      "learning_rate": 0.00010205925925925925,
      "loss": 4.8471,
      "step": 6620
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.035711221396923065,
      "learning_rate": 0.00010191111111111111,
      "loss": 4.8625,
      "step": 6630
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 0.03084324486553669,
      "learning_rate": 0.00010176296296296297,
      "loss": 4.8349,
      "step": 6640
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.03332410007715225,
      "learning_rate": 0.00010161481481481483,
      "loss": 4.7869,
      "step": 6650
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.053797122091054916,
      "learning_rate": 0.00010146666666666666,
      "loss": 4.7648,
      "step": 6660
    },
    {
      "epoch": 1.482222222222222,
      "grad_norm": 0.03506150841712952,
      "learning_rate": 0.00010131851851851852,
      "loss": 4.8049,
      "step": 6670
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 0.06615491211414337,
      "learning_rate": 0.00010117037037037038,
      "loss": 4.8569,
      "step": 6680
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.019253505393862724,
      "learning_rate": 0.00010102222222222221,
      "loss": 4.7968,
      "step": 6690
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.0742836743593216,
      "learning_rate": 0.00010087407407407407,
      "loss": 4.8036,
      "step": 6700
    },
    {
      "epoch": 1.491111111111111,
      "grad_norm": 0.027742469683289528,
      "learning_rate": 0.00010072592592592593,
      "loss": 4.7446,
      "step": 6710
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04087437689304352,
      "learning_rate": 0.0001005777777777778,
      "loss": 4.7858,
      "step": 6720
    },
    {
      "epoch": 1.4955555555555555,
      "grad_norm": 0.019296251237392426,
      "learning_rate": 0.00010042962962962963,
      "loss": 4.8095,
      "step": 6730
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.020938368514180183,
      "learning_rate": 0.00010028148148148149,
      "loss": 4.7858,
      "step": 6740
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3648899793624878,
      "learning_rate": 0.00010013333333333335,
      "loss": 4.8142,
      "step": 6750
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 0.014508887194097042,
      "learning_rate": 9.998518518518519e-05,
      "loss": 4.7672,
      "step": 6760
    },
    {
      "epoch": 1.5044444444444445,
      "grad_norm": 0.01872050203382969,
      "learning_rate": 9.983703703703704e-05,
      "loss": 4.7392,
      "step": 6770
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.05703608691692352,
      "learning_rate": 9.96888888888889e-05,
      "loss": 4.7679,
      "step": 6780
    },
    {
      "epoch": 1.508888888888889,
      "grad_norm": 0.12451297044754028,
      "learning_rate": 9.954074074074074e-05,
      "loss": 4.7286,
      "step": 6790
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.026907838881015778,
      "learning_rate": 9.939259259259259e-05,
      "loss": 4.7887,
      "step": 6800
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.04832625389099121,
      "learning_rate": 9.924444444444445e-05,
      "loss": 4.7865,
      "step": 6810
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.042281441390514374,
      "learning_rate": 9.90962962962963e-05,
      "loss": 4.8302,
      "step": 6820
    },
    {
      "epoch": 1.517777777777778,
      "grad_norm": 0.1119837835431099,
      "learning_rate": 9.894814814814816e-05,
      "loss": 4.8037,
      "step": 6830
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.024302266538143158,
      "learning_rate": 9.88e-05,
      "loss": 4.7733,
      "step": 6840
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.04001694172620773,
      "learning_rate": 9.865185185185186e-05,
      "loss": 4.7582,
      "step": 6850
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 0.17025546729564667,
      "learning_rate": 9.850370370370371e-05,
      "loss": 4.7555,
      "step": 6860
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.2994921803474426,
      "learning_rate": 9.835555555555556e-05,
      "loss": 4.8209,
      "step": 6870
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 0.06324845552444458,
      "learning_rate": 9.820740740740741e-05,
      "loss": 4.8309,
      "step": 6880
    },
    {
      "epoch": 1.531111111111111,
      "grad_norm": 0.18186601996421814,
      "learning_rate": 9.805925925925926e-05,
      "loss": 4.7693,
      "step": 6890
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.1512231826782227,
      "learning_rate": 9.791111111111112e-05,
      "loss": 4.7441,
      "step": 6900
    },
    {
      "epoch": 1.5355555555555556,
      "grad_norm": 0.23159898817539215,
      "learning_rate": 9.776296296296297e-05,
      "loss": 4.783,
      "step": 6910
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 0.06279203295707703,
      "learning_rate": 9.761481481481481e-05,
      "loss": 4.7792,
      "step": 6920
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.0744008794426918,
      "learning_rate": 9.746666666666667e-05,
      "loss": 4.789,
      "step": 6930
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 0.023967662826180458,
      "learning_rate": 9.731851851851852e-05,
      "loss": 4.8028,
      "step": 6940
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.046916622668504715,
      "learning_rate": 9.717037037037038e-05,
      "loss": 4.7866,
      "step": 6950
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.08816631138324738,
      "learning_rate": 9.702222222222223e-05,
      "loss": 4.7821,
      "step": 6960
    },
    {
      "epoch": 1.548888888888889,
      "grad_norm": 0.2811928987503052,
      "learning_rate": 9.687407407407409e-05,
      "loss": 4.7899,
      "step": 6970
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.022983692586421967,
      "learning_rate": 9.672592592592593e-05,
      "loss": 4.7321,
      "step": 6980
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.06913766264915466,
      "learning_rate": 9.657777777777778e-05,
      "loss": 4.8278,
      "step": 6990
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.02734098955988884,
      "learning_rate": 9.642962962962964e-05,
      "loss": 4.7697,
      "step": 7000
    },
    {
      "epoch": 1.557777777777778,
      "grad_norm": 0.3679670989513397,
      "learning_rate": 9.628148148148148e-05,
      "loss": 4.8235,
      "step": 7010
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.09171559661626816,
      "learning_rate": 9.613333333333334e-05,
      "loss": 4.8089,
      "step": 7020
    },
    {
      "epoch": 1.5622222222222222,
      "grad_norm": 0.032119765877723694,
      "learning_rate": 9.598518518518519e-05,
      "loss": 4.7766,
      "step": 7030
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 0.12721389532089233,
      "learning_rate": 9.583703703703704e-05,
      "loss": 4.7925,
      "step": 7040
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.014687255956232548,
      "learning_rate": 9.56888888888889e-05,
      "loss": 4.7578,
      "step": 7050
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.17396128177642822,
      "learning_rate": 9.554074074074074e-05,
      "loss": 4.7704,
      "step": 7060
    },
    {
      "epoch": 1.5711111111111111,
      "grad_norm": 0.052233289927244186,
      "learning_rate": 9.53925925925926e-05,
      "loss": 4.728,
      "step": 7070
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.03497440367937088,
      "learning_rate": 9.524444444444445e-05,
      "loss": 4.7695,
      "step": 7080
    },
    {
      "epoch": 1.5755555555555556,
      "grad_norm": 0.06302551925182343,
      "learning_rate": 9.509629629629631e-05,
      "loss": 4.7689,
      "step": 7090
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.0357128381729126,
      "learning_rate": 9.494814814814815e-05,
      "loss": 4.7389,
      "step": 7100
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.03966705873608589,
      "learning_rate": 9.48e-05,
      "loss": 4.7677,
      "step": 7110
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 0.026304086670279503,
      "learning_rate": 9.465185185185186e-05,
      "loss": 4.766,
      "step": 7120
    },
    {
      "epoch": 1.5844444444444443,
      "grad_norm": 0.06291903555393219,
      "learning_rate": 9.45037037037037e-05,
      "loss": 4.7856,
      "step": 7130
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04882756620645523,
      "learning_rate": 9.435555555555557e-05,
      "loss": 4.81,
      "step": 7140
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 0.021733568981289864,
      "learning_rate": 9.420740740740741e-05,
      "loss": 4.7745,
      "step": 7150
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 0.028146861121058464,
      "learning_rate": 9.405925925925927e-05,
      "loss": 4.7696,
      "step": 7160
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.050057269632816315,
      "learning_rate": 9.391111111111112e-05,
      "loss": 4.7935,
      "step": 7170
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 0.023657282814383507,
      "learning_rate": 9.376296296296296e-05,
      "loss": 4.7715,
      "step": 7180
    },
    {
      "epoch": 1.5977777777777777,
      "grad_norm": 84.54401397705078,
      "learning_rate": 9.361481481481482e-05,
      "loss": 4.774,
      "step": 7190
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.022510504350066185,
      "learning_rate": 9.346666666666667e-05,
      "loss": 4.8028,
      "step": 7200
    },
    {
      "epoch": 1.6022222222222222,
      "grad_norm": 0.10322275757789612,
      "learning_rate": 9.331851851851853e-05,
      "loss": 4.7364,
      "step": 7210
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.05915692448616028,
      "learning_rate": 9.317037037037038e-05,
      "loss": 4.7899,
      "step": 7220
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.04321300610899925,
      "learning_rate": 9.302222222222222e-05,
      "loss": 4.7987,
      "step": 7230
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 0.21013814210891724,
      "learning_rate": 9.287407407407408e-05,
      "loss": 4.8393,
      "step": 7240
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.06787678599357605,
      "learning_rate": 9.272592592592593e-05,
      "loss": 4.7892,
      "step": 7250
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.023083709180355072,
      "learning_rate": 9.257777777777779e-05,
      "loss": 4.7707,
      "step": 7260
    },
    {
      "epoch": 1.6155555555555554,
      "grad_norm": 0.01846177875995636,
      "learning_rate": 9.242962962962964e-05,
      "loss": 4.7886,
      "step": 7270
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 0.05379510298371315,
      "learning_rate": 9.22814814814815e-05,
      "loss": 4.7676,
      "step": 7280
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.014884069561958313,
      "learning_rate": 9.213333333333334e-05,
      "loss": 4.8209,
      "step": 7290
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.020587341859936714,
      "learning_rate": 9.198518518518519e-05,
      "loss": 4.7721,
      "step": 7300
    },
    {
      "epoch": 1.6244444444444444,
      "grad_norm": 0.06923718750476837,
      "learning_rate": 9.183703703703705e-05,
      "loss": 4.8113,
      "step": 7310
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.012267197482287884,
      "learning_rate": 9.16888888888889e-05,
      "loss": 4.7974,
      "step": 7320
    },
    {
      "epoch": 1.628888888888889,
      "grad_norm": 0.015164549462497234,
      "learning_rate": 9.154074074074075e-05,
      "loss": 4.7484,
      "step": 7330
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 0.014141907915472984,
      "learning_rate": 9.13925925925926e-05,
      "loss": 4.7866,
      "step": 7340
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.022709010168910027,
      "learning_rate": 9.124444444444445e-05,
      "loss": 4.777,
      "step": 7350
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 0.017679180949926376,
      "learning_rate": 9.10962962962963e-05,
      "loss": 4.7858,
      "step": 7360
    },
    {
      "epoch": 1.6377777777777778,
      "grad_norm": 0.014446559362113476,
      "learning_rate": 9.094814814814815e-05,
      "loss": 4.7441,
      "step": 7370
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.03953954949975014,
      "learning_rate": 9.080000000000001e-05,
      "loss": 4.8299,
      "step": 7380
    },
    {
      "epoch": 1.6422222222222222,
      "grad_norm": 0.030880998820066452,
      "learning_rate": 9.065185185185186e-05,
      "loss": 4.756,
      "step": 7390
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.019634654745459557,
      "learning_rate": 9.050370370370372e-05,
      "loss": 4.7168,
      "step": 7400
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.04154511168599129,
      "learning_rate": 9.035555555555556e-05,
      "loss": 4.7959,
      "step": 7410
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 0.01602252572774887,
      "learning_rate": 9.020740740740741e-05,
      "loss": 4.7857,
      "step": 7420
    },
    {
      "epoch": 1.6511111111111112,
      "grad_norm": 0.0213463194668293,
      "learning_rate": 9.005925925925927e-05,
      "loss": 4.7506,
      "step": 7430
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.05031523108482361,
      "learning_rate": 8.991111111111112e-05,
      "loss": 4.785,
      "step": 7440
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.053473856300115585,
      "learning_rate": 8.976296296296298e-05,
      "loss": 4.765,
      "step": 7450
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.047420453280210495,
      "learning_rate": 8.961481481481482e-05,
      "loss": 4.8094,
      "step": 7460
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.38248005509376526,
      "learning_rate": 8.946666666666668e-05,
      "loss": 4.762,
      "step": 7470
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 0.07644687592983246,
      "learning_rate": 8.931851851851853e-05,
      "loss": 4.7846,
      "step": 7480
    },
    {
      "epoch": 1.6644444444444444,
      "grad_norm": 0.25888878107070923,
      "learning_rate": 8.917037037037037e-05,
      "loss": 4.7878,
      "step": 7490
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.05631846562027931,
      "learning_rate": 8.902222222222223e-05,
      "loss": 4.7715,
      "step": 7500
    },
    {
      "epoch": 1.6688888888888889,
      "grad_norm": 0.02119169570505619,
      "learning_rate": 8.887407407407408e-05,
      "loss": 4.7799,
      "step": 7510
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 0.04177568480372429,
      "learning_rate": 8.872592592592594e-05,
      "loss": 4.7968,
      "step": 7520
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.015302588231861591,
      "learning_rate": 8.857777777777779e-05,
      "loss": 4.825,
      "step": 7530
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.10206057131290436,
      "learning_rate": 8.842962962962963e-05,
      "loss": 4.7894,
      "step": 7540
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.17667719721794128,
      "learning_rate": 8.828148148148149e-05,
      "loss": 4.8077,
      "step": 7550
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.029276590794324875,
      "learning_rate": 8.813333333333334e-05,
      "loss": 4.7948,
      "step": 7560
    },
    {
      "epoch": 1.6822222222222223,
      "grad_norm": 0.026592064648866653,
      "learning_rate": 8.79851851851852e-05,
      "loss": 4.7926,
      "step": 7570
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 0.071540966629982,
      "learning_rate": 8.783703703703704e-05,
      "loss": 4.7974,
      "step": 7580
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.0299769826233387,
      "learning_rate": 8.76888888888889e-05,
      "loss": 4.7798,
      "step": 7590
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 4.428685665130615,
      "learning_rate": 8.754074074074075e-05,
      "loss": 4.7098,
      "step": 7600
    },
    {
      "epoch": 1.6911111111111112,
      "grad_norm": 51.94633483886719,
      "learning_rate": 8.73925925925926e-05,
      "loss": 4.7668,
      "step": 7610
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.035800181329250336,
      "learning_rate": 8.724444444444446e-05,
      "loss": 4.8321,
      "step": 7620
    },
    {
      "epoch": 1.6955555555555555,
      "grad_norm": 0.05334119871258736,
      "learning_rate": 8.70962962962963e-05,
      "loss": 4.7781,
      "step": 7630
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 0.0719485729932785,
      "learning_rate": 8.694814814814816e-05,
      "loss": 4.772,
      "step": 7640
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.05427587404847145,
      "learning_rate": 8.680000000000001e-05,
      "loss": 4.8474,
      "step": 7650
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 0.31747299432754517,
      "learning_rate": 8.665185185185186e-05,
      "loss": 4.8155,
      "step": 7660
    },
    {
      "epoch": 1.7044444444444444,
      "grad_norm": 0.140298530459404,
      "learning_rate": 8.650370370370372e-05,
      "loss": 4.79,
      "step": 7670
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.03432507812976837,
      "learning_rate": 8.635555555555556e-05,
      "loss": 4.8204,
      "step": 7680
    },
    {
      "epoch": 1.708888888888889,
      "grad_norm": 0.06496324390172958,
      "learning_rate": 8.620740740740742e-05,
      "loss": 4.774,
      "step": 7690
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.047915980219841,
      "learning_rate": 8.605925925925927e-05,
      "loss": 4.7829,
      "step": 7700
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.259055495262146,
      "learning_rate": 8.591111111111111e-05,
      "loss": 4.7702,
      "step": 7710
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 0.01595759019255638,
      "learning_rate": 8.576296296296297e-05,
      "loss": 4.7582,
      "step": 7720
    },
    {
      "epoch": 1.7177777777777776,
      "grad_norm": 0.04065530747175217,
      "learning_rate": 8.561481481481482e-05,
      "loss": 4.8098,
      "step": 7730
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.08619870990514755,
      "learning_rate": 8.546666666666667e-05,
      "loss": 4.7999,
      "step": 7740
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.026119383051991463,
      "learning_rate": 8.531851851851853e-05,
      "loss": 4.8064,
      "step": 7750
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 0.047194626182317734,
      "learning_rate": 8.517037037037037e-05,
      "loss": 4.8302,
      "step": 7760
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.05311191827058792,
      "learning_rate": 8.502222222222223e-05,
      "loss": 4.818,
      "step": 7770
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.036054015159606934,
      "learning_rate": 8.487407407407408e-05,
      "loss": 4.7907,
      "step": 7780
    },
    {
      "epoch": 1.7311111111111113,
      "grad_norm": 0.022552624344825745,
      "learning_rate": 8.472592592592592e-05,
      "loss": 4.8073,
      "step": 7790
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.021292103454470634,
      "learning_rate": 8.457777777777778e-05,
      "loss": 4.8328,
      "step": 7800
    },
    {
      "epoch": 1.7355555555555555,
      "grad_norm": 0.19436657428741455,
      "learning_rate": 8.442962962962963e-05,
      "loss": 4.8416,
      "step": 7810
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 0.784782886505127,
      "learning_rate": 8.428148148148149e-05,
      "loss": 4.7341,
      "step": 7820
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.06449822336435318,
      "learning_rate": 8.413333333333334e-05,
      "loss": 4.7718,
      "step": 7830
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 0.025556525215506554,
      "learning_rate": 8.398518518518518e-05,
      "loss": 4.7851,
      "step": 7840
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.06837764382362366,
      "learning_rate": 8.383703703703704e-05,
      "loss": 4.7967,
      "step": 7850
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.018297815695405006,
      "learning_rate": 8.368888888888889e-05,
      "loss": 4.7725,
      "step": 7860
    },
    {
      "epoch": 1.748888888888889,
      "grad_norm": 0.06496886163949966,
      "learning_rate": 8.354074074074073e-05,
      "loss": 4.8009,
      "step": 7870
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 0.012334400787949562,
      "learning_rate": 8.33925925925926e-05,
      "loss": 4.7337,
      "step": 7880
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.07518225908279419,
      "learning_rate": 8.324444444444444e-05,
      "loss": 4.7988,
      "step": 7890
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.01840183325111866,
      "learning_rate": 8.30962962962963e-05,
      "loss": 4.7885,
      "step": 7900
    },
    {
      "epoch": 1.7577777777777777,
      "grad_norm": 0.04776506870985031,
      "learning_rate": 8.294814814814815e-05,
      "loss": 4.7545,
      "step": 7910
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.04511884227395058,
      "learning_rate": 8.28e-05,
      "loss": 4.7734,
      "step": 7920
    },
    {
      "epoch": 1.7622222222222224,
      "grad_norm": 0.05504671856760979,
      "learning_rate": 8.265185185185185e-05,
      "loss": 4.7724,
      "step": 7930
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.03255520015954971,
      "learning_rate": 8.25037037037037e-05,
      "loss": 4.7791,
      "step": 7940
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.04940218850970268,
      "learning_rate": 8.235555555555556e-05,
      "loss": 4.82,
      "step": 7950
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 0.03131173551082611,
      "learning_rate": 8.22074074074074e-05,
      "loss": 4.8246,
      "step": 7960
    },
    {
      "epoch": 1.771111111111111,
      "grad_norm": 0.06879370659589767,
      "learning_rate": 8.205925925925925e-05,
      "loss": 4.7465,
      "step": 7970
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.0833025798201561,
      "learning_rate": 8.191111111111111e-05,
      "loss": 4.774,
      "step": 7980
    },
    {
      "epoch": 1.7755555555555556,
      "grad_norm": 0.031204884871840477,
      "learning_rate": 8.176296296296296e-05,
      "loss": 4.8225,
      "step": 7990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.0702601745724678,
      "learning_rate": 8.161481481481482e-05,
      "loss": 4.7665,
      "step": 8000
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.03041098639369011,
      "learning_rate": 8.146666666666666e-05,
      "loss": 4.8135,
      "step": 8010
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.04187975823879242,
      "learning_rate": 8.131851851851852e-05,
      "loss": 4.7709,
      "step": 8020
    },
    {
      "epoch": 1.7844444444444445,
      "grad_norm": 0.07035078853368759,
      "learning_rate": 8.117037037037037e-05,
      "loss": 4.777,
      "step": 8030
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.016266392543911934,
      "learning_rate": 8.102222222222222e-05,
      "loss": 4.7666,
      "step": 8040
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.45527100563049316,
      "learning_rate": 8.087407407407408e-05,
      "loss": 4.7705,
      "step": 8050
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 0.019073113799095154,
      "learning_rate": 8.072592592592592e-05,
      "loss": 4.7867,
      "step": 8060
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.022714106366038322,
      "learning_rate": 8.057777777777778e-05,
      "loss": 4.8654,
      "step": 8070
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 0.03212907537817955,
      "learning_rate": 8.042962962962963e-05,
      "loss": 4.7485,
      "step": 8080
    },
    {
      "epoch": 1.7977777777777777,
      "grad_norm": 0.08758372068405151,
      "learning_rate": 8.028148148148149e-05,
      "loss": 4.7646,
      "step": 8090
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.030290285125374794,
      "learning_rate": 8.013333333333333e-05,
      "loss": 4.7784,
      "step": 8100
    },
    {
      "epoch": 1.8022222222222222,
      "grad_norm": 0.07665334641933441,
      "learning_rate": 7.998518518518518e-05,
      "loss": 4.7993,
      "step": 8110
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 0.05381690338253975,
      "learning_rate": 7.983703703703704e-05,
      "loss": 4.7751,
      "step": 8120
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.048354845494031906,
      "learning_rate": 7.968888888888889e-05,
      "loss": 4.78,
      "step": 8130
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 0.030021613463759422,
      "learning_rate": 7.954074074074075e-05,
      "loss": 4.7735,
      "step": 8140
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.025207238271832466,
      "learning_rate": 7.939259259259259e-05,
      "loss": 4.7863,
      "step": 8150
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.07817792892456055,
      "learning_rate": 7.924444444444444e-05,
      "loss": 4.7679,
      "step": 8160
    },
    {
      "epoch": 1.8155555555555556,
      "grad_norm": 0.12076462060213089,
      "learning_rate": 7.90962962962963e-05,
      "loss": 4.7485,
      "step": 8170
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 0.09023202955722809,
      "learning_rate": 7.894814814814814e-05,
      "loss": 4.7618,
      "step": 8180
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.033520281314849854,
      "learning_rate": 7.88e-05,
      "loss": 4.8144,
      "step": 8190
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.021434413269162178,
      "learning_rate": 7.865185185185185e-05,
      "loss": 4.7868,
      "step": 8200
    },
    {
      "epoch": 1.8244444444444445,
      "grad_norm": 0.03651293367147446,
      "learning_rate": 7.850370370370371e-05,
      "loss": 4.7924,
      "step": 8210
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.3312828540802002,
      "learning_rate": 7.835555555555556e-05,
      "loss": 4.8059,
      "step": 8220
    },
    {
      "epoch": 1.8288888888888888,
      "grad_norm": 0.04854823648929596,
      "learning_rate": 7.82074074074074e-05,
      "loss": 4.7935,
      "step": 8230
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 0.03763295337557793,
      "learning_rate": 7.805925925925926e-05,
      "loss": 4.7731,
      "step": 8240
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.025031670928001404,
      "learning_rate": 7.791111111111111e-05,
      "loss": 4.754,
      "step": 8250
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.0627504214644432,
      "learning_rate": 7.776296296296297e-05,
      "loss": 4.7665,
      "step": 8260
    },
    {
      "epoch": 1.8377777777777777,
      "grad_norm": 0.04140225425362587,
      "learning_rate": 7.761481481481482e-05,
      "loss": 4.8197,
      "step": 8270
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.02222491428256035,
      "learning_rate": 7.746666666666666e-05,
      "loss": 4.8045,
      "step": 8280
    },
    {
      "epoch": 1.8422222222222222,
      "grad_norm": 0.029125133529305458,
      "learning_rate": 7.731851851851852e-05,
      "loss": 4.7666,
      "step": 8290
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.01426610630005598,
      "learning_rate": 7.717037037037037e-05,
      "loss": 4.818,
      "step": 8300
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.020797835662961006,
      "learning_rate": 7.702222222222223e-05,
      "loss": 4.7507,
      "step": 8310
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 0.04389188066124916,
      "learning_rate": 7.687407407407407e-05,
      "loss": 4.7901,
      "step": 8320
    },
    {
      "epoch": 1.8511111111111112,
      "grad_norm": 0.028638498857617378,
      "learning_rate": 7.672592592592593e-05,
      "loss": 4.7964,
      "step": 8330
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.020097455009818077,
      "learning_rate": 7.657777777777778e-05,
      "loss": 4.7671,
      "step": 8340
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.5687950849533081,
      "learning_rate": 7.642962962962963e-05,
      "loss": 4.7562,
      "step": 8350
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 0.031497105956077576,
      "learning_rate": 7.628148148148149e-05,
      "loss": 4.7999,
      "step": 8360
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.03283747285604477,
      "learning_rate": 7.613333333333333e-05,
      "loss": 4.8026,
      "step": 8370
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.27723944187164307,
      "learning_rate": 7.598518518518519e-05,
      "loss": 4.7627,
      "step": 8380
    },
    {
      "epoch": 1.8644444444444446,
      "grad_norm": 0.15052483975887299,
      "learning_rate": 7.583703703703704e-05,
      "loss": 4.836,
      "step": 8390
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.039762068539857864,
      "learning_rate": 7.56888888888889e-05,
      "loss": 4.7621,
      "step": 8400
    },
    {
      "epoch": 1.8688888888888888,
      "grad_norm": 0.020502883940935135,
      "learning_rate": 7.554074074074074e-05,
      "loss": 4.807,
      "step": 8410
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.05552402138710022,
      "learning_rate": 7.539259259259259e-05,
      "loss": 4.8351,
      "step": 8420
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03308184817433357,
      "learning_rate": 7.524444444444445e-05,
      "loss": 4.7658,
      "step": 8430
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 0.04382378235459328,
      "learning_rate": 7.50962962962963e-05,
      "loss": 4.8107,
      "step": 8440
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.03125786781311035,
      "learning_rate": 7.494814814814816e-05,
      "loss": 4.7657,
      "step": 8450
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.20509254932403564,
      "learning_rate": 7.48e-05,
      "loss": 4.7355,
      "step": 8460
    },
    {
      "epoch": 1.8822222222222222,
      "grad_norm": 0.1049448698759079,
      "learning_rate": 7.465185185185185e-05,
      "loss": 4.7808,
      "step": 8470
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 0.16937416791915894,
      "learning_rate": 7.450370370370371e-05,
      "loss": 4.7672,
      "step": 8480
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.02399841696023941,
      "learning_rate": 7.435555555555555e-05,
      "loss": 4.8398,
      "step": 8490
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.04040263220667839,
      "learning_rate": 7.420740740740741e-05,
      "loss": 4.8293,
      "step": 8500
    },
    {
      "epoch": 1.891111111111111,
      "grad_norm": 0.054273542016744614,
      "learning_rate": 7.405925925925926e-05,
      "loss": 4.8215,
      "step": 8510
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.1359177678823471,
      "learning_rate": 7.391111111111112e-05,
      "loss": 4.693,
      "step": 8520
    },
    {
      "epoch": 1.8955555555555557,
      "grad_norm": 0.04011915996670723,
      "learning_rate": 7.376296296296297e-05,
      "loss": 4.7448,
      "step": 8530
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 0.039244696497917175,
      "learning_rate": 7.361481481481481e-05,
      "loss": 4.7839,
      "step": 8540
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.0570581890642643,
      "learning_rate": 7.346666666666667e-05,
      "loss": 4.801,
      "step": 8550
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 0.01750405877828598,
      "learning_rate": 7.331851851851852e-05,
      "loss": 4.7938,
      "step": 8560
    },
    {
      "epoch": 1.9044444444444446,
      "grad_norm": 0.037849750369787216,
      "learning_rate": 7.317037037037038e-05,
      "loss": 4.7783,
      "step": 8570
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.024877332150936127,
      "learning_rate": 7.302222222222222e-05,
      "loss": 4.7883,
      "step": 8580
    },
    {
      "epoch": 1.9088888888888889,
      "grad_norm": 0.019133392721414566,
      "learning_rate": 7.287407407407407e-05,
      "loss": 4.7817,
      "step": 8590
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.04096653684973717,
      "learning_rate": 7.272592592592593e-05,
      "loss": 4.8206,
      "step": 8600
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.028099313378334045,
      "learning_rate": 7.257777777777778e-05,
      "loss": 4.7417,
      "step": 8610
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 0.042140886187553406,
      "learning_rate": 7.242962962962964e-05,
      "loss": 4.7622,
      "step": 8620
    },
    {
      "epoch": 1.9177777777777778,
      "grad_norm": 0.04200953617691994,
      "learning_rate": 7.228148148148148e-05,
      "loss": 4.799,
      "step": 8630
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.04436963051557541,
      "learning_rate": 7.213333333333334e-05,
      "loss": 4.8162,
      "step": 8640
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.018077073618769646,
      "learning_rate": 7.198518518518519e-05,
      "loss": 4.7946,
      "step": 8650
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.044587310403585434,
      "learning_rate": 7.183703703703704e-05,
      "loss": 4.7964,
      "step": 8660
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.017168698832392693,
      "learning_rate": 7.16888888888889e-05,
      "loss": 4.8247,
      "step": 8670
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 0.059473372995853424,
      "learning_rate": 7.154074074074074e-05,
      "loss": 4.8355,
      "step": 8680
    },
    {
      "epoch": 1.931111111111111,
      "grad_norm": 0.08711030334234238,
      "learning_rate": 7.13925925925926e-05,
      "loss": 4.8228,
      "step": 8690
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.02275068312883377,
      "learning_rate": 7.124444444444445e-05,
      "loss": 4.7687,
      "step": 8700
    },
    {
      "epoch": 1.9355555555555557,
      "grad_norm": 0.20656365156173706,
      "learning_rate": 7.10962962962963e-05,
      "loss": 4.7452,
      "step": 8710
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 0.09993719309568405,
      "learning_rate": 7.094814814814815e-05,
      "loss": 4.8149,
      "step": 8720
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.04074909910559654,
      "learning_rate": 7.08e-05,
      "loss": 4.8154,
      "step": 8730
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.011787938885390759,
      "learning_rate": 7.065185185185186e-05,
      "loss": 4.8221,
      "step": 8740
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.21555423736572266,
      "learning_rate": 7.05037037037037e-05,
      "loss": 4.7651,
      "step": 8750
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.29072001576423645,
      "learning_rate": 7.035555555555557e-05,
      "loss": 4.7763,
      "step": 8760
    },
    {
      "epoch": 1.948888888888889,
      "grad_norm": 0.14316126704216003,
      "learning_rate": 7.020740740740741e-05,
      "loss": 4.8056,
      "step": 8770
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 0.0458902083337307,
      "learning_rate": 7.005925925925926e-05,
      "loss": 4.7905,
      "step": 8780
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.051533911377191544,
      "learning_rate": 6.991111111111112e-05,
      "loss": 4.7701,
      "step": 8790
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.015837382525205612,
      "learning_rate": 6.976296296296296e-05,
      "loss": 4.8007,
      "step": 8800
    },
    {
      "epoch": 1.9577777777777778,
      "grad_norm": 0.033586494624614716,
      "learning_rate": 6.961481481481482e-05,
      "loss": 4.7614,
      "step": 8810
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.0185445137321949,
      "learning_rate": 6.946666666666667e-05,
      "loss": 4.7705,
      "step": 8820
    },
    {
      "epoch": 1.962222222222222,
      "grad_norm": 0.06500991433858871,
      "learning_rate": 6.931851851851853e-05,
      "loss": 4.8442,
      "step": 8830
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 0.02879033051431179,
      "learning_rate": 6.917037037037038e-05,
      "loss": 4.8123,
      "step": 8840
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.021390490233898163,
      "learning_rate": 6.902222222222222e-05,
      "loss": 4.79,
      "step": 8850
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 0.043508633971214294,
      "learning_rate": 6.887407407407408e-05,
      "loss": 4.754,
      "step": 8860
    },
    {
      "epoch": 1.971111111111111,
      "grad_norm": 0.019054541364312172,
      "learning_rate": 6.872592592592593e-05,
      "loss": 4.7716,
      "step": 8870
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.021622782573103905,
      "learning_rate": 6.857777777777779e-05,
      "loss": 4.769,
      "step": 8880
    },
    {
      "epoch": 1.9755555555555555,
      "grad_norm": 0.013371851295232773,
      "learning_rate": 6.842962962962963e-05,
      "loss": 4.7623,
      "step": 8890
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.022389763966202736,
      "learning_rate": 6.828148148148148e-05,
      "loss": 4.7879,
      "step": 8900
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.054806675761938095,
      "learning_rate": 6.813333333333334e-05,
      "loss": 4.7875,
      "step": 8910
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 0.024656830355525017,
      "learning_rate": 6.798518518518519e-05,
      "loss": 4.7947,
      "step": 8920
    },
    {
      "epoch": 1.9844444444444445,
      "grad_norm": 0.028606893494725227,
      "learning_rate": 6.783703703703705e-05,
      "loss": 4.79,
      "step": 8930
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.016429323703050613,
      "learning_rate": 6.768888888888889e-05,
      "loss": 4.8743,
      "step": 8940
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.01861606165766716,
      "learning_rate": 6.754074074074075e-05,
      "loss": 4.7952,
      "step": 8950
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 0.09242598712444305,
      "learning_rate": 6.73925925925926e-05,
      "loss": 4.8072,
      "step": 8960
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.051094572991132736,
      "learning_rate": 6.724444444444445e-05,
      "loss": 4.7931,
      "step": 8970
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.020190926268696785,
      "learning_rate": 6.70962962962963e-05,
      "loss": 4.7874,
      "step": 8980
    },
    {
      "epoch": 1.9977777777777779,
      "grad_norm": 0.053774408996105194,
      "learning_rate": 6.694814814814815e-05,
      "loss": 4.8005,
      "step": 8990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.03758550435304642,
      "learning_rate": 6.680000000000001e-05,
      "loss": 4.8181,
      "step": 9000
    },
    {
      "epoch": 2.002222222222222,
      "grad_norm": 0.040715429931879044,
      "learning_rate": 6.665185185185186e-05,
      "loss": 4.8008,
      "step": 9010
    },
    {
      "epoch": 2.0044444444444443,
      "grad_norm": 0.050143636763095856,
      "learning_rate": 6.65037037037037e-05,
      "loss": 4.747,
      "step": 9020
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.04301682859659195,
      "learning_rate": 6.635555555555556e-05,
      "loss": 4.7595,
      "step": 9030
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 0.022336725145578384,
      "learning_rate": 6.620740740740741e-05,
      "loss": 4.7355,
      "step": 9040
    },
    {
      "epoch": 2.011111111111111,
      "grad_norm": 0.03539406135678291,
      "learning_rate": 6.605925925925927e-05,
      "loss": 4.7699,
      "step": 9050
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.03802209720015526,
      "learning_rate": 6.591111111111112e-05,
      "loss": 4.7727,
      "step": 9060
    },
    {
      "epoch": 2.0155555555555558,
      "grad_norm": 0.052456844598054886,
      "learning_rate": 6.576296296296298e-05,
      "loss": 4.7892,
      "step": 9070
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.011170719750225544,
      "learning_rate": 6.561481481481482e-05,
      "loss": 4.8284,
      "step": 9080
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.017797918990254402,
      "learning_rate": 6.546666666666667e-05,
      "loss": 4.805,
      "step": 9090
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 0.05990951880812645,
      "learning_rate": 6.531851851851853e-05,
      "loss": 4.7453,
      "step": 9100
    },
    {
      "epoch": 2.0244444444444443,
      "grad_norm": 0.014071709476411343,
      "learning_rate": 6.517037037037037e-05,
      "loss": 4.8008,
      "step": 9110
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.05951762571930885,
      "learning_rate": 6.502222222222223e-05,
      "loss": 4.7987,
      "step": 9120
    },
    {
      "epoch": 2.028888888888889,
      "grad_norm": 0.030132809653878212,
      "learning_rate": 6.487407407407408e-05,
      "loss": 4.7983,
      "step": 9130
    },
    {
      "epoch": 2.031111111111111,
      "grad_norm": 0.05675189942121506,
      "learning_rate": 6.472592592592594e-05,
      "loss": 4.7766,
      "step": 9140
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.017705794423818588,
      "learning_rate": 6.457777777777779e-05,
      "loss": 4.804,
      "step": 9150
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 0.012458106502890587,
      "learning_rate": 6.442962962962963e-05,
      "loss": 4.7613,
      "step": 9160
    },
    {
      "epoch": 2.037777777777778,
      "grad_norm": 0.31176915764808655,
      "learning_rate": 6.428148148148149e-05,
      "loss": 4.7118,
      "step": 9170
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.028072074055671692,
      "learning_rate": 6.413333333333334e-05,
      "loss": 4.8168,
      "step": 9180
    },
    {
      "epoch": 2.042222222222222,
      "grad_norm": 0.03184431046247482,
      "learning_rate": 6.39851851851852e-05,
      "loss": 4.787,
      "step": 9190
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 1.1701029539108276,
      "learning_rate": 6.383703703703704e-05,
      "loss": 4.8746,
      "step": 9200
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.0668109580874443,
      "learning_rate": 6.368888888888889e-05,
      "loss": 4.8102,
      "step": 9210
    },
    {
      "epoch": 2.048888888888889,
      "grad_norm": 0.0515754260122776,
      "learning_rate": 6.354074074074075e-05,
      "loss": 4.8095,
      "step": 9220
    },
    {
      "epoch": 2.051111111111111,
      "grad_norm": 0.03130165860056877,
      "learning_rate": 6.33925925925926e-05,
      "loss": 4.783,
      "step": 9230
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.028630081564188004,
      "learning_rate": 6.324444444444446e-05,
      "loss": 4.7719,
      "step": 9240
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.12255147099494934,
      "learning_rate": 6.30962962962963e-05,
      "loss": 4.7556,
      "step": 9250
    },
    {
      "epoch": 2.057777777777778,
      "grad_norm": 0.04065941646695137,
      "learning_rate": 6.294814814814816e-05,
      "loss": 4.8193,
      "step": 9260
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.020923912525177002,
      "learning_rate": 6.280000000000001e-05,
      "loss": 4.779,
      "step": 9270
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 0.04312373697757721,
      "learning_rate": 6.265185185185185e-05,
      "loss": 4.809,
      "step": 9280
    },
    {
      "epoch": 2.0644444444444443,
      "grad_norm": 0.015694843605160713,
      "learning_rate": 6.250370370370371e-05,
      "loss": 4.7846,
      "step": 9290
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.05148002877831459,
      "learning_rate": 6.235555555555556e-05,
      "loss": 4.7981,
      "step": 9300
    },
    {
      "epoch": 2.068888888888889,
      "grad_norm": 0.04765152558684349,
      "learning_rate": 6.220740740740742e-05,
      "loss": 4.7574,
      "step": 9310
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.02242053672671318,
      "learning_rate": 6.205925925925927e-05,
      "loss": 4.788,
      "step": 9320
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.0290693286806345,
      "learning_rate": 6.191111111111111e-05,
      "loss": 4.7853,
      "step": 9330
    },
    {
      "epoch": 2.0755555555555554,
      "grad_norm": 0.015820350497961044,
      "learning_rate": 6.176296296296297e-05,
      "loss": 4.7984,
      "step": 9340
    },
    {
      "epoch": 2.077777777777778,
      "grad_norm": 0.031221313402056694,
      "learning_rate": 6.161481481481482e-05,
      "loss": 4.7957,
      "step": 9350
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.051107458770275116,
      "learning_rate": 6.146666666666668e-05,
      "loss": 4.7843,
      "step": 9360
    },
    {
      "epoch": 2.082222222222222,
      "grad_norm": 0.0136928865686059,
      "learning_rate": 6.131851851851853e-05,
      "loss": 4.8173,
      "step": 9370
    },
    {
      "epoch": 2.0844444444444443,
      "grad_norm": 0.022198719903826714,
      "learning_rate": 6.118518518518518e-05,
      "loss": 4.8278,
      "step": 9380
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.03693699464201927,
      "learning_rate": 6.103703703703703e-05,
      "loss": 4.74,
      "step": 9390
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.05300689488649368,
      "learning_rate": 6.08888888888889e-05,
      "loss": 4.7974,
      "step": 9400
    },
    {
      "epoch": 2.091111111111111,
      "grad_norm": 0.05103786289691925,
      "learning_rate": 6.074074074074074e-05,
      "loss": 4.7858,
      "step": 9410
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.05127611756324768,
      "learning_rate": 6.05925925925926e-05,
      "loss": 4.8447,
      "step": 9420
    },
    {
      "epoch": 2.0955555555555554,
      "grad_norm": 0.013466172851622105,
      "learning_rate": 6.044444444444445e-05,
      "loss": 4.7397,
      "step": 9430
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 0.020388288423419,
      "learning_rate": 6.0296296296296295e-05,
      "loss": 4.7777,
      "step": 9440
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.24282987415790558,
      "learning_rate": 6.0148148148148155e-05,
      "loss": 4.8165,
      "step": 9450
    },
    {
      "epoch": 2.102222222222222,
      "grad_norm": 0.014945239759981632,
      "learning_rate": 6e-05,
      "loss": 4.8348,
      "step": 9460
    },
    {
      "epoch": 2.1044444444444443,
      "grad_norm": 0.07525017857551575,
      "learning_rate": 5.985185185185186e-05,
      "loss": 4.7796,
      "step": 9470
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.02372501976788044,
      "learning_rate": 5.970370370370371e-05,
      "loss": 4.8007,
      "step": 9480
    },
    {
      "epoch": 2.108888888888889,
      "grad_norm": 0.036788273602724075,
      "learning_rate": 5.9555555555555554e-05,
      "loss": 4.7895,
      "step": 9490
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.04653805494308472,
      "learning_rate": 5.9407407407407414e-05,
      "loss": 4.8592,
      "step": 9500
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.0359809547662735,
      "learning_rate": 5.925925925925926e-05,
      "loss": 4.8266,
      "step": 9510
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 0.07393533736467361,
      "learning_rate": 5.911111111111112e-05,
      "loss": 4.7756,
      "step": 9520
    },
    {
      "epoch": 2.117777777777778,
      "grad_norm": 0.02236444130539894,
      "learning_rate": 5.8962962962962966e-05,
      "loss": 4.8599,
      "step": 9530
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.05884178355336189,
      "learning_rate": 5.8814814814814826e-05,
      "loss": 4.7993,
      "step": 9540
    },
    {
      "epoch": 2.1222222222222222,
      "grad_norm": 0.02611437253654003,
      "learning_rate": 5.866666666666667e-05,
      "loss": 4.7626,
      "step": 9550
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 0.031387630850076675,
      "learning_rate": 5.851851851851852e-05,
      "loss": 4.7735,
      "step": 9560
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.035355910658836365,
      "learning_rate": 5.837037037037038e-05,
      "loss": 4.8329,
      "step": 9570
    },
    {
      "epoch": 2.128888888888889,
      "grad_norm": 0.2243671864271164,
      "learning_rate": 5.8222222222222224e-05,
      "loss": 4.7827,
      "step": 9580
    },
    {
      "epoch": 2.131111111111111,
      "grad_norm": 0.012910086661577225,
      "learning_rate": 5.8074074074074084e-05,
      "loss": 4.7958,
      "step": 9590
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.02117615006864071,
      "learning_rate": 5.792592592592593e-05,
      "loss": 4.7826,
      "step": 9600
    },
    {
      "epoch": 2.1355555555555554,
      "grad_norm": 0.022952664643526077,
      "learning_rate": 5.7777777777777776e-05,
      "loss": 4.7846,
      "step": 9610
    },
    {
      "epoch": 2.137777777777778,
      "grad_norm": 0.035221174359321594,
      "learning_rate": 5.7629629629629636e-05,
      "loss": 4.7093,
      "step": 9620
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.47330284118652344,
      "learning_rate": 5.748148148148148e-05,
      "loss": 4.825,
      "step": 9630
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 0.014688712544739246,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 4.8135,
      "step": 9640
    },
    {
      "epoch": 2.1444444444444444,
      "grad_norm": 0.05367036163806915,
      "learning_rate": 5.718518518518519e-05,
      "loss": 4.7894,
      "step": 9650
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.06257419288158417,
      "learning_rate": 5.703703703703704e-05,
      "loss": 4.7422,
      "step": 9660
    },
    {
      "epoch": 2.148888888888889,
      "grad_norm": 0.04669830575585365,
      "learning_rate": 5.6888888888888895e-05,
      "loss": 4.7902,
      "step": 9670
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 0.01625998318195343,
      "learning_rate": 5.674074074074074e-05,
      "loss": 4.7774,
      "step": 9680
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.04827247932553291,
      "learning_rate": 5.6592592592592594e-05,
      "loss": 4.7806,
      "step": 9690
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 0.014131520874798298,
      "learning_rate": 5.644444444444445e-05,
      "loss": 4.7971,
      "step": 9700
    },
    {
      "epoch": 2.1577777777777776,
      "grad_norm": 0.04281817004084587,
      "learning_rate": 5.62962962962963e-05,
      "loss": 4.8146,
      "step": 9710
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.03720099851489067,
      "learning_rate": 5.614814814814815e-05,
      "loss": 4.8096,
      "step": 9720
    },
    {
      "epoch": 2.1622222222222223,
      "grad_norm": 0.06768249720335007,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 4.7744,
      "step": 9730
    },
    {
      "epoch": 2.1644444444444444,
      "grad_norm": 0.029525630176067352,
      "learning_rate": 5.585185185185185e-05,
      "loss": 4.8052,
      "step": 9740
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.04338464513421059,
      "learning_rate": 5.5703703703703705e-05,
      "loss": 4.8039,
      "step": 9750
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 0.031337037682533264,
      "learning_rate": 5.555555555555556e-05,
      "loss": 4.7526,
      "step": 9760
    },
    {
      "epoch": 2.171111111111111,
      "grad_norm": 0.01488360296934843,
      "learning_rate": 5.540740740740741e-05,
      "loss": 4.7882,
      "step": 9770
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.048729926347732544,
      "learning_rate": 5.5259259259259264e-05,
      "loss": 4.8534,
      "step": 9780
    },
    {
      "epoch": 2.1755555555555555,
      "grad_norm": 0.02866036258637905,
      "learning_rate": 5.511111111111111e-05,
      "loss": 4.7621,
      "step": 9790
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.04010368511080742,
      "learning_rate": 5.4962962962962964e-05,
      "loss": 4.8125,
      "step": 9800
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.042873986065387726,
      "learning_rate": 5.4814814814814817e-05,
      "loss": 4.813,
      "step": 9810
    },
    {
      "epoch": 2.1822222222222223,
      "grad_norm": 0.060320742428302765,
      "learning_rate": 5.466666666666666e-05,
      "loss": 4.8032,
      "step": 9820
    },
    {
      "epoch": 2.1844444444444444,
      "grad_norm": 0.04066314548254013,
      "learning_rate": 5.451851851851852e-05,
      "loss": 4.81,
      "step": 9830
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.023517146706581116,
      "learning_rate": 5.437037037037037e-05,
      "loss": 4.8102,
      "step": 9840
    },
    {
      "epoch": 2.188888888888889,
      "grad_norm": 0.021769143640995026,
      "learning_rate": 5.422222222222223e-05,
      "loss": 4.8068,
      "step": 9850
    },
    {
      "epoch": 2.1911111111111112,
      "grad_norm": 0.014032172970473766,
      "learning_rate": 5.4074074074074075e-05,
      "loss": 4.8526,
      "step": 9860
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.05489742383360863,
      "learning_rate": 5.392592592592592e-05,
      "loss": 4.7823,
      "step": 9870
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 0.060008689761161804,
      "learning_rate": 5.377777777777778e-05,
      "loss": 4.7474,
      "step": 9880
    },
    {
      "epoch": 2.1977777777777776,
      "grad_norm": 0.03440314531326294,
      "learning_rate": 5.362962962962963e-05,
      "loss": 4.792,
      "step": 9890
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.015018332749605179,
      "learning_rate": 5.348148148148149e-05,
      "loss": 4.7623,
      "step": 9900
    },
    {
      "epoch": 2.2022222222222223,
      "grad_norm": 0.049169424921274185,
      "learning_rate": 5.333333333333333e-05,
      "loss": 4.7748,
      "step": 9910
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 0.04109453782439232,
      "learning_rate": 5.318518518518518e-05,
      "loss": 4.8002,
      "step": 9920
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.036721549928188324,
      "learning_rate": 5.303703703703704e-05,
      "loss": 4.7534,
      "step": 9930
    },
    {
      "epoch": 2.2088888888888887,
      "grad_norm": 0.04455844685435295,
      "learning_rate": 5.2888888888888885e-05,
      "loss": 4.8154,
      "step": 9940
    },
    {
      "epoch": 2.2111111111111112,
      "grad_norm": 0.01640467345714569,
      "learning_rate": 5.2740740740740745e-05,
      "loss": 4.7617,
      "step": 9950
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.015398278832435608,
      "learning_rate": 5.259259259259259e-05,
      "loss": 4.7389,
      "step": 9960
    },
    {
      "epoch": 2.2155555555555555,
      "grad_norm": 0.01759389601647854,
      "learning_rate": 5.244444444444445e-05,
      "loss": 4.8055,
      "step": 9970
    },
    {
      "epoch": 2.2177777777777776,
      "grad_norm": 0.020790282636880875,
      "learning_rate": 5.22962962962963e-05,
      "loss": 4.7925,
      "step": 9980
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.020207319408655167,
      "learning_rate": 5.2148148148148144e-05,
      "loss": 4.8184,
      "step": 9990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.02091982588171959,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 4.7768,
      "step": 10000
    },
    {
      "epoch": 2.2244444444444444,
      "grad_norm": 0.276771605014801,
      "learning_rate": 5.185185185185185e-05,
      "loss": 4.8024,
      "step": 10010
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.01832306943833828,
      "learning_rate": 5.170370370370371e-05,
      "loss": 4.7507,
      "step": 10020
    },
    {
      "epoch": 2.2288888888888887,
      "grad_norm": 0.3621026575565338,
      "learning_rate": 5.1555555555555556e-05,
      "loss": 4.8222,
      "step": 10030
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 0.026617946103215218,
      "learning_rate": 5.1407407407407416e-05,
      "loss": 4.7425,
      "step": 10040
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.01609373278915882,
      "learning_rate": 5.125925925925926e-05,
      "loss": 4.82,
      "step": 10050
    },
    {
      "epoch": 2.2355555555555555,
      "grad_norm": 0.014415598474442959,
      "learning_rate": 5.111111111111111e-05,
      "loss": 4.7488,
      "step": 10060
    },
    {
      "epoch": 2.2377777777777776,
      "grad_norm": 0.052069056779146194,
      "learning_rate": 5.096296296296297e-05,
      "loss": 4.7752,
      "step": 10070
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.017133114859461784,
      "learning_rate": 5.0814814814814814e-05,
      "loss": 4.7915,
      "step": 10080
    },
    {
      "epoch": 2.2422222222222223,
      "grad_norm": 0.025588613003492355,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 4.7696,
      "step": 10090
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 0.07898721843957901,
      "learning_rate": 5.051851851851852e-05,
      "loss": 4.7787,
      "step": 10100
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.016442980617284775,
      "learning_rate": 5.0370370370370366e-05,
      "loss": 4.7982,
      "step": 10110
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.036520976573228836,
      "learning_rate": 5.0222222222222226e-05,
      "loss": 4.7832,
      "step": 10120
    },
    {
      "epoch": 2.2511111111111113,
      "grad_norm": 0.03210611641407013,
      "learning_rate": 5.007407407407407e-05,
      "loss": 4.7735,
      "step": 10130
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.030380187556147575,
      "learning_rate": 4.9925925925925926e-05,
      "loss": 4.831,
      "step": 10140
    },
    {
      "epoch": 2.2555555555555555,
      "grad_norm": 0.03719385340809822,
      "learning_rate": 4.977777777777778e-05,
      "loss": 4.7601,
      "step": 10150
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 0.02139943838119507,
      "learning_rate": 4.962962962962963e-05,
      "loss": 4.786,
      "step": 10160
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.050060518085956573,
      "learning_rate": 4.9481481481481485e-05,
      "loss": 4.7778,
      "step": 10170
    },
    {
      "epoch": 2.2622222222222224,
      "grad_norm": 0.03096412681043148,
      "learning_rate": 4.933333333333334e-05,
      "loss": 4.8248,
      "step": 10180
    },
    {
      "epoch": 2.2644444444444445,
      "grad_norm": 65.67086791992188,
      "learning_rate": 4.918518518518519e-05,
      "loss": 4.8171,
      "step": 10190
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.03469786047935486,
      "learning_rate": 4.903703703703704e-05,
      "loss": 4.8021,
      "step": 10200
    },
    {
      "epoch": 2.2688888888888887,
      "grad_norm": 0.016254806891083717,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.7853,
      "step": 10210
    },
    {
      "epoch": 2.2711111111111113,
      "grad_norm": 0.019916515797376633,
      "learning_rate": 4.874074074074074e-05,
      "loss": 4.7469,
      "step": 10220
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.021258307620882988,
      "learning_rate": 4.8592592592592596e-05,
      "loss": 4.8001,
      "step": 10230
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 0.02489163540303707,
      "learning_rate": 4.844444444444445e-05,
      "loss": 4.7549,
      "step": 10240
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.026360321789979935,
      "learning_rate": 4.82962962962963e-05,
      "loss": 4.7905,
      "step": 10250
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.031547244638204575,
      "learning_rate": 4.814814814814815e-05,
      "loss": 4.8131,
      "step": 10260
    },
    {
      "epoch": 2.2822222222222224,
      "grad_norm": 0.025347208604216576,
      "learning_rate": 4.8e-05,
      "loss": 4.7778,
      "step": 10270
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 0.014189189299941063,
      "learning_rate": 4.7851851851851854e-05,
      "loss": 4.7635,
      "step": 10280
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 3.2206740379333496,
      "learning_rate": 4.770370370370371e-05,
      "loss": 4.8012,
      "step": 10290
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 0.031439244747161865,
      "learning_rate": 4.755555555555556e-05,
      "loss": 4.7895,
      "step": 10300
    },
    {
      "epoch": 2.2911111111111113,
      "grad_norm": 0.012913122773170471,
      "learning_rate": 4.740740740740741e-05,
      "loss": 4.7563,
      "step": 10310
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.01924247108399868,
      "learning_rate": 4.7259259259259266e-05,
      "loss": 4.7717,
      "step": 10320
    },
    {
      "epoch": 2.2955555555555556,
      "grad_norm": 0.24491901695728302,
      "learning_rate": 4.711111111111111e-05,
      "loss": 4.7615,
      "step": 10330
    },
    {
      "epoch": 2.2977777777777777,
      "grad_norm": 0.023230770602822304,
      "learning_rate": 4.6962962962962966e-05,
      "loss": 4.7732,
      "step": 10340
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.02004755102097988,
      "learning_rate": 4.681481481481482e-05,
      "loss": 4.7697,
      "step": 10350
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 0.361701637506485,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.7871,
      "step": 10360
    },
    {
      "epoch": 2.3044444444444445,
      "grad_norm": 0.04533567652106285,
      "learning_rate": 4.6518518518518525e-05,
      "loss": 4.7997,
      "step": 10370
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.021753473207354546,
      "learning_rate": 4.637037037037038e-05,
      "loss": 4.7527,
      "step": 10380
    },
    {
      "epoch": 2.3088888888888888,
      "grad_norm": 0.6769200563430786,
      "learning_rate": 4.6222222222222224e-05,
      "loss": 4.7753,
      "step": 10390
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.01842132769525051,
      "learning_rate": 4.607407407407408e-05,
      "loss": 4.7696,
      "step": 10400
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.037546440958976746,
      "learning_rate": 4.592592592592593e-05,
      "loss": 4.7868,
      "step": 10410
    },
    {
      "epoch": 2.3155555555555556,
      "grad_norm": 0.027904553338885307,
      "learning_rate": 4.577777777777778e-05,
      "loss": 4.8097,
      "step": 10420
    },
    {
      "epoch": 2.3177777777777777,
      "grad_norm": 1.2873908281326294,
      "learning_rate": 4.5629629629629636e-05,
      "loss": 4.8084,
      "step": 10430
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.013527292758226395,
      "learning_rate": 4.548148148148149e-05,
      "loss": 4.7651,
      "step": 10440
    },
    {
      "epoch": 2.3222222222222224,
      "grad_norm": 0.19015707075595856,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 4.7428,
      "step": 10450
    },
    {
      "epoch": 2.3244444444444445,
      "grad_norm": 0.26344725489616394,
      "learning_rate": 4.518518518518519e-05,
      "loss": 4.8002,
      "step": 10460
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.011766313575208187,
      "learning_rate": 4.503703703703704e-05,
      "loss": 4.7504,
      "step": 10470
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 0.02755844220519066,
      "learning_rate": 4.4888888888888894e-05,
      "loss": 4.7762,
      "step": 10480
    },
    {
      "epoch": 2.3311111111111114,
      "grad_norm": 0.017741218209266663,
      "learning_rate": 4.474074074074075e-05,
      "loss": 4.7781,
      "step": 10490
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.038955871015787125,
      "learning_rate": 4.4592592592592594e-05,
      "loss": 4.7727,
      "step": 10500
    },
    {
      "epoch": 2.3355555555555556,
      "grad_norm": 0.017413202673196793,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 4.7841,
      "step": 10510
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 0.020466728135943413,
      "learning_rate": 4.42962962962963e-05,
      "loss": 4.7638,
      "step": 10520
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.01396955270320177,
      "learning_rate": 4.414814814814815e-05,
      "loss": 4.811,
      "step": 10530
    },
    {
      "epoch": 2.3422222222222224,
      "grad_norm": 0.03402962535619736,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 4.7978,
      "step": 10540
    },
    {
      "epoch": 2.3444444444444446,
      "grad_norm": 0.022206278517842293,
      "learning_rate": 4.385185185185185e-05,
      "loss": 4.7705,
      "step": 10550
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.0448383092880249,
      "learning_rate": 4.3703703703703705e-05,
      "loss": 4.7799,
      "step": 10560
    },
    {
      "epoch": 2.348888888888889,
      "grad_norm": 0.2503483295440674,
      "learning_rate": 4.355555555555556e-05,
      "loss": 4.8482,
      "step": 10570
    },
    {
      "epoch": 2.351111111111111,
      "grad_norm": 0.05589409917593002,
      "learning_rate": 4.340740740740741e-05,
      "loss": 4.8085,
      "step": 10580
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.038423191756010056,
      "learning_rate": 4.325925925925926e-05,
      "loss": 4.8218,
      "step": 10590
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.05219445005059242,
      "learning_rate": 4.311111111111111e-05,
      "loss": 4.7564,
      "step": 10600
    },
    {
      "epoch": 2.3577777777777778,
      "grad_norm": 2.663262367248535,
      "learning_rate": 4.296296296296296e-05,
      "loss": 4.9049,
      "step": 10610
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.019053971394896507,
      "learning_rate": 4.2814814814814816e-05,
      "loss": 4.7606,
      "step": 10620
    },
    {
      "epoch": 2.362222222222222,
      "grad_norm": 0.01613202504813671,
      "learning_rate": 4.266666666666667e-05,
      "loss": 4.7765,
      "step": 10630
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 0.033707503229379654,
      "learning_rate": 4.2518518518518515e-05,
      "loss": 4.7885,
      "step": 10640
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.05086464062333107,
      "learning_rate": 4.237037037037037e-05,
      "loss": 4.7527,
      "step": 10650
    },
    {
      "epoch": 2.368888888888889,
      "grad_norm": 0.2561078667640686,
      "learning_rate": 4.222222222222222e-05,
      "loss": 4.7784,
      "step": 10660
    },
    {
      "epoch": 2.371111111111111,
      "grad_norm": 0.04228660464286804,
      "learning_rate": 4.2074074074074075e-05,
      "loss": 4.7328,
      "step": 10670
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.053465865552425385,
      "learning_rate": 4.192592592592593e-05,
      "loss": 4.76,
      "step": 10680
    },
    {
      "epoch": 2.3755555555555556,
      "grad_norm": 0.18788626790046692,
      "learning_rate": 4.177777777777778e-05,
      "loss": 4.7584,
      "step": 10690
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.034596037119627,
      "learning_rate": 4.162962962962963e-05,
      "loss": 4.7631,
      "step": 10700
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.02251489646732807,
      "learning_rate": 4.148148148148148e-05,
      "loss": 4.8029,
      "step": 10710
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 0.017199352383613586,
      "learning_rate": 4.133333333333333e-05,
      "loss": 4.7999,
      "step": 10720
    },
    {
      "epoch": 2.3844444444444446,
      "grad_norm": 0.02564406394958496,
      "learning_rate": 4.1185185185185186e-05,
      "loss": 4.7953,
      "step": 10730
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.013216948136687279,
      "learning_rate": 4.103703703703704e-05,
      "loss": 4.8029,
      "step": 10740
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.03799864277243614,
      "learning_rate": 4.088888888888889e-05,
      "loss": 4.7867,
      "step": 10750
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 0.030319899320602417,
      "learning_rate": 4.074074074074074e-05,
      "loss": 4.7913,
      "step": 10760
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.052818845957517624,
      "learning_rate": 4.059259259259259e-05,
      "loss": 4.7875,
      "step": 10770
    },
    {
      "epoch": 2.3955555555555557,
      "grad_norm": 0.02007259428501129,
      "learning_rate": 4.0444444444444444e-05,
      "loss": 4.7895,
      "step": 10780
    },
    {
      "epoch": 2.397777777777778,
      "grad_norm": 0.05342024192214012,
      "learning_rate": 4.02962962962963e-05,
      "loss": 4.7914,
      "step": 10790
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.03298693150281906,
      "learning_rate": 4.014814814814815e-05,
      "loss": 4.7911,
      "step": 10800
    },
    {
      "epoch": 2.402222222222222,
      "grad_norm": 0.04991075396537781,
      "learning_rate": 4e-05,
      "loss": 4.8171,
      "step": 10810
    },
    {
      "epoch": 2.4044444444444446,
      "grad_norm": 0.02188926376402378,
      "learning_rate": 3.985185185185185e-05,
      "loss": 4.7821,
      "step": 10820
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.04536442458629608,
      "learning_rate": 3.97037037037037e-05,
      "loss": 4.8475,
      "step": 10830
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 0.06071795895695686,
      "learning_rate": 3.9555555555555556e-05,
      "loss": 4.8156,
      "step": 10840
    },
    {
      "epoch": 2.411111111111111,
      "grad_norm": 0.05269455164670944,
      "learning_rate": 3.940740740740741e-05,
      "loss": 4.8131,
      "step": 10850
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.04552660137414932,
      "learning_rate": 3.925925925925926e-05,
      "loss": 4.7783,
      "step": 10860
    },
    {
      "epoch": 2.4155555555555557,
      "grad_norm": 0.02638457715511322,
      "learning_rate": 3.9111111111111115e-05,
      "loss": 4.7499,
      "step": 10870
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 0.0437808521091938,
      "learning_rate": 3.896296296296296e-05,
      "loss": 4.7507,
      "step": 10880
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.016851741820573807,
      "learning_rate": 3.8814814814814814e-05,
      "loss": 4.7694,
      "step": 10890
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 0.04777079075574875,
      "learning_rate": 3.866666666666667e-05,
      "loss": 4.822,
      "step": 10900
    },
    {
      "epoch": 2.4244444444444446,
      "grad_norm": 0.02475629933178425,
      "learning_rate": 3.851851851851852e-05,
      "loss": 4.8225,
      "step": 10910
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.029206933453679085,
      "learning_rate": 3.837037037037037e-05,
      "loss": 4.8202,
      "step": 10920
    },
    {
      "epoch": 2.428888888888889,
      "grad_norm": 0.020625321194529533,
      "learning_rate": 3.8222222222222226e-05,
      "loss": 4.7791,
      "step": 10930
    },
    {
      "epoch": 2.431111111111111,
      "grad_norm": 0.03048066608607769,
      "learning_rate": 3.807407407407408e-05,
      "loss": 4.8137,
      "step": 10940
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.021684274077415466,
      "learning_rate": 3.7925925925925925e-05,
      "loss": 4.7654,
      "step": 10950
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 3.660790205001831,
      "learning_rate": 3.777777777777778e-05,
      "loss": 4.8394,
      "step": 10960
    },
    {
      "epoch": 2.437777777777778,
      "grad_norm": 0.04505251348018646,
      "learning_rate": 3.762962962962963e-05,
      "loss": 4.7485,
      "step": 10970
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.0751950740814209,
      "learning_rate": 3.7481481481481484e-05,
      "loss": 4.7766,
      "step": 10980
    },
    {
      "epoch": 2.442222222222222,
      "grad_norm": 0.03122737817466259,
      "learning_rate": 3.733333333333334e-05,
      "loss": 4.7895,
      "step": 10990
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.014772627502679825,
      "learning_rate": 3.718518518518519e-05,
      "loss": 4.7597,
      "step": 11000
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.045823149383068085,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 4.7654,
      "step": 11010
    },
    {
      "epoch": 2.448888888888889,
      "grad_norm": 0.015258202329277992,
      "learning_rate": 3.688888888888889e-05,
      "loss": 4.7658,
      "step": 11020
    },
    {
      "epoch": 2.451111111111111,
      "grad_norm": 0.013897606171667576,
      "learning_rate": 3.674074074074074e-05,
      "loss": 4.7739,
      "step": 11030
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.013761743903160095,
      "learning_rate": 3.6592592592592596e-05,
      "loss": 4.7596,
      "step": 11040
    },
    {
      "epoch": 2.4555555555555557,
      "grad_norm": 0.014555529691278934,
      "learning_rate": 3.644444444444445e-05,
      "loss": 4.7728,
      "step": 11050
    },
    {
      "epoch": 2.457777777777778,
      "grad_norm": 0.046519987285137177,
      "learning_rate": 3.62962962962963e-05,
      "loss": 4.7746,
      "step": 11060
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.04865781217813492,
      "learning_rate": 3.614814814814815e-05,
      "loss": 4.8032,
      "step": 11070
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 0.01994059421122074,
      "learning_rate": 3.6e-05,
      "loss": 4.7464,
      "step": 11080
    },
    {
      "epoch": 2.464444444444444,
      "grad_norm": 0.017329679802060127,
      "learning_rate": 3.5851851851851854e-05,
      "loss": 4.8085,
      "step": 11090
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.01691867969930172,
      "learning_rate": 3.570370370370371e-05,
      "loss": 4.7521,
      "step": 11100
    },
    {
      "epoch": 2.468888888888889,
      "grad_norm": 0.012143258936703205,
      "learning_rate": 3.555555555555556e-05,
      "loss": 4.7768,
      "step": 11110
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 0.05388771742582321,
      "learning_rate": 3.540740740740741e-05,
      "loss": 4.7556,
      "step": 11120
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.04576040431857109,
      "learning_rate": 3.525925925925926e-05,
      "loss": 4.7433,
      "step": 11130
    },
    {
      "epoch": 2.4755555555555557,
      "grad_norm": 0.2989542484283447,
      "learning_rate": 3.511111111111111e-05,
      "loss": 4.7957,
      "step": 11140
    },
    {
      "epoch": 2.477777777777778,
      "grad_norm": 0.039934173226356506,
      "learning_rate": 3.4962962962962965e-05,
      "loss": 4.7585,
      "step": 11150
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.0193475391715765,
      "learning_rate": 3.481481481481482e-05,
      "loss": 4.777,
      "step": 11160
    },
    {
      "epoch": 2.482222222222222,
      "grad_norm": 0.047997500747442245,
      "learning_rate": 3.466666666666667e-05,
      "loss": 4.8007,
      "step": 11170
    },
    {
      "epoch": 2.4844444444444447,
      "grad_norm": 0.048261839896440506,
      "learning_rate": 3.4518518518518524e-05,
      "loss": 4.8087,
      "step": 11180
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.027571743354201317,
      "learning_rate": 3.437037037037037e-05,
      "loss": 4.8535,
      "step": 11190
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.021710995584726334,
      "learning_rate": 3.4222222222222224e-05,
      "loss": 4.7572,
      "step": 11200
    },
    {
      "epoch": 2.491111111111111,
      "grad_norm": 0.025177551433444023,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 4.7727,
      "step": 11210
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.03714282438158989,
      "learning_rate": 3.392592592592593e-05,
      "loss": 4.7908,
      "step": 11220
    },
    {
      "epoch": 2.4955555555555557,
      "grad_norm": 0.03372937813401222,
      "learning_rate": 3.377777777777778e-05,
      "loss": 4.7756,
      "step": 11230
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.02602626383304596,
      "learning_rate": 3.3629629629629636e-05,
      "loss": 4.7542,
      "step": 11240
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.013127782382071018,
      "learning_rate": 3.348148148148148e-05,
      "loss": 4.774,
      "step": 11250
    },
    {
      "epoch": 2.502222222222222,
      "grad_norm": 0.022724859416484833,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.8377,
      "step": 11260
    },
    {
      "epoch": 2.5044444444444443,
      "grad_norm": 0.026578782126307487,
      "learning_rate": 3.318518518518519e-05,
      "loss": 4.7639,
      "step": 11270
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.018658068031072617,
      "learning_rate": 3.303703703703704e-05,
      "loss": 4.7482,
      "step": 11280
    },
    {
      "epoch": 2.508888888888889,
      "grad_norm": 0.03371724486351013,
      "learning_rate": 3.2888888888888894e-05,
      "loss": 4.7471,
      "step": 11290
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.05220164731144905,
      "learning_rate": 3.274074074074075e-05,
      "loss": 4.7315,
      "step": 11300
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.020845912396907806,
      "learning_rate": 3.25925925925926e-05,
      "loss": 4.7896,
      "step": 11310
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 0.03878985345363617,
      "learning_rate": 3.2444444444444446e-05,
      "loss": 4.8364,
      "step": 11320
    },
    {
      "epoch": 2.517777777777778,
      "grad_norm": 0.0111640440300107,
      "learning_rate": 3.22962962962963e-05,
      "loss": 4.7475,
      "step": 11330
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.04649900645017624,
      "learning_rate": 3.214814814814815e-05,
      "loss": 4.7945,
      "step": 11340
    },
    {
      "epoch": 2.522222222222222,
      "grad_norm": 0.029736211523413658,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 4.797,
      "step": 11350
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 0.039193689823150635,
      "learning_rate": 3.185185185185185e-05,
      "loss": 4.7812,
      "step": 11360
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.023053212091326714,
      "learning_rate": 3.1703703703703705e-05,
      "loss": 4.7484,
      "step": 11370
    },
    {
      "epoch": 2.528888888888889,
      "grad_norm": 0.020430631935596466,
      "learning_rate": 3.155555555555556e-05,
      "loss": 4.7615,
      "step": 11380
    },
    {
      "epoch": 2.531111111111111,
      "grad_norm": 0.022781934589147568,
      "learning_rate": 3.140740740740741e-05,
      "loss": 4.802,
      "step": 11390
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.09742365032434464,
      "learning_rate": 3.1259259259259264e-05,
      "loss": 4.7395,
      "step": 11400
    },
    {
      "epoch": 2.535555555555556,
      "grad_norm": 0.03158874809741974,
      "learning_rate": 3.111111111111111e-05,
      "loss": 4.8316,
      "step": 11410
    },
    {
      "epoch": 2.537777777777778,
      "grad_norm": 0.01856403239071369,
      "learning_rate": 3.096296296296296e-05,
      "loss": 4.7946,
      "step": 11420
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.05612536892294884,
      "learning_rate": 3.0814814814814816e-05,
      "loss": 4.7428,
      "step": 11430
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 0.011962855234742165,
      "learning_rate": 3.066666666666667e-05,
      "loss": 4.8089,
      "step": 11440
    },
    {
      "epoch": 2.5444444444444443,
      "grad_norm": 0.015550721436738968,
      "learning_rate": 3.0518518518518515e-05,
      "loss": 4.7464,
      "step": 11450
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.03298875689506531,
      "learning_rate": 3.037037037037037e-05,
      "loss": 4.8047,
      "step": 11460
    },
    {
      "epoch": 2.548888888888889,
      "grad_norm": 0.3725041449069977,
      "learning_rate": 3.0222222222222225e-05,
      "loss": 4.7992,
      "step": 11470
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.011649779044091702,
      "learning_rate": 3.0074074074074078e-05,
      "loss": 4.7334,
      "step": 11480
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.05869251489639282,
      "learning_rate": 2.992592592592593e-05,
      "loss": 4.8184,
      "step": 11490
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.023602856323122978,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 4.8444,
      "step": 11500
    },
    {
      "epoch": 2.557777777777778,
      "grad_norm": 0.20917846262454987,
      "learning_rate": 2.962962962962963e-05,
      "loss": 4.7835,
      "step": 11510
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.0249968059360981,
      "learning_rate": 2.9481481481481483e-05,
      "loss": 4.7285,
      "step": 11520
    },
    {
      "epoch": 2.562222222222222,
      "grad_norm": 0.03160659596323967,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 4.7803,
      "step": 11530
    },
    {
      "epoch": 2.5644444444444443,
      "grad_norm": 0.02002646215260029,
      "learning_rate": 2.918518518518519e-05,
      "loss": 4.7745,
      "step": 11540
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.03430097550153732,
      "learning_rate": 2.9037037037037042e-05,
      "loss": 4.7865,
      "step": 11550
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 0.027886416763067245,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 4.7934,
      "step": 11560
    },
    {
      "epoch": 2.571111111111111,
      "grad_norm": 0.052595559507608414,
      "learning_rate": 2.874074074074074e-05,
      "loss": 4.7443,
      "step": 11570
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.04502531886100769,
      "learning_rate": 2.8592592592592594e-05,
      "loss": 4.7954,
      "step": 11580
    },
    {
      "epoch": 2.575555555555556,
      "grad_norm": 0.012844785116612911,
      "learning_rate": 2.8444444444444447e-05,
      "loss": 4.7877,
      "step": 11590
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 0.014972737990319729,
      "learning_rate": 2.8296296296296297e-05,
      "loss": 4.7633,
      "step": 11600
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.026317333802580833,
      "learning_rate": 2.814814814814815e-05,
      "loss": 4.8211,
      "step": 11610
    },
    {
      "epoch": 2.582222222222222,
      "grad_norm": 0.021655060350894928,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 4.7008,
      "step": 11620
    },
    {
      "epoch": 2.5844444444444443,
      "grad_norm": 0.2892467975616455,
      "learning_rate": 2.7851851851851853e-05,
      "loss": 4.8089,
      "step": 11630
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.009945821948349476,
      "learning_rate": 2.7703703703703706e-05,
      "loss": 4.7894,
      "step": 11640
    },
    {
      "epoch": 2.588888888888889,
      "grad_norm": 0.012334652245044708,
      "learning_rate": 2.7555555555555555e-05,
      "loss": 4.7692,
      "step": 11650
    },
    {
      "epoch": 2.591111111111111,
      "grad_norm": 0.035235609859228134,
      "learning_rate": 2.7407407407407408e-05,
      "loss": 4.8104,
      "step": 11660
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.011337969452142715,
      "learning_rate": 2.725925925925926e-05,
      "loss": 4.7149,
      "step": 11670
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 0.03467532619833946,
      "learning_rate": 2.7111111111111114e-05,
      "loss": 4.7625,
      "step": 11680
    },
    {
      "epoch": 2.597777777777778,
      "grad_norm": 0.015682261437177658,
      "learning_rate": 2.696296296296296e-05,
      "loss": 4.7507,
      "step": 11690
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.040877413004636765,
      "learning_rate": 2.6814814814814814e-05,
      "loss": 4.7723,
      "step": 11700
    },
    {
      "epoch": 2.602222222222222,
      "grad_norm": 0.018462032079696655,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 4.7324,
      "step": 11710
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.05611780658364296,
      "learning_rate": 2.651851851851852e-05,
      "loss": 4.8731,
      "step": 11720
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.04787042737007141,
      "learning_rate": 2.6370370370370373e-05,
      "loss": 4.8059,
      "step": 11730
    },
    {
      "epoch": 2.608888888888889,
      "grad_norm": 0.047313474118709564,
      "learning_rate": 2.6222222222222226e-05,
      "loss": 4.8154,
      "step": 11740
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 0.02387346886098385,
      "learning_rate": 2.6074074074074072e-05,
      "loss": 4.7507,
      "step": 11750
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.0357792042195797,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 4.8074,
      "step": 11760
    },
    {
      "epoch": 2.6155555555555554,
      "grad_norm": 0.022681334987282753,
      "learning_rate": 2.5777777777777778e-05,
      "loss": 4.7748,
      "step": 11770
    },
    {
      "epoch": 2.6177777777777775,
      "grad_norm": 0.010937979444861412,
      "learning_rate": 2.562962962962963e-05,
      "loss": 4.7476,
      "step": 11780
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.01404405478388071,
      "learning_rate": 2.5481481481481484e-05,
      "loss": 4.8073,
      "step": 11790
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.01876279152929783,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 4.7832,
      "step": 11800
    },
    {
      "epoch": 2.6244444444444444,
      "grad_norm": 0.03759901598095894,
      "learning_rate": 2.5185185185185183e-05,
      "loss": 4.8113,
      "step": 11810
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.03825812786817551,
      "learning_rate": 2.5037037037037036e-05,
      "loss": 4.822,
      "step": 11820
    },
    {
      "epoch": 2.628888888888889,
      "grad_norm": 0.017154790461063385,
      "learning_rate": 2.488888888888889e-05,
      "loss": 4.8069,
      "step": 11830
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 0.017354179173707962,
      "learning_rate": 2.4740740740740742e-05,
      "loss": 4.7958,
      "step": 11840
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.02828225865960121,
      "learning_rate": 2.4592592592592595e-05,
      "loss": 4.7167,
      "step": 11850
    },
    {
      "epoch": 2.6355555555555554,
      "grad_norm": 0.015078363008797169,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 4.7935,
      "step": 11860
    },
    {
      "epoch": 2.637777777777778,
      "grad_norm": 0.017114948481321335,
      "learning_rate": 2.4296296296296298e-05,
      "loss": 4.7278,
      "step": 11870
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.05071018263697624,
      "learning_rate": 2.414814814814815e-05,
      "loss": 4.8303,
      "step": 11880
    },
    {
      "epoch": 2.6422222222222222,
      "grad_norm": 0.04365075007081032,
      "learning_rate": 2.4e-05,
      "loss": 4.7478,
      "step": 11890
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 0.01613781601190567,
      "learning_rate": 2.3851851851851854e-05,
      "loss": 4.7951,
      "step": 11900
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.04685024544596672,
      "learning_rate": 2.3703703703703707e-05,
      "loss": 4.77,
      "step": 11910
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 0.016051828861236572,
      "learning_rate": 2.3555555555555556e-05,
      "loss": 4.7666,
      "step": 11920
    },
    {
      "epoch": 2.651111111111111,
      "grad_norm": 0.05037212744355202,
      "learning_rate": 2.340740740740741e-05,
      "loss": 4.75,
      "step": 11930
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.02002042531967163,
      "learning_rate": 2.3259259259259262e-05,
      "loss": 4.8584,
      "step": 11940
    },
    {
      "epoch": 2.6555555555555554,
      "grad_norm": 0.019152026623487473,
      "learning_rate": 2.3111111111111112e-05,
      "loss": 4.8219,
      "step": 11950
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.027767369523644447,
      "learning_rate": 2.2962962962962965e-05,
      "loss": 4.7433,
      "step": 11960
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.020220916718244553,
      "learning_rate": 2.2814814814814818e-05,
      "loss": 4.824,
      "step": 11970
    },
    {
      "epoch": 2.6622222222222223,
      "grad_norm": 0.023811424151062965,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 4.8428,
      "step": 11980
    },
    {
      "epoch": 2.6644444444444444,
      "grad_norm": 0.018230507150292397,
      "learning_rate": 2.251851851851852e-05,
      "loss": 4.8307,
      "step": 11990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.01446299348026514,
      "learning_rate": 2.2370370370370374e-05,
      "loss": 4.7203,
      "step": 12000
    },
    {
      "epoch": 2.6688888888888886,
      "grad_norm": 0.05419611185789108,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 4.766,
      "step": 12010
    },
    {
      "epoch": 2.671111111111111,
      "grad_norm": 0.026895325630903244,
      "learning_rate": 2.2074074074074076e-05,
      "loss": 4.7489,
      "step": 12020
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.012505058199167252,
      "learning_rate": 2.1925925925925926e-05,
      "loss": 4.7796,
      "step": 12030
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 0.012103063985705376,
      "learning_rate": 2.177777777777778e-05,
      "loss": 4.8118,
      "step": 12040
    },
    {
      "epoch": 2.677777777777778,
      "grad_norm": 0.02781413309276104,
      "learning_rate": 2.162962962962963e-05,
      "loss": 4.7516,
      "step": 12050
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.02029207907617092,
      "learning_rate": 2.148148148148148e-05,
      "loss": 4.7788,
      "step": 12060
    },
    {
      "epoch": 2.6822222222222223,
      "grad_norm": 0.016324486583471298,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 4.7492,
      "step": 12070
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 0.0415760837495327,
      "learning_rate": 2.1185185185185184e-05,
      "loss": 4.784,
      "step": 12080
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.029654476791620255,
      "learning_rate": 2.1037037037037037e-05,
      "loss": 4.8192,
      "step": 12090
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 0.042572323232889175,
      "learning_rate": 2.088888888888889e-05,
      "loss": 4.7654,
      "step": 12100
    },
    {
      "epoch": 2.6911111111111112,
      "grad_norm": 0.019196083769202232,
      "learning_rate": 2.074074074074074e-05,
      "loss": 4.8004,
      "step": 12110
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.01885063201189041,
      "learning_rate": 2.0592592592592593e-05,
      "loss": 4.7494,
      "step": 12120
    },
    {
      "epoch": 2.6955555555555555,
      "grad_norm": 0.04906611144542694,
      "learning_rate": 2.0444444444444446e-05,
      "loss": 4.79,
      "step": 12130
    },
    {
      "epoch": 2.6977777777777776,
      "grad_norm": 0.030000807717442513,
      "learning_rate": 2.0296296296296296e-05,
      "loss": 4.7757,
      "step": 12140
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.022979026660323143,
      "learning_rate": 2.014814814814815e-05,
      "loss": 4.8307,
      "step": 12150
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 0.01521350722759962,
      "learning_rate": 2e-05,
      "loss": 4.7397,
      "step": 12160
    },
    {
      "epoch": 2.7044444444444444,
      "grad_norm": 0.015277853235602379,
      "learning_rate": 1.985185185185185e-05,
      "loss": 4.74,
      "step": 12170
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.02707783691585064,
      "learning_rate": 1.9703703703703704e-05,
      "loss": 4.7029,
      "step": 12180
    },
    {
      "epoch": 2.7088888888888887,
      "grad_norm": 0.04809034243226051,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 4.7796,
      "step": 12190
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.05079369619488716,
      "learning_rate": 1.9407407407407407e-05,
      "loss": 4.8139,
      "step": 12200
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.03285904601216316,
      "learning_rate": 1.925925925925926e-05,
      "loss": 4.7684,
      "step": 12210
    },
    {
      "epoch": 2.7155555555555555,
      "grad_norm": 0.013238845393061638,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 4.8107,
      "step": 12220
    },
    {
      "epoch": 2.7177777777777776,
      "grad_norm": 0.020993025973439217,
      "learning_rate": 1.8962962962962963e-05,
      "loss": 4.7953,
      "step": 12230
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.016310185194015503,
      "learning_rate": 1.8814814814814816e-05,
      "loss": 4.7915,
      "step": 12240
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.03172897547483444,
      "learning_rate": 1.866666666666667e-05,
      "loss": 4.7958,
      "step": 12250
    },
    {
      "epoch": 2.7244444444444444,
      "grad_norm": 0.07821458578109741,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 4.7881,
      "step": 12260
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.029091591015458107,
      "learning_rate": 1.837037037037037e-05,
      "loss": 4.8428,
      "step": 12270
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.025117194280028343,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 4.7672,
      "step": 12280
    },
    {
      "epoch": 2.7311111111111113,
      "grad_norm": 0.044549837708473206,
      "learning_rate": 1.8074074074074074e-05,
      "loss": 4.8005,
      "step": 12290
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.08396508544683456,
      "learning_rate": 1.7925925925925927e-05,
      "loss": 4.7687,
      "step": 12300
    },
    {
      "epoch": 2.7355555555555555,
      "grad_norm": 0.03281542286276817,
      "learning_rate": 1.777777777777778e-05,
      "loss": 4.7961,
      "step": 12310
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 0.05138647183775902,
      "learning_rate": 1.762962962962963e-05,
      "loss": 4.7624,
      "step": 12320
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.025503408163785934,
      "learning_rate": 1.7481481481481483e-05,
      "loss": 4.745,
      "step": 12330
    },
    {
      "epoch": 2.7422222222222223,
      "grad_norm": 0.017712362110614777,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 4.779,
      "step": 12340
    },
    {
      "epoch": 2.7444444444444445,
      "grad_norm": 0.010526113212108612,
      "learning_rate": 1.7185185185185185e-05,
      "loss": 4.8003,
      "step": 12350
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.0493413545191288,
      "learning_rate": 1.7037037037037038e-05,
      "loss": 4.7469,
      "step": 12360
    },
    {
      "epoch": 2.7488888888888887,
      "grad_norm": 0.0193901676684618,
      "learning_rate": 1.688888888888889e-05,
      "loss": 4.806,
      "step": 12370
    },
    {
      "epoch": 2.7511111111111113,
      "grad_norm": 0.02176443487405777,
      "learning_rate": 1.674074074074074e-05,
      "loss": 4.7195,
      "step": 12380
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.14517006278038025,
      "learning_rate": 1.6592592592592594e-05,
      "loss": 4.7982,
      "step": 12390
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 0.04729590192437172,
      "learning_rate": 1.6444444444444447e-05,
      "loss": 4.7773,
      "step": 12400
    },
    {
      "epoch": 2.7577777777777777,
      "grad_norm": 0.03822440281510353,
      "learning_rate": 1.62962962962963e-05,
      "loss": 4.7668,
      "step": 12410
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.0529300794005394,
      "learning_rate": 1.614814814814815e-05,
      "loss": 4.7723,
      "step": 12420
    },
    {
      "epoch": 2.7622222222222224,
      "grad_norm": 0.04507890343666077,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.7673,
      "step": 12430
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 0.025922399014234543,
      "learning_rate": 1.5851851851851852e-05,
      "loss": 4.8421,
      "step": 12440
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.022258764132857323,
      "learning_rate": 1.5703703703703705e-05,
      "loss": 4.7619,
      "step": 12450
    },
    {
      "epoch": 2.7688888888888887,
      "grad_norm": 0.01263378281146288,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 4.7688,
      "step": 12460
    },
    {
      "epoch": 2.771111111111111,
      "grad_norm": 0.04033989831805229,
      "learning_rate": 1.5407407407407408e-05,
      "loss": 4.7947,
      "step": 12470
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.02409324049949646,
      "learning_rate": 1.5259259259259258e-05,
      "loss": 4.7988,
      "step": 12480
    },
    {
      "epoch": 2.7755555555555556,
      "grad_norm": 0.032761164009571075,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 4.8557,
      "step": 12490
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.01978980377316475,
      "learning_rate": 1.4962962962962965e-05,
      "loss": 4.789,
      "step": 12500
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.05533546954393387,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 4.7823,
      "step": 12510
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.017216220498085022,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 4.7581,
      "step": 12520
    },
    {
      "epoch": 2.7844444444444445,
      "grad_norm": 0.027148770168423653,
      "learning_rate": 1.4518518518518521e-05,
      "loss": 4.7944,
      "step": 12530
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.09001972526311874,
      "learning_rate": 1.437037037037037e-05,
      "loss": 4.8269,
      "step": 12540
    },
    {
      "epoch": 2.7888888888888888,
      "grad_norm": 0.016460910439491272,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 4.7113,
      "step": 12550
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 0.047049108892679214,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 4.7526,
      "step": 12560
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.02034393511712551,
      "learning_rate": 1.3925925925925926e-05,
      "loss": 4.7464,
      "step": 12570
    },
    {
      "epoch": 2.7955555555555556,
      "grad_norm": 0.020037665963172913,
      "learning_rate": 1.3777777777777778e-05,
      "loss": 4.7951,
      "step": 12580
    },
    {
      "epoch": 2.7977777777777777,
      "grad_norm": 0.03649074584245682,
      "learning_rate": 1.362962962962963e-05,
      "loss": 4.7802,
      "step": 12590
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.014531534165143967,
      "learning_rate": 1.348148148148148e-05,
      "loss": 4.7927,
      "step": 12600
    },
    {
      "epoch": 2.8022222222222224,
      "grad_norm": 0.009897004812955856,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 4.7437,
      "step": 12610
    },
    {
      "epoch": 2.8044444444444445,
      "grad_norm": 0.04289824515581131,
      "learning_rate": 1.3185185185185186e-05,
      "loss": 4.823,
      "step": 12620
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.05517866834998131,
      "learning_rate": 1.3037037037037036e-05,
      "loss": 4.7692,
      "step": 12630
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 0.015763336792588234,
      "learning_rate": 1.2888888888888889e-05,
      "loss": 4.7806,
      "step": 12640
    },
    {
      "epoch": 2.811111111111111,
      "grad_norm": 0.015211211517453194,
      "learning_rate": 1.2740740740740742e-05,
      "loss": 4.8275,
      "step": 12650
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.018673771992325783,
      "learning_rate": 1.2592592592592592e-05,
      "loss": 4.8024,
      "step": 12660
    },
    {
      "epoch": 2.8155555555555556,
      "grad_norm": 0.040490590035915375,
      "learning_rate": 1.2444444444444445e-05,
      "loss": 4.8175,
      "step": 12670
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 0.03166000545024872,
      "learning_rate": 1.2296296296296298e-05,
      "loss": 4.821,
      "step": 12680
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.03131434693932533,
      "learning_rate": 1.2148148148148149e-05,
      "loss": 4.7666,
      "step": 12690
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 0.048780351877212524,
      "learning_rate": 1.2e-05,
      "loss": 4.8073,
      "step": 12700
    },
    {
      "epoch": 2.8244444444444445,
      "grad_norm": 0.04315441474318504,
      "learning_rate": 1.1851851851851853e-05,
      "loss": 4.7527,
      "step": 12710
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.34036803245544434,
      "learning_rate": 1.1703703703703705e-05,
      "loss": 4.819,
      "step": 12720
    },
    {
      "epoch": 2.828888888888889,
      "grad_norm": 0.04063940420746803,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 4.8249,
      "step": 12730
    },
    {
      "epoch": 2.8311111111111114,
      "grad_norm": 0.036747146397829056,
      "learning_rate": 1.1407407407407409e-05,
      "loss": 4.7615,
      "step": 12740
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.05579852685332298,
      "learning_rate": 1.125925925925926e-05,
      "loss": 4.7426,
      "step": 12750
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.6294364333152771,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 4.8061,
      "step": 12760
    },
    {
      "epoch": 2.8377777777777777,
      "grad_norm": 0.03145953640341759,
      "learning_rate": 1.0962962962962963e-05,
      "loss": 4.8106,
      "step": 12770
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.02073548547923565,
      "learning_rate": 1.0814814814814814e-05,
      "loss": 4.8275,
      "step": 12780
    },
    {
      "epoch": 2.8422222222222224,
      "grad_norm": 0.023171843960881233,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 4.7809,
      "step": 12790
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.014959245920181274,
      "learning_rate": 1.0518518518518519e-05,
      "loss": 4.8308,
      "step": 12800
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.01865643635392189,
      "learning_rate": 1.037037037037037e-05,
      "loss": 4.7647,
      "step": 12810
    },
    {
      "epoch": 2.848888888888889,
      "grad_norm": 0.5811854600906372,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 4.7522,
      "step": 12820
    },
    {
      "epoch": 2.851111111111111,
      "grad_norm": 0.014920654706656933,
      "learning_rate": 1.0088888888888889e-05,
      "loss": 4.7875,
      "step": 12830
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.039362017065286636,
      "learning_rate": 9.94074074074074e-06,
      "loss": 4.8207,
      "step": 12840
    },
    {
      "epoch": 2.8555555555555556,
      "grad_norm": 0.01197381503880024,
      "learning_rate": 9.792592592592593e-06,
      "loss": 4.8289,
      "step": 12850
    },
    {
      "epoch": 2.8577777777777778,
      "grad_norm": 0.021929461508989334,
      "learning_rate": 9.644444444444444e-06,
      "loss": 4.7646,
      "step": 12860
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.05947815254330635,
      "learning_rate": 9.496296296296296e-06,
      "loss": 4.7952,
      "step": 12870
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 0.05177685618400574,
      "learning_rate": 9.348148148148149e-06,
      "loss": 4.7747,
      "step": 12880
    },
    {
      "epoch": 2.8644444444444446,
      "grad_norm": 0.03332027792930603,
      "learning_rate": 9.2e-06,
      "loss": 4.7126,
      "step": 12890
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.022729307413101196,
      "learning_rate": 9.051851851851851e-06,
      "loss": 4.7324,
      "step": 12900
    },
    {
      "epoch": 2.868888888888889,
      "grad_norm": 0.04244235157966614,
      "learning_rate": 8.903703703703704e-06,
      "loss": 4.8278,
      "step": 12910
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 0.03598755598068237,
      "learning_rate": 8.755555555555556e-06,
      "loss": 4.7987,
      "step": 12920
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.04024863243103027,
      "learning_rate": 8.607407407407409e-06,
      "loss": 4.8159,
      "step": 12930
    },
    {
      "epoch": 2.8755555555555556,
      "grad_norm": 0.017769429832696915,
      "learning_rate": 8.45925925925926e-06,
      "loss": 4.7675,
      "step": 12940
    },
    {
      "epoch": 2.8777777777777778,
      "grad_norm": 0.021524442359805107,
      "learning_rate": 8.311111111111111e-06,
      "loss": 4.7467,
      "step": 12950
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.024661684408783913,
      "learning_rate": 8.162962962962964e-06,
      "loss": 4.8195,
      "step": 12960
    },
    {
      "epoch": 2.8822222222222225,
      "grad_norm": 0.0723172053694725,
      "learning_rate": 8.014814814814816e-06,
      "loss": 4.8174,
      "step": 12970
    },
    {
      "epoch": 2.8844444444444446,
      "grad_norm": 0.04230748862028122,
      "learning_rate": 7.866666666666667e-06,
      "loss": 4.7706,
      "step": 12980
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.03524310886859894,
      "learning_rate": 7.71851851851852e-06,
      "loss": 4.7829,
      "step": 12990
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.014476675540208817,
      "learning_rate": 7.5703703703703705e-06,
      "loss": 4.7677,
      "step": 13000
    },
    {
      "epoch": 2.891111111111111,
      "grad_norm": 0.04816456511616707,
      "learning_rate": 7.422222222222222e-06,
      "loss": 4.7912,
      "step": 13010
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.015987204387784004,
      "learning_rate": 7.274074074074075e-06,
      "loss": 4.8051,
      "step": 13020
    },
    {
      "epoch": 2.8955555555555557,
      "grad_norm": 0.02301763743162155,
      "learning_rate": 7.125925925925926e-06,
      "loss": 4.7817,
      "step": 13030
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 0.009994115680456161,
      "learning_rate": 6.9777777777777775e-06,
      "loss": 4.8103,
      "step": 13040
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.018502911552786827,
      "learning_rate": 6.8296296296296305e-06,
      "loss": 4.7955,
      "step": 13050
    },
    {
      "epoch": 2.902222222222222,
      "grad_norm": 0.03741158917546272,
      "learning_rate": 6.681481481481482e-06,
      "loss": 4.7536,
      "step": 13060
    },
    {
      "epoch": 2.9044444444444446,
      "grad_norm": 0.015361430123448372,
      "learning_rate": 6.533333333333333e-06,
      "loss": 4.7419,
      "step": 13070
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.01985730044543743,
      "learning_rate": 6.385185185185185e-06,
      "loss": 4.7886,
      "step": 13080
    },
    {
      "epoch": 2.908888888888889,
      "grad_norm": 0.1254027932882309,
      "learning_rate": 6.237037037037037e-06,
      "loss": 4.8133,
      "step": 13090
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.5362343192100525,
      "learning_rate": 6.088888888888889e-06,
      "loss": 4.8076,
      "step": 13100
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.012372951954603195,
      "learning_rate": 5.940740740740741e-06,
      "loss": 4.8286,
      "step": 13110
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 0.15865015983581543,
      "learning_rate": 5.792592592592593e-06,
      "loss": 4.806,
      "step": 13120
    },
    {
      "epoch": 2.917777777777778,
      "grad_norm": 0.019770383834838867,
      "learning_rate": 5.6444444444444445e-06,
      "loss": 4.7845,
      "step": 13130
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.033692553639411926,
      "learning_rate": 5.496296296296297e-06,
      "loss": 4.7667,
      "step": 13140
    },
    {
      "epoch": 2.9222222222222225,
      "grad_norm": 0.013533312827348709,
      "learning_rate": 5.348148148148149e-06,
      "loss": 4.7481,
      "step": 13150
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 1.6082026958465576,
      "learning_rate": 5.2e-06,
      "loss": 4.8122,
      "step": 13160
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.036186106503009796,
      "learning_rate": 5.051851851851852e-06,
      "loss": 4.846,
      "step": 13170
    },
    {
      "epoch": 2.928888888888889,
      "grad_norm": 0.011799009516835213,
      "learning_rate": 4.903703703703704e-06,
      "loss": 4.7481,
      "step": 13180
    },
    {
      "epoch": 2.931111111111111,
      "grad_norm": 0.035033803433179855,
      "learning_rate": 4.755555555555556e-06,
      "loss": 4.7971,
      "step": 13190
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.012762031517922878,
      "learning_rate": 4.607407407407407e-06,
      "loss": 4.7746,
      "step": 13200
    },
    {
      "epoch": 2.9355555555555557,
      "grad_norm": 0.1566975861787796,
      "learning_rate": 4.459259259259259e-06,
      "loss": 4.8201,
      "step": 13210
    },
    {
      "epoch": 2.937777777777778,
      "grad_norm": 0.03963204845786095,
      "learning_rate": 4.3111111111111115e-06,
      "loss": 4.8133,
      "step": 13220
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.054384104907512665,
      "learning_rate": 4.162962962962963e-06,
      "loss": 4.7941,
      "step": 13230
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 0.01735556311905384,
      "learning_rate": 4.014814814814815e-06,
      "loss": 4.7594,
      "step": 13240
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 0.02738477848470211,
      "learning_rate": 3.866666666666667e-06,
      "loss": 4.7232,
      "step": 13250
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.023403488099575043,
      "learning_rate": 3.7185185185185185e-06,
      "loss": 4.7927,
      "step": 13260
    },
    {
      "epoch": 2.948888888888889,
      "grad_norm": 0.019678613170981407,
      "learning_rate": 3.5703703703703703e-06,
      "loss": 4.784,
      "step": 13270
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 0.020272010937333107,
      "learning_rate": 3.4222222222222224e-06,
      "loss": 4.7723,
      "step": 13280
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.03764444217085838,
      "learning_rate": 3.2740740740740746e-06,
      "loss": 4.7805,
      "step": 13290
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 0.023905517533421516,
      "learning_rate": 3.125925925925926e-06,
      "loss": 4.7287,
      "step": 13300
    },
    {
      "epoch": 2.957777777777778,
      "grad_norm": 0.017145911231637,
      "learning_rate": 2.977777777777778e-06,
      "loss": 4.8062,
      "step": 13310
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.017300179228186607,
      "learning_rate": 2.8296296296296294e-06,
      "loss": 4.7788,
      "step": 13320
    },
    {
      "epoch": 2.962222222222222,
      "grad_norm": 0.021817922592163086,
      "learning_rate": 2.6814814814814816e-06,
      "loss": 4.7909,
      "step": 13330
    },
    {
      "epoch": 2.964444444444444,
      "grad_norm": 0.029871050268411636,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 4.742,
      "step": 13340
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.023380937054753304,
      "learning_rate": 2.3851851851851855e-06,
      "loss": 4.7555,
      "step": 13350
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 0.03611946105957031,
      "learning_rate": 2.2370370370370373e-06,
      "loss": 4.7942,
      "step": 13360
    },
    {
      "epoch": 2.971111111111111,
      "grad_norm": 0.018716037273406982,
      "learning_rate": 2.088888888888889e-06,
      "loss": 4.7478,
      "step": 13370
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.013794698752462864,
      "learning_rate": 1.9407407407407408e-06,
      "loss": 4.7526,
      "step": 13380
    },
    {
      "epoch": 2.9755555555555553,
      "grad_norm": 0.02495049685239792,
      "learning_rate": 1.7925925925925925e-06,
      "loss": 4.7845,
      "step": 13390
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.024907302111387253,
      "learning_rate": 1.6444444444444447e-06,
      "loss": 4.7736,
      "step": 13400
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.08400260657072067,
      "learning_rate": 1.4962962962962962e-06,
      "loss": 4.7494,
      "step": 13410
    },
    {
      "epoch": 2.982222222222222,
      "grad_norm": 0.015685047954320908,
      "learning_rate": 1.3481481481481482e-06,
      "loss": 4.7927,
      "step": 13420
    },
    {
      "epoch": 2.9844444444444447,
      "grad_norm": 0.018190983682870865,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 4.7592,
      "step": 13430
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.017651300877332687,
      "learning_rate": 1.051851851851852e-06,
      "loss": 4.8359,
      "step": 13440
    },
    {
      "epoch": 2.988888888888889,
      "grad_norm": 0.07341013848781586,
      "learning_rate": 9.037037037037039e-07,
      "loss": 4.8307,
      "step": 13450
    },
    {
      "epoch": 2.991111111111111,
      "grad_norm": 0.01501956395804882,
      "learning_rate": 7.555555555555556e-07,
      "loss": 4.7739,
      "step": 13460
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.04981452226638794,
      "learning_rate": 6.074074074074074e-07,
      "loss": 4.7895,
      "step": 13470
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 0.014970537275075912,
      "learning_rate": 4.5925925925925927e-07,
      "loss": 4.7713,
      "step": 13480
    },
    {
      "epoch": 2.997777777777778,
      "grad_norm": 0.040941618382930756,
      "learning_rate": 3.111111111111111e-07,
      "loss": 4.7876,
      "step": 13490
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.039699096232652664,
      "learning_rate": 1.6296296296296295e-07,
      "loss": 4.8024,
      "step": 13500
    }
  ],
  "logging_steps": 10,
  "max_steps": 13500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.8651966439424e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
