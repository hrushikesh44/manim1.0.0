{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 945,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 1.0490097999572754,
      "learning_rate": 0.0001980952380952381,
      "loss": 4.8805,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.8426593542098999,
      "learning_rate": 0.000195978835978836,
      "loss": 4.8774,
      "step": 20
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.3686378598213196,
      "learning_rate": 0.00019386243386243388,
      "loss": 4.8763,
      "step": 30
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.8733265399932861,
      "learning_rate": 0.00019174603174603176,
      "loss": 4.8804,
      "step": 40
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.49864646792411804,
      "learning_rate": 0.00018962962962962965,
      "loss": 4.8371,
      "step": 50
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.40140509605407715,
      "learning_rate": 0.00018751322751322754,
      "loss": 4.86,
      "step": 60
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.590747594833374,
      "learning_rate": 0.0001853968253968254,
      "loss": 4.8729,
      "step": 70
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.2542964816093445,
      "learning_rate": 0.0001832804232804233,
      "loss": 4.8179,
      "step": 80
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.7737321257591248,
      "learning_rate": 0.00018116402116402118,
      "loss": 4.8389,
      "step": 90
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.15740245580673218,
      "learning_rate": 0.00017904761904761907,
      "loss": 4.8099,
      "step": 100
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.13844478130340576,
      "learning_rate": 0.00017693121693121696,
      "loss": 4.7714,
      "step": 110
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.30443912744522095,
      "learning_rate": 0.00017481481481481482,
      "loss": 4.7952,
      "step": 120
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.1185825765132904,
      "learning_rate": 0.0001726984126984127,
      "loss": 4.8654,
      "step": 130
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.124901682138443,
      "learning_rate": 0.0001705820105820106,
      "loss": 4.7853,
      "step": 140
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 1.3953602313995361,
      "learning_rate": 0.00016846560846560849,
      "loss": 4.7735,
      "step": 150
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.1738581359386444,
      "learning_rate": 0.00016634920634920637,
      "loss": 4.7672,
      "step": 160
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.1799364537000656,
      "learning_rate": 0.00016423280423280424,
      "loss": 4.8226,
      "step": 170
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.1794021129608154,
      "learning_rate": 0.00016211640211640212,
      "loss": 4.7825,
      "step": 180
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.35684099793434143,
      "learning_rate": 0.00016,
      "loss": 4.8727,
      "step": 190
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.4316657483577728,
      "learning_rate": 0.0001578835978835979,
      "loss": 4.7754,
      "step": 200
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.16950182616710663,
      "learning_rate": 0.0001557671957671958,
      "loss": 4.8305,
      "step": 210
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.15660233795642853,
      "learning_rate": 0.00015365079365079368,
      "loss": 4.8271,
      "step": 220
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.13223589956760406,
      "learning_rate": 0.00015153439153439154,
      "loss": 4.8026,
      "step": 230
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.3092662990093231,
      "learning_rate": 0.00014941798941798943,
      "loss": 4.724,
      "step": 240
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.18680024147033691,
      "learning_rate": 0.00014730158730158732,
      "loss": 4.7222,
      "step": 250
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.2757992744445801,
      "learning_rate": 0.0001451851851851852,
      "loss": 4.7253,
      "step": 260
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.2621934711933136,
      "learning_rate": 0.0001430687830687831,
      "loss": 4.7731,
      "step": 270
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.3135625422000885,
      "learning_rate": 0.00014095238095238096,
      "loss": 4.8226,
      "step": 280
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.2686675786972046,
      "learning_rate": 0.00013883597883597885,
      "loss": 4.7891,
      "step": 290
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.23831111192703247,
      "learning_rate": 0.00013693121693121693,
      "loss": 4.7969,
      "step": 300
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.35282137989997864,
      "learning_rate": 0.00013481481481481482,
      "loss": 4.7981,
      "step": 310
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.5079484581947327,
      "learning_rate": 0.0001326984126984127,
      "loss": 4.7966,
      "step": 320
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.23138335347175598,
      "learning_rate": 0.00013058201058201057,
      "loss": 4.7761,
      "step": 330
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.27712008357048035,
      "learning_rate": 0.00012846560846560846,
      "loss": 4.807,
      "step": 340
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.4537782669067383,
      "learning_rate": 0.00012634920634920635,
      "loss": 4.8284,
      "step": 350
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.09389454126358032,
      "learning_rate": 0.00012423280423280424,
      "loss": 4.8122,
      "step": 360
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.36371147632598877,
      "learning_rate": 0.00012211640211640213,
      "loss": 4.7252,
      "step": 370
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.36760157346725464,
      "learning_rate": 0.00012,
      "loss": 4.7896,
      "step": 380
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 6.4026079177856445,
      "learning_rate": 0.00011788359788359789,
      "loss": 4.7492,
      "step": 390
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.10570772737264633,
      "learning_rate": 0.00011576719576719577,
      "loss": 4.7678,
      "step": 400
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.3377036452293396,
      "learning_rate": 0.00011365079365079366,
      "loss": 4.802,
      "step": 410
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.07083970308303833,
      "learning_rate": 0.00011153439153439153,
      "loss": 4.8322,
      "step": 420
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.0735473781824112,
      "learning_rate": 0.00010941798941798942,
      "loss": 4.7317,
      "step": 430
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.2314993441104889,
      "learning_rate": 0.00010730158730158731,
      "loss": 4.7413,
      "step": 440
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.26184573769569397,
      "learning_rate": 0.00010518518518518518,
      "loss": 4.8227,
      "step": 450
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.2103554904460907,
      "learning_rate": 0.00010306878306878307,
      "loss": 4.775,
      "step": 460
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.1758352518081665,
      "learning_rate": 0.00010095238095238096,
      "loss": 4.7727,
      "step": 470
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.2990726828575134,
      "learning_rate": 9.883597883597884e-05,
      "loss": 4.7642,
      "step": 480
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.16714833676815033,
      "learning_rate": 9.671957671957672e-05,
      "loss": 4.7748,
      "step": 490
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.249447762966156,
      "learning_rate": 9.46031746031746e-05,
      "loss": 4.7901,
      "step": 500
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.12835188210010529,
      "learning_rate": 9.248677248677249e-05,
      "loss": 4.8002,
      "step": 510
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.1986141800880432,
      "learning_rate": 9.037037037037038e-05,
      "loss": 4.8256,
      "step": 520
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.36304038763046265,
      "learning_rate": 8.825396825396825e-05,
      "loss": 4.8105,
      "step": 530
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.44788408279418945,
      "learning_rate": 8.613756613756614e-05,
      "loss": 4.793,
      "step": 540
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.1359819620847702,
      "learning_rate": 8.402116402116403e-05,
      "loss": 4.8317,
      "step": 550
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.07118509709835052,
      "learning_rate": 8.19047619047619e-05,
      "loss": 4.7632,
      "step": 560
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.25864216685295105,
      "learning_rate": 8e-05,
      "loss": 4.7226,
      "step": 570
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.08813190460205078,
      "learning_rate": 7.78835978835979e-05,
      "loss": 4.7946,
      "step": 580
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.197225883603096,
      "learning_rate": 7.576719576719577e-05,
      "loss": 4.782,
      "step": 590
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.042354848235845566,
      "learning_rate": 7.365079365079366e-05,
      "loss": 4.7014,
      "step": 600
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.26285624504089355,
      "learning_rate": 7.153439153439155e-05,
      "loss": 4.7139,
      "step": 610
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.28284645080566406,
      "learning_rate": 6.941798941798942e-05,
      "loss": 4.725,
      "step": 620
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.21220076084136963,
      "learning_rate": 6.730158730158731e-05,
      "loss": 4.7832,
      "step": 630
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 2.355304479598999,
      "learning_rate": 6.51851851851852e-05,
      "loss": 4.8187,
      "step": 640
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.15257662534713745,
      "learning_rate": 6.306878306878308e-05,
      "loss": 4.7815,
      "step": 650
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.05210183560848236,
      "learning_rate": 6.0952380952380964e-05,
      "loss": 4.8107,
      "step": 660
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.18967638909816742,
      "learning_rate": 5.883597883597883e-05,
      "loss": 4.8143,
      "step": 670
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.11443278938531876,
      "learning_rate": 5.6719576719576714e-05,
      "loss": 4.7804,
      "step": 680
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.06874918937683105,
      "learning_rate": 5.46031746031746e-05,
      "loss": 4.8036,
      "step": 690
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.06719576567411423,
      "learning_rate": 5.2486772486772485e-05,
      "loss": 4.8134,
      "step": 700
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.059888362884521484,
      "learning_rate": 5.0370370370370366e-05,
      "loss": 4.7955,
      "step": 710
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.08380963653326035,
      "learning_rate": 4.8253968253968255e-05,
      "loss": 4.8016,
      "step": 720
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.1758289486169815,
      "learning_rate": 4.6137566137566144e-05,
      "loss": 4.7177,
      "step": 730
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.14142757654190063,
      "learning_rate": 4.4021164021164026e-05,
      "loss": 4.7731,
      "step": 740
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.1215607300400734,
      "learning_rate": 4.190476190476191e-05,
      "loss": 4.8226,
      "step": 750
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.1749119609594345,
      "learning_rate": 3.978835978835979e-05,
      "loss": 4.6909,
      "step": 760
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.1656808853149414,
      "learning_rate": 3.767195767195768e-05,
      "loss": 4.84,
      "step": 770
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.17996764183044434,
      "learning_rate": 3.555555555555556e-05,
      "loss": 4.7579,
      "step": 780
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.047714367508888245,
      "learning_rate": 3.343915343915344e-05,
      "loss": 4.7828,
      "step": 790
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.04227297753095627,
      "learning_rate": 3.1322751322751324e-05,
      "loss": 4.6879,
      "step": 800
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.4960554838180542,
      "learning_rate": 2.920634920634921e-05,
      "loss": 4.7366,
      "step": 810
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.05375390499830246,
      "learning_rate": 2.7089947089947094e-05,
      "loss": 4.7428,
      "step": 820
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.13788338005542755,
      "learning_rate": 2.4973544973544973e-05,
      "loss": 4.7275,
      "step": 830
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.05989723652601242,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 4.7951,
      "step": 840
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 2.7173173427581787,
      "learning_rate": 2.074074074074074e-05,
      "loss": 4.7479,
      "step": 850
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.2546066343784332,
      "learning_rate": 1.8624338624338625e-05,
      "loss": 4.7743,
      "step": 860
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.33002957701683044,
      "learning_rate": 1.6507936507936507e-05,
      "loss": 4.6854,
      "step": 870
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.058325618505477905,
      "learning_rate": 1.4391534391534392e-05,
      "loss": 4.7615,
      "step": 880
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.2001088261604309,
      "learning_rate": 1.2275132275132276e-05,
      "loss": 4.7117,
      "step": 890
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.08257918059825897,
      "learning_rate": 1.015873015873016e-05,
      "loss": 4.7793,
      "step": 900
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.09174592047929764,
      "learning_rate": 8.042328042328043e-06,
      "loss": 4.7702,
      "step": 910
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.13717420399188995,
      "learning_rate": 5.925925925925927e-06,
      "loss": 4.7629,
      "step": 920
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.08844475448131561,
      "learning_rate": 3.8095238095238102e-06,
      "loss": 4.7922,
      "step": 930
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.04611245542764664,
      "learning_rate": 1.6931216931216934e-06,
      "loss": 4.7748,
      "step": 940
    }
  ],
  "logging_steps": 10,
  "max_steps": 945,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4105637650759680.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
