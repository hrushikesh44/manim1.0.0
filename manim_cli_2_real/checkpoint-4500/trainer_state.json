{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 6.225846767425537,
      "learning_rate": 0.0001999111111111111,
      "loss": 10.1395,
      "step": 10
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 4.225636959075928,
      "learning_rate": 0.00019976296296296298,
      "loss": 9.1283,
      "step": 20
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.4864975214004517,
      "learning_rate": 0.00019961481481481483,
      "loss": 7.7224,
      "step": 30
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 1.652529239654541,
      "learning_rate": 0.00019946666666666667,
      "loss": 6.8205,
      "step": 40
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 2.579355239868164,
      "learning_rate": 0.00019931851851851852,
      "loss": 6.163,
      "step": 50
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.8612302541732788,
      "learning_rate": 0.0001991703703703704,
      "loss": 5.7051,
      "step": 60
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 1.4752705097198486,
      "learning_rate": 0.0001990222222222222,
      "loss": 5.4,
      "step": 70
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.364428162574768,
      "learning_rate": 0.00019887407407407408,
      "loss": 5.2438,
      "step": 80
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.771734893321991,
      "learning_rate": 0.00019872592592592593,
      "loss": 5.0789,
      "step": 90
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.6452211737632751,
      "learning_rate": 0.0001985777777777778,
      "loss": 5.0787,
      "step": 100
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 1.1411465406417847,
      "learning_rate": 0.00019842962962962962,
      "loss": 5.0332,
      "step": 110
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.6597524285316467,
      "learning_rate": 0.0001982814814814815,
      "loss": 4.97,
      "step": 120
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 0.48184919357299805,
      "learning_rate": 0.00019813333333333334,
      "loss": 4.9535,
      "step": 130
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 1.1146373748779297,
      "learning_rate": 0.0001979851851851852,
      "loss": 4.9555,
      "step": 140
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3618299663066864,
      "learning_rate": 0.00019783703703703704,
      "loss": 4.9256,
      "step": 150
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.40859565138816833,
      "learning_rate": 0.0001976888888888889,
      "loss": 4.9245,
      "step": 160
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 0.40305325388908386,
      "learning_rate": 0.00019754074074074075,
      "loss": 4.9214,
      "step": 170
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.3685086667537689,
      "learning_rate": 0.0001973925925925926,
      "loss": 4.8996,
      "step": 180
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 0.4647376835346222,
      "learning_rate": 0.00019724444444444445,
      "loss": 4.9315,
      "step": 190
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.4640187621116638,
      "learning_rate": 0.00019709629629629632,
      "loss": 4.914,
      "step": 200
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.4320927560329437,
      "learning_rate": 0.00019694814814814814,
      "loss": 4.9306,
      "step": 210
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 0.2586047053337097,
      "learning_rate": 0.0001968,
      "loss": 4.8955,
      "step": 220
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 0.37456658482551575,
      "learning_rate": 0.00019665185185185186,
      "loss": 4.9392,
      "step": 230
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.23325705528259277,
      "learning_rate": 0.00019650370370370373,
      "loss": 4.8518,
      "step": 240
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.5212029218673706,
      "learning_rate": 0.00019635555555555555,
      "loss": 4.8612,
      "step": 250
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 0.6987076997756958,
      "learning_rate": 0.00019620740740740743,
      "loss": 4.8812,
      "step": 260
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.39385896921157837,
      "learning_rate": 0.00019605925925925927,
      "loss": 4.8125,
      "step": 270
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 0.1816035658121109,
      "learning_rate": 0.00019591111111111112,
      "loss": 4.8842,
      "step": 280
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 0.22836747765541077,
      "learning_rate": 0.00019576296296296296,
      "loss": 4.8934,
      "step": 290
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.37063872814178467,
      "learning_rate": 0.00019561481481481484,
      "loss": 4.8959,
      "step": 300
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 0.26113709807395935,
      "learning_rate": 0.00019546666666666668,
      "loss": 4.8563,
      "step": 310
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.4359486699104309,
      "learning_rate": 0.00019531851851851853,
      "loss": 4.8754,
      "step": 320
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.35568204522132874,
      "learning_rate": 0.00019517037037037038,
      "loss": 4.8734,
      "step": 330
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 0.41173622012138367,
      "learning_rate": 0.00019502222222222225,
      "loss": 4.8785,
      "step": 340
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 0.5408313870429993,
      "learning_rate": 0.00019487407407407407,
      "loss": 4.8375,
      "step": 350
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.7096452116966248,
      "learning_rate": 0.00019472592592592594,
      "loss": 4.8731,
      "step": 360
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 0.5935046076774597,
      "learning_rate": 0.0001945777777777778,
      "loss": 4.8385,
      "step": 370
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 0.2911219298839569,
      "learning_rate": 0.00019442962962962963,
      "loss": 4.8834,
      "step": 380
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.39248591661453247,
      "learning_rate": 0.00019428148148148148,
      "loss": 4.9184,
      "step": 390
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.3067822754383087,
      "learning_rate": 0.00019413333333333335,
      "loss": 4.8557,
      "step": 400
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 0.2045900970697403,
      "learning_rate": 0.0001939851851851852,
      "loss": 4.8343,
      "step": 410
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.22982415556907654,
      "learning_rate": 0.00019383703703703705,
      "loss": 4.8365,
      "step": 420
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 0.15578646957874298,
      "learning_rate": 0.0001936888888888889,
      "loss": 4.8555,
      "step": 430
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.2889108657836914,
      "learning_rate": 0.00019354074074074077,
      "loss": 4.8753,
      "step": 440
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.14415167272090912,
      "learning_rate": 0.00019339259259259259,
      "loss": 4.8203,
      "step": 450
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 0.35246098041534424,
      "learning_rate": 0.00019324444444444446,
      "loss": 4.8741,
      "step": 460
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 0.49699148535728455,
      "learning_rate": 0.0001930962962962963,
      "loss": 4.8456,
      "step": 470
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.18623782694339752,
      "learning_rate": 0.00019294814814814818,
      "loss": 4.8145,
      "step": 480
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 0.1282183676958084,
      "learning_rate": 0.0001928,
      "loss": 4.8736,
      "step": 490
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.12000176310539246,
      "learning_rate": 0.00019265185185185187,
      "loss": 4.838,
      "step": 500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2719639241695404,
      "learning_rate": 0.00019250370370370372,
      "loss": 4.8135,
      "step": 510
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 0.1470160186290741,
      "learning_rate": 0.00019235555555555556,
      "loss": 4.8576,
      "step": 520
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 0.3521769940853119,
      "learning_rate": 0.0001922074074074074,
      "loss": 4.8298,
      "step": 530
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.261871874332428,
      "learning_rate": 0.00019205925925925928,
      "loss": 4.8592,
      "step": 540
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 0.20727117359638214,
      "learning_rate": 0.00019191111111111113,
      "loss": 4.8603,
      "step": 550
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.5540192127227783,
      "learning_rate": 0.00019176296296296298,
      "loss": 4.8674,
      "step": 560
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.14969080686569214,
      "learning_rate": 0.00019161481481481482,
      "loss": 4.8552,
      "step": 570
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 0.2017495036125183,
      "learning_rate": 0.0001914666666666667,
      "loss": 4.8381,
      "step": 580
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 0.23540093004703522,
      "learning_rate": 0.00019131851851851851,
      "loss": 4.8453,
      "step": 590
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.11461997032165527,
      "learning_rate": 0.0001911703703703704,
      "loss": 4.8461,
      "step": 600
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 0.4314859211444855,
      "learning_rate": 0.00019102222222222223,
      "loss": 4.8552,
      "step": 610
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 0.2765265107154846,
      "learning_rate": 0.0001908740740740741,
      "loss": 4.8576,
      "step": 620
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.19034814834594727,
      "learning_rate": 0.00019072592592592593,
      "loss": 4.8385,
      "step": 630
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": NaN,
      "learning_rate": 0.0001905777777777778,
      "loss": 4.9411,
      "step": 640
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.1758766621351242,
      "learning_rate": 0.00019044444444444444,
      "loss": 4.8719,
      "step": 650
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.22520118951797485,
      "learning_rate": 0.00019029629629629632,
      "loss": 4.8389,
      "step": 660
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 0.2431105524301529,
      "learning_rate": 0.00019014814814814816,
      "loss": 4.879,
      "step": 670
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.09286356717348099,
      "learning_rate": 0.00019,
      "loss": 4.8446,
      "step": 680
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.15588346123695374,
      "learning_rate": 0.00018985185185185186,
      "loss": 4.885,
      "step": 690
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.0809885561466217,
      "learning_rate": 0.00018970370370370373,
      "loss": 4.909,
      "step": 700
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 0.1147589385509491,
      "learning_rate": 0.00018955555555555558,
      "loss": 4.9048,
      "step": 710
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.089451864361763,
      "learning_rate": 0.00018940740740740742,
      "loss": 4.8481,
      "step": 720
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 0.1127135157585144,
      "learning_rate": 0.00018925925925925927,
      "loss": 4.8291,
      "step": 730
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 0.2691830098628998,
      "learning_rate": 0.00018911111111111112,
      "loss": 4.8429,
      "step": 740
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.6335787177085876,
      "learning_rate": 0.00018896296296296296,
      "loss": 4.7833,
      "step": 750
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.07316829264163971,
      "learning_rate": 0.00018881481481481483,
      "loss": 4.8824,
      "step": 760
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 0.42868587374687195,
      "learning_rate": 0.00018866666666666668,
      "loss": 4.8487,
      "step": 770
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.23224855959415436,
      "learning_rate": 0.00018851851851851853,
      "loss": 4.823,
      "step": 780
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 0.26916834712028503,
      "learning_rate": 0.00018837037037037037,
      "loss": 4.8186,
      "step": 790
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.1982819139957428,
      "learning_rate": 0.00018822222222222222,
      "loss": 4.8339,
      "step": 800
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.15245532989501953,
      "learning_rate": 0.0001880740740740741,
      "loss": 4.8191,
      "step": 810
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.36395251750946045,
      "learning_rate": 0.00018792592592592594,
      "loss": 4.8373,
      "step": 820
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 0.3229612708091736,
      "learning_rate": 0.00018777777777777779,
      "loss": 4.8741,
      "step": 830
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.11741460859775543,
      "learning_rate": 0.00018762962962962963,
      "loss": 4.8519,
      "step": 840
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.12124843895435333,
      "learning_rate": 0.00018748148148148148,
      "loss": 4.8212,
      "step": 850
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 0.0715753436088562,
      "learning_rate": 0.00018733333333333335,
      "loss": 4.8651,
      "step": 860
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.2490977793931961,
      "learning_rate": 0.0001871851851851852,
      "loss": 4.84,
      "step": 870
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 0.043998729437589645,
      "learning_rate": 0.00018703703703703704,
      "loss": 4.8428,
      "step": 880
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 0.20516645908355713,
      "learning_rate": 0.0001868888888888889,
      "loss": 4.8314,
      "step": 890
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2136959433555603,
      "learning_rate": 0.00018674074074074074,
      "loss": 4.819,
      "step": 900
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 0.10561199486255646,
      "learning_rate": 0.0001865925925925926,
      "loss": 4.8507,
      "step": 910
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.35473087430000305,
      "learning_rate": 0.00018644444444444446,
      "loss": 4.8739,
      "step": 920
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.1472129076719284,
      "learning_rate": 0.0001862962962962963,
      "loss": 4.8269,
      "step": 930
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 0.4117239713668823,
      "learning_rate": 0.00018614814814814815,
      "loss": 4.8498,
      "step": 940
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 0.05495855584740639,
      "learning_rate": 0.00018600000000000002,
      "loss": 4.8102,
      "step": 950
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06877665221691132,
      "learning_rate": 0.00018585185185185187,
      "loss": 4.8637,
      "step": 960
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 0.16302725672721863,
      "learning_rate": 0.00018570370370370371,
      "loss": 4.865,
      "step": 970
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.2345418632030487,
      "learning_rate": 0.00018555555555555556,
      "loss": 4.8285,
      "step": 980
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.1729120910167694,
      "learning_rate": 0.0001854074074074074,
      "loss": 4.8293,
      "step": 990
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.11146701872348785,
      "learning_rate": 0.00018525925925925925,
      "loss": 4.8355,
      "step": 1000
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 0.2214905321598053,
      "learning_rate": 0.00018511111111111113,
      "loss": 4.8148,
      "step": 1010
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.1841714233160019,
      "learning_rate": 0.00018496296296296297,
      "loss": 4.8191,
      "step": 1020
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 0.10226409882307053,
      "learning_rate": 0.00018481481481481482,
      "loss": 4.816,
      "step": 1030
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.17372289299964905,
      "learning_rate": 0.00018466666666666666,
      "loss": 4.8827,
      "step": 1040
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 3.3862955570220947,
      "learning_rate": 0.00018451851851851854,
      "loss": 4.8101,
      "step": 1050
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 0.16539266705513,
      "learning_rate": 0.00018437037037037038,
      "loss": 4.7907,
      "step": 1060
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 0.08927718549966812,
      "learning_rate": 0.00018422222222222223,
      "loss": 4.8297,
      "step": 1070
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.29183894395828247,
      "learning_rate": 0.00018407407407407408,
      "loss": 4.8969,
      "step": 1080
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 0.16662229597568512,
      "learning_rate": 0.00018392592592592592,
      "loss": 4.8217,
      "step": 1090
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.3479580283164978,
      "learning_rate": 0.00018377777777777777,
      "loss": 4.8299,
      "step": 1100
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.10477970540523529,
      "learning_rate": 0.00018362962962962964,
      "loss": 4.8605,
      "step": 1110
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.11046769469976425,
      "learning_rate": 0.0001834814814814815,
      "loss": 4.7804,
      "step": 1120
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 0.06127776950597763,
      "learning_rate": 0.00018333333333333334,
      "loss": 4.8645,
      "step": 1130
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.09219725430011749,
      "learning_rate": 0.00018318518518518518,
      "loss": 4.8399,
      "step": 1140
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.10261193662881851,
      "learning_rate": 0.00018303703703703705,
      "loss": 4.8078,
      "step": 1150
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.22127504646778107,
      "learning_rate": 0.00018288888888888887,
      "loss": 4.8222,
      "step": 1160
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.07819434255361557,
      "learning_rate": 0.00018274074074074075,
      "loss": 4.8256,
      "step": 1170
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.2039748579263687,
      "learning_rate": 0.0001825925925925926,
      "loss": 4.7943,
      "step": 1180
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 0.7903993725776672,
      "learning_rate": 0.00018244444444444447,
      "loss": 4.8048,
      "step": 1190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.0804237127304077,
      "learning_rate": 0.00018229629629629629,
      "loss": 4.8742,
      "step": 1200
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 0.07221407443284988,
      "learning_rate": 0.00018214814814814816,
      "loss": 4.8183,
      "step": 1210
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 0.20203007757663727,
      "learning_rate": 0.000182,
      "loss": 4.8093,
      "step": 1220
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.10640046745538712,
      "learning_rate": 0.00018185185185185185,
      "loss": 4.8083,
      "step": 1230
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.08235148340463638,
      "learning_rate": 0.0001817037037037037,
      "loss": 4.8277,
      "step": 1240
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.08674979954957962,
      "learning_rate": 0.00018155555555555557,
      "loss": 4.7807,
      "step": 1250
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.07111555337905884,
      "learning_rate": 0.00018140740740740742,
      "loss": 4.8097,
      "step": 1260
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 0.03507561236619949,
      "learning_rate": 0.00018125925925925926,
      "loss": 4.7956,
      "step": 1270
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.05117575079202652,
      "learning_rate": 0.0001811111111111111,
      "loss": 4.8366,
      "step": 1280
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.09060432016849518,
      "learning_rate": 0.00018096296296296298,
      "loss": 4.7954,
      "step": 1290
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.1350524127483368,
      "learning_rate": 0.0001808148148148148,
      "loss": 4.8007,
      "step": 1300
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 0.3557530641555786,
      "learning_rate": 0.00018066666666666668,
      "loss": 4.8659,
      "step": 1310
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.08523300290107727,
      "learning_rate": 0.00018051851851851852,
      "loss": 4.8382,
      "step": 1320
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 0.07186221331357956,
      "learning_rate": 0.0001803703703703704,
      "loss": 4.7904,
      "step": 1330
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 0.03733265399932861,
      "learning_rate": 0.00018022222222222221,
      "loss": 4.8151,
      "step": 1340
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.06911791861057281,
      "learning_rate": 0.0001800740740740741,
      "loss": 4.874,
      "step": 1350
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 3.320232629776001,
      "learning_rate": 0.00017992592592592593,
      "loss": 4.8332,
      "step": 1360
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 0.2257617563009262,
      "learning_rate": 0.00017977777777777778,
      "loss": 4.8269,
      "step": 1370
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.04689428210258484,
      "learning_rate": 0.00017962962962962963,
      "loss": 4.8531,
      "step": 1380
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 0.30755850672721863,
      "learning_rate": 0.0001794814814814815,
      "loss": 4.8152,
      "step": 1390
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.0688280239701271,
      "learning_rate": 0.00017933333333333332,
      "loss": 4.7822,
      "step": 1400
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.07995197176933289,
      "learning_rate": 0.0001791851851851852,
      "loss": 4.7778,
      "step": 1410
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 0.32423603534698486,
      "learning_rate": 0.00017903703703703704,
      "loss": 4.7847,
      "step": 1420
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 0.08564295619726181,
      "learning_rate": 0.0001788888888888889,
      "loss": 4.791,
      "step": 1430
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.06000882387161255,
      "learning_rate": 0.00017874074074074073,
      "loss": 4.7949,
      "step": 1440
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 15.912732124328613,
      "learning_rate": 0.00017860740740740743,
      "loss": 4.9082,
      "step": 1450
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 0.3114972710609436,
      "learning_rate": 0.00017847407407407408,
      "loss": 5.0546,
      "step": 1460
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.3143916428089142,
      "learning_rate": 0.00017832592592592595,
      "loss": 4.8511,
      "step": 1470
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 1.1049848794937134,
      "learning_rate": 0.00017817777777777777,
      "loss": 4.8039,
      "step": 1480
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 0.11288613080978394,
      "learning_rate": 0.00017802962962962964,
      "loss": 4.7715,
      "step": 1490
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.037048663944005966,
      "learning_rate": 0.0001778814814814815,
      "loss": 4.8199,
      "step": 1500
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 0.07504619657993317,
      "learning_rate": 0.00017773333333333336,
      "loss": 4.8672,
      "step": 1510
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.12516652047634125,
      "learning_rate": 0.00017758518518518518,
      "loss": 4.7971,
      "step": 1520
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07029396295547485,
      "learning_rate": 0.00017743703703703705,
      "loss": 4.8247,
      "step": 1530
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 0.03191877156496048,
      "learning_rate": 0.0001772888888888889,
      "loss": 4.8328,
      "step": 1540
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.1010226309299469,
      "learning_rate": 0.00017714074074074075,
      "loss": 4.7859,
      "step": 1550
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.33741921186447144,
      "learning_rate": 0.0001769925925925926,
      "loss": 4.8315,
      "step": 1560
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 0.21538840234279633,
      "learning_rate": 0.00017684444444444447,
      "loss": 4.8381,
      "step": 1570
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 0.035015348345041275,
      "learning_rate": 0.0001766962962962963,
      "loss": 4.7954,
      "step": 1580
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.3413379192352295,
      "learning_rate": 0.00017654814814814816,
      "loss": 4.8276,
      "step": 1590
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.09113043546676636,
      "learning_rate": 0.0001764,
      "loss": 4.8392,
      "step": 1600
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 0.201874241232872,
      "learning_rate": 0.00017625185185185188,
      "loss": 4.7693,
      "step": 1610
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.05491882562637329,
      "learning_rate": 0.0001761037037037037,
      "loss": 4.8264,
      "step": 1620
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 0.05842265486717224,
      "learning_rate": 0.00017595555555555557,
      "loss": 4.7852,
      "step": 1630
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.04852772504091263,
      "learning_rate": 0.00017580740740740742,
      "loss": 4.8419,
      "step": 1640
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.054340530186891556,
      "learning_rate": 0.00017565925925925926,
      "loss": 4.8349,
      "step": 1650
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 0.264435350894928,
      "learning_rate": 0.0001755111111111111,
      "loss": 4.7926,
      "step": 1660
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 0.12340620160102844,
      "learning_rate": 0.00017536296296296298,
      "loss": 4.817,
      "step": 1670
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.0568724200129509,
      "learning_rate": 0.00017521481481481483,
      "loss": 4.7946,
      "step": 1680
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 0.10169481486082077,
      "learning_rate": 0.00017506666666666668,
      "loss": 4.8356,
      "step": 1690
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 1.0170198678970337,
      "learning_rate": 0.00017491851851851852,
      "loss": 4.826,
      "step": 1700
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.1736062616109848,
      "learning_rate": 0.0001747703703703704,
      "loss": 4.8071,
      "step": 1710
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.06196598708629608,
      "learning_rate": 0.00017462222222222221,
      "loss": 4.743,
      "step": 1720
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 0.04486741125583649,
      "learning_rate": 0.0001744740740740741,
      "loss": 4.8424,
      "step": 1730
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.045939262956380844,
      "learning_rate": 0.00017432592592592593,
      "loss": 4.8313,
      "step": 1740
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.027429169043898582,
      "learning_rate": 0.0001741777777777778,
      "loss": 4.7999,
      "step": 1750
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.04754316061735153,
      "learning_rate": 0.00017402962962962963,
      "loss": 4.8012,
      "step": 1760
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.1826382726430893,
      "learning_rate": 0.0001738814814814815,
      "loss": 4.7772,
      "step": 1770
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 0.03608357161283493,
      "learning_rate": 0.00017373333333333335,
      "loss": 4.8255,
      "step": 1780
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 0.0551888681948185,
      "learning_rate": 0.0001735851851851852,
      "loss": 4.8024,
      "step": 1790
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.21744665503501892,
      "learning_rate": 0.00017343703703703704,
      "loss": 4.7796,
      "step": 1800
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 0.036127861589193344,
      "learning_rate": 0.0001732888888888889,
      "loss": 4.8535,
      "step": 1810
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.035309772938489914,
      "learning_rate": 0.00017314074074074076,
      "loss": 4.8471,
      "step": 1820
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.09789478778839111,
      "learning_rate": 0.0001729925925925926,
      "loss": 4.8584,
      "step": 1830
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.08884906023740768,
      "learning_rate": 0.00017284444444444445,
      "loss": 4.7995,
      "step": 1840
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.02342410758137703,
      "learning_rate": 0.00017269629629629632,
      "loss": 4.7509,
      "step": 1850
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.11575110256671906,
      "learning_rate": 0.00017254814814814814,
      "loss": 4.8008,
      "step": 1860
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 0.09495913237333298,
      "learning_rate": 0.00017240000000000002,
      "loss": 4.7875,
      "step": 1870
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.06455512344837189,
      "learning_rate": 0.00017225185185185186,
      "loss": 4.8014,
      "step": 1880
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.0464775450527668,
      "learning_rate": 0.00017210370370370374,
      "loss": 4.8193,
      "step": 1890
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.22632479667663574,
      "learning_rate": 0.00017195555555555556,
      "loss": 4.8217,
      "step": 1900
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 0.05434798076748848,
      "learning_rate": 0.00017180740740740743,
      "loss": 4.8897,
      "step": 1910
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.02603461965918541,
      "learning_rate": 0.00017165925925925927,
      "loss": 4.802,
      "step": 1920
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 0.02507615275681019,
      "learning_rate": 0.00017151111111111112,
      "loss": 4.7948,
      "step": 1930
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 0.11721260100603104,
      "learning_rate": 0.00017136296296296297,
      "loss": 4.815,
      "step": 1940
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.5435838103294373,
      "learning_rate": 0.00017121481481481484,
      "loss": 4.8385,
      "step": 1950
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.18674038350582123,
      "learning_rate": 0.00017106666666666666,
      "loss": 4.8546,
      "step": 1960
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 0.03779018297791481,
      "learning_rate": 0.00017091851851851853,
      "loss": 4.8485,
      "step": 1970
    },
    {
      "epoch": 0.44,
      "grad_norm": 34.14052200317383,
      "learning_rate": 0.00017077037037037038,
      "loss": 4.8256,
      "step": 1980
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 0.04346787557005882,
      "learning_rate": 0.00017062222222222223,
      "loss": 4.8328,
      "step": 1990
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.13076938688755035,
      "learning_rate": 0.00017047407407407407,
      "loss": 4.7986,
      "step": 2000
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.402369886636734,
      "learning_rate": 0.00017034074074074074,
      "loss": 4.8459,
      "step": 2010
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.02312617190182209,
      "learning_rate": 0.0001701925925925926,
      "loss": 4.8068,
      "step": 2020
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 0.06896031647920609,
      "learning_rate": 0.00017004444444444446,
      "loss": 4.8437,
      "step": 2030
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.07789898663759232,
      "learning_rate": 0.0001698962962962963,
      "loss": 4.821,
      "step": 2040
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.4466211497783661,
      "learning_rate": 0.00016974814814814816,
      "loss": 4.7937,
      "step": 2050
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.05345480889081955,
      "learning_rate": 0.0001696,
      "loss": 4.7825,
      "step": 2060
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.08653102070093155,
      "learning_rate": 0.00016945185185185185,
      "loss": 4.8123,
      "step": 2070
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.10900440067052841,
      "learning_rate": 0.00016930370370370372,
      "loss": 4.7588,
      "step": 2080
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 0.03177042677998543,
      "learning_rate": 0.00016915555555555557,
      "loss": 4.7803,
      "step": 2090
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.03538152575492859,
      "learning_rate": 0.00016900740740740741,
      "loss": 4.831,
      "step": 2100
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 0.03634023666381836,
      "learning_rate": 0.00016885925925925926,
      "loss": 4.8306,
      "step": 2110
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.06607925146818161,
      "learning_rate": 0.0001687111111111111,
      "loss": 4.834,
      "step": 2120
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.02397308498620987,
      "learning_rate": 0.00016856296296296298,
      "loss": 4.7839,
      "step": 2130
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 0.25402572751045227,
      "learning_rate": 0.00016841481481481483,
      "loss": 4.7887,
      "step": 2140
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.04983317852020264,
      "learning_rate": 0.00016826666666666667,
      "loss": 4.8392,
      "step": 2150
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.0893629640340805,
      "learning_rate": 0.00016811851851851852,
      "loss": 4.814,
      "step": 2160
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 0.22175899147987366,
      "learning_rate": 0.00016797037037037037,
      "loss": 4.7824,
      "step": 2170
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 0.06697045266628265,
      "learning_rate": 0.00016782222222222224,
      "loss": 4.8242,
      "step": 2180
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.057267703115940094,
      "learning_rate": 0.00016767407407407408,
      "loss": 4.8639,
      "step": 2190
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.07415208220481873,
      "learning_rate": 0.00016752592592592593,
      "loss": 4.8266,
      "step": 2200
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 0.6119957566261292,
      "learning_rate": 0.00016737777777777778,
      "loss": 4.8179,
      "step": 2210
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.2886906862258911,
      "learning_rate": 0.00016722962962962965,
      "loss": 4.7874,
      "step": 2220
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 0.0805521160364151,
      "learning_rate": 0.00016708148148148147,
      "loss": 4.8066,
      "step": 2230
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.05512950196862221,
      "learning_rate": 0.00016693333333333334,
      "loss": 4.8144,
      "step": 2240
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.05888549983501434,
      "learning_rate": 0.0001667851851851852,
      "loss": 4.815,
      "step": 2250
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.028582138940691948,
      "learning_rate": 0.00016663703703703704,
      "loss": 4.819,
      "step": 2260
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 3.8194327354431152,
      "learning_rate": 0.00016648888888888888,
      "loss": 4.8496,
      "step": 2270
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.03565702214837074,
      "learning_rate": 0.00016634074074074076,
      "loss": 4.8311,
      "step": 2280
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 0.045316241681575775,
      "learning_rate": 0.0001661925925925926,
      "loss": 4.792,
      "step": 2290
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.45526930689811707,
      "learning_rate": 0.00016604444444444445,
      "loss": 4.8741,
      "step": 2300
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.2710990607738495,
      "learning_rate": 0.0001658962962962963,
      "loss": 4.7478,
      "step": 2310
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.03241879492998123,
      "learning_rate": 0.00016574814814814817,
      "loss": 4.8018,
      "step": 2320
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 0.1902332603931427,
      "learning_rate": 0.0001656,
      "loss": 4.8263,
      "step": 2330
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.08178194612264633,
      "learning_rate": 0.00016545185185185186,
      "loss": 4.8098,
      "step": 2340
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.3877515196800232,
      "learning_rate": 0.0001653037037037037,
      "loss": 4.8623,
      "step": 2350
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.12245471030473709,
      "learning_rate": 0.00016515555555555558,
      "loss": 4.8385,
      "step": 2360
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.04057373106479645,
      "learning_rate": 0.0001650074074074074,
      "loss": 4.7685,
      "step": 2370
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 0.08279513567686081,
      "learning_rate": 0.00016485925925925927,
      "loss": 4.7981,
      "step": 2380
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 0.19693955779075623,
      "learning_rate": 0.00016471111111111112,
      "loss": 4.8019,
      "step": 2390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.04385685920715332,
      "learning_rate": 0.00016456296296296296,
      "loss": 4.8518,
      "step": 2400
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 0.05642789974808693,
      "learning_rate": 0.0001644148148148148,
      "loss": 4.8138,
      "step": 2410
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.08452783524990082,
      "learning_rate": 0.00016426666666666668,
      "loss": 4.8674,
      "step": 2420
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.020838331431150436,
      "learning_rate": 0.0001641185185185185,
      "loss": 4.816,
      "step": 2430
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.03530994802713394,
      "learning_rate": 0.00016397037037037038,
      "loss": 4.7694,
      "step": 2440
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.07283475250005722,
      "learning_rate": 0.00016382222222222222,
      "loss": 4.7976,
      "step": 2450
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.10329196602106094,
      "learning_rate": 0.0001636740740740741,
      "loss": 4.7605,
      "step": 2460
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 0.15117445588111877,
      "learning_rate": 0.00016352592592592592,
      "loss": 4.8116,
      "step": 2470
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.04708515852689743,
      "learning_rate": 0.0001633777777777778,
      "loss": 4.8072,
      "step": 2480
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.023962868377566338,
      "learning_rate": 0.00016322962962962963,
      "loss": 4.8451,
      "step": 2490
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.02796465903520584,
      "learning_rate": 0.00016308148148148148,
      "loss": 4.7772,
      "step": 2500
    },
    {
      "epoch": 0.5577777777777778,
      "grad_norm": 0.09123159199953079,
      "learning_rate": 0.00016293333333333333,
      "loss": 4.7863,
      "step": 2510
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.22173824906349182,
      "learning_rate": 0.0001627851851851852,
      "loss": 4.7899,
      "step": 2520
    },
    {
      "epoch": 0.5622222222222222,
      "grad_norm": 0.06814306974411011,
      "learning_rate": 0.00016263703703703705,
      "loss": 4.825,
      "step": 2530
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.07562677562236786,
      "learning_rate": 0.0001624888888888889,
      "loss": 4.7791,
      "step": 2540
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.039130281656980515,
      "learning_rate": 0.00016234074074074074,
      "loss": 4.7907,
      "step": 2550
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.024463443085551262,
      "learning_rate": 0.0001621925925925926,
      "loss": 4.7994,
      "step": 2560
    },
    {
      "epoch": 0.5711111111111111,
      "grad_norm": 0.025398829951882362,
      "learning_rate": 0.00016204444444444443,
      "loss": 4.8044,
      "step": 2570
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.06839799880981445,
      "learning_rate": 0.0001618962962962963,
      "loss": 4.7922,
      "step": 2580
    },
    {
      "epoch": 0.5755555555555556,
      "grad_norm": 0.46191322803497314,
      "learning_rate": 0.00016174814814814815,
      "loss": 4.8475,
      "step": 2590
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.8350647687911987,
      "learning_rate": 0.00016160000000000002,
      "loss": 4.7586,
      "step": 2600
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.16785189509391785,
      "learning_rate": 0.00016145185185185184,
      "loss": 4.7913,
      "step": 2610
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 0.08069221675395966,
      "learning_rate": 0.00016130370370370372,
      "loss": 4.7924,
      "step": 2620
    },
    {
      "epoch": 0.5844444444444444,
      "grad_norm": 0.021457675844430923,
      "learning_rate": 0.00016115555555555556,
      "loss": 4.7934,
      "step": 2630
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.12207910418510437,
      "learning_rate": 0.0001610074074074074,
      "loss": 4.7906,
      "step": 2640
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.03473467007279396,
      "learning_rate": 0.00016085925925925926,
      "loss": 4.7768,
      "step": 2650
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 0.595558762550354,
      "learning_rate": 0.00016071111111111113,
      "loss": 4.8214,
      "step": 2660
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.051868222653865814,
      "learning_rate": 0.00016056296296296298,
      "loss": 4.8332,
      "step": 2670
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.12981002032756805,
      "learning_rate": 0.00016041481481481482,
      "loss": 4.7715,
      "step": 2680
    },
    {
      "epoch": 0.5977777777777777,
      "grad_norm": 0.05930769443511963,
      "learning_rate": 0.00016026666666666667,
      "loss": 4.7894,
      "step": 2690
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01983875408768654,
      "learning_rate": 0.00016011851851851854,
      "loss": 4.7918,
      "step": 2700
    },
    {
      "epoch": 0.6022222222222222,
      "grad_norm": 0.04878874495625496,
      "learning_rate": 0.00015997037037037036,
      "loss": 4.8316,
      "step": 2710
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.31241539120674133,
      "learning_rate": 0.00015982222222222223,
      "loss": 4.7602,
      "step": 2720
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.02190900221467018,
      "learning_rate": 0.00015967407407407408,
      "loss": 4.7977,
      "step": 2730
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.025188762694597244,
      "learning_rate": 0.00015952592592592593,
      "loss": 4.8047,
      "step": 2740
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.03448328748345375,
      "learning_rate": 0.00015937777777777777,
      "loss": 4.7763,
      "step": 2750
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.09709175676107407,
      "learning_rate": 0.00015922962962962965,
      "loss": 4.8538,
      "step": 2760
    },
    {
      "epoch": 0.6155555555555555,
      "grad_norm": 0.08132929354906082,
      "learning_rate": 0.0001590814814814815,
      "loss": 4.8268,
      "step": 2770
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 0.05931726470589638,
      "learning_rate": 0.00015893333333333334,
      "loss": 4.8351,
      "step": 2780
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.3755083084106445,
      "learning_rate": 0.00015878518518518518,
      "loss": 4.7895,
      "step": 2790
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.025575334206223488,
      "learning_rate": 0.00015863703703703706,
      "loss": 4.7844,
      "step": 2800
    },
    {
      "epoch": 0.6244444444444445,
      "grad_norm": 0.025471709668636322,
      "learning_rate": 0.00015848888888888888,
      "loss": 4.7852,
      "step": 2810
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.01796501688659191,
      "learning_rate": 0.00015834074074074075,
      "loss": 4.7849,
      "step": 2820
    },
    {
      "epoch": 0.6288888888888889,
      "grad_norm": 0.048570871353149414,
      "learning_rate": 0.0001581925925925926,
      "loss": 4.79,
      "step": 2830
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.021736647933721542,
      "learning_rate": 0.00015804444444444447,
      "loss": 4.7862,
      "step": 2840
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.027525577694177628,
      "learning_rate": 0.0001578962962962963,
      "loss": 4.8399,
      "step": 2850
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 0.05499672144651413,
      "learning_rate": 0.00015774814814814816,
      "loss": 4.7586,
      "step": 2860
    },
    {
      "epoch": 0.6377777777777778,
      "grad_norm": 0.04042093828320503,
      "learning_rate": 0.0001576,
      "loss": 4.7781,
      "step": 2870
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.06387072801589966,
      "learning_rate": 0.00015745185185185186,
      "loss": 4.7031,
      "step": 2880
    },
    {
      "epoch": 0.6422222222222222,
      "grad_norm": 0.04832005500793457,
      "learning_rate": 0.0001573037037037037,
      "loss": 4.8716,
      "step": 2890
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.2567991316318512,
      "learning_rate": 0.00015715555555555557,
      "loss": 4.7927,
      "step": 2900
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.02953645959496498,
      "learning_rate": 0.00015700740740740742,
      "loss": 4.8001,
      "step": 2910
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.04758251830935478,
      "learning_rate": 0.00015685925925925927,
      "loss": 4.7915,
      "step": 2920
    },
    {
      "epoch": 0.6511111111111111,
      "grad_norm": 0.12686872482299805,
      "learning_rate": 0.0001567111111111111,
      "loss": 4.8512,
      "step": 2930
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.02003282867372036,
      "learning_rate": 0.000156562962962963,
      "loss": 4.8142,
      "step": 2940
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.05091472715139389,
      "learning_rate": 0.0001564148148148148,
      "loss": 4.8068,
      "step": 2950
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.08855864405632019,
      "learning_rate": 0.00015626666666666668,
      "loss": 4.8325,
      "step": 2960
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.04448270425200462,
      "learning_rate": 0.00015611851851851853,
      "loss": 4.7891,
      "step": 2970
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.04784185066819191,
      "learning_rate": 0.0001559703703703704,
      "loss": 4.8306,
      "step": 2980
    },
    {
      "epoch": 0.6644444444444444,
      "grad_norm": 0.1011221781373024,
      "learning_rate": 0.00015582222222222222,
      "loss": 4.7458,
      "step": 2990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.04285053908824921,
      "learning_rate": 0.0001556740740740741,
      "loss": 4.7933,
      "step": 3000
    },
    {
      "epoch": 0.6688888888888889,
      "grad_norm": 0.13265873491764069,
      "learning_rate": 0.00015552592592592594,
      "loss": 4.8112,
      "step": 3010
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 0.028287548571825027,
      "learning_rate": 0.00015537777777777778,
      "loss": 4.8242,
      "step": 3020
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.03308146819472313,
      "learning_rate": 0.00015522962962962963,
      "loss": 4.8608,
      "step": 3030
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.05597485229372978,
      "learning_rate": 0.0001550814814814815,
      "loss": 4.8075,
      "step": 3040
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.06473412364721298,
      "learning_rate": 0.00015493333333333332,
      "loss": 4.8239,
      "step": 3050
    },
    {
      "epoch": 0.68,
      "grad_norm": 5.453907489776611,
      "learning_rate": 0.0001547851851851852,
      "loss": 4.9119,
      "step": 3060
    },
    {
      "epoch": 0.6822222222222222,
      "grad_norm": 0.09795532375574112,
      "learning_rate": 0.00015463703703703704,
      "loss": 4.8935,
      "step": 3070
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.19963125884532928,
      "learning_rate": 0.00015448888888888892,
      "loss": 4.842,
      "step": 3080
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.48394325375556946,
      "learning_rate": 0.00015434074074074073,
      "loss": 4.9079,
      "step": 3090
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.2704543173313141,
      "learning_rate": 0.0001541925925925926,
      "loss": 4.8056,
      "step": 3100
    },
    {
      "epoch": 0.6911111111111111,
      "grad_norm": 0.24303196370601654,
      "learning_rate": 0.00015404444444444445,
      "loss": 4.8226,
      "step": 3110
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.05319428816437721,
      "learning_rate": 0.0001538962962962963,
      "loss": 4.8039,
      "step": 3120
    },
    {
      "epoch": 0.6955555555555556,
      "grad_norm": 0.03242554888129234,
      "learning_rate": 0.00015374814814814815,
      "loss": 4.8396,
      "step": 3130
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 0.03477591648697853,
      "learning_rate": 0.00015360000000000002,
      "loss": 4.7811,
      "step": 3140
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04443289712071419,
      "learning_rate": 0.00015345185185185187,
      "loss": 4.8457,
      "step": 3150
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 22.163921356201172,
      "learning_rate": 0.0001533037037037037,
      "loss": 4.7535,
      "step": 3160
    },
    {
      "epoch": 0.7044444444444444,
      "grad_norm": 0.028610048815608025,
      "learning_rate": 0.00015315555555555556,
      "loss": 4.8491,
      "step": 3170
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.03832579404115677,
      "learning_rate": 0.00015300740740740743,
      "loss": 4.7938,
      "step": 3180
    },
    {
      "epoch": 0.7088888888888889,
      "grad_norm": 0.05353081598877907,
      "learning_rate": 0.00015285925925925925,
      "loss": 4.7668,
      "step": 3190
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.17786644399166107,
      "learning_rate": 0.00015271111111111112,
      "loss": 4.7995,
      "step": 3200
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.13153599202632904,
      "learning_rate": 0.00015256296296296297,
      "loss": 4.8011,
      "step": 3210
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.04630366712808609,
      "learning_rate": 0.00015241481481481484,
      "loss": 4.8315,
      "step": 3220
    },
    {
      "epoch": 0.7177777777777777,
      "grad_norm": 0.180622860789299,
      "learning_rate": 0.00015226666666666666,
      "loss": 4.7944,
      "step": 3230
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.14740556478500366,
      "learning_rate": 0.00015211851851851854,
      "loss": 4.7645,
      "step": 3240
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.0676250234246254,
      "learning_rate": 0.00015197037037037038,
      "loss": 4.8171,
      "step": 3250
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 1.061899185180664,
      "learning_rate": 0.00015182222222222223,
      "loss": 4.8499,
      "step": 3260
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.03788484260439873,
      "learning_rate": 0.00015167407407407408,
      "loss": 4.892,
      "step": 3270
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.022214988246560097,
      "learning_rate": 0.00015152592592592595,
      "loss": 4.7369,
      "step": 3280
    },
    {
      "epoch": 0.7311111111111112,
      "grad_norm": 0.07346963882446289,
      "learning_rate": 0.0001513777777777778,
      "loss": 4.8326,
      "step": 3290
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.030612295493483543,
      "learning_rate": 0.00015122962962962964,
      "loss": 4.837,
      "step": 3300
    },
    {
      "epoch": 0.7355555555555555,
      "grad_norm": 0.0907873660326004,
      "learning_rate": 0.0001510814814814815,
      "loss": 4.738,
      "step": 3310
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.057164162397384644,
      "learning_rate": 0.00015093333333333336,
      "loss": 4.807,
      "step": 3320
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.7134498953819275,
      "learning_rate": 0.00015078518518518518,
      "loss": 4.7882,
      "step": 3330
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.08121614903211594,
      "learning_rate": 0.00015063703703703705,
      "loss": 4.7829,
      "step": 3340
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.05506924167275429,
      "learning_rate": 0.0001504888888888889,
      "loss": 4.7671,
      "step": 3350
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.03904391825199127,
      "learning_rate": 0.00015034074074074075,
      "loss": 4.8073,
      "step": 3360
    },
    {
      "epoch": 0.7488888888888889,
      "grad_norm": 0.06783798336982727,
      "learning_rate": 0.0001501925925925926,
      "loss": 4.8331,
      "step": 3370
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.06048039346933365,
      "learning_rate": 0.00015004444444444447,
      "loss": 4.7749,
      "step": 3380
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06031946465373039,
      "learning_rate": 0.0001498962962962963,
      "loss": 4.8174,
      "step": 3390
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.05767936632037163,
      "learning_rate": 0.00014974814814814816,
      "loss": 4.7826,
      "step": 3400
    },
    {
      "epoch": 0.7577777777777778,
      "grad_norm": 0.06567662209272385,
      "learning_rate": 0.0001496,
      "loss": 4.7699,
      "step": 3410
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.12990343570709229,
      "learning_rate": 0.00014945185185185188,
      "loss": 4.8346,
      "step": 3420
    },
    {
      "epoch": 0.7622222222222222,
      "grad_norm": 0.5535753965377808,
      "learning_rate": 0.0001493037037037037,
      "loss": 4.7643,
      "step": 3430
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.12236610800027847,
      "learning_rate": 0.00014915555555555557,
      "loss": 4.8038,
      "step": 3440
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.47755059599876404,
      "learning_rate": 0.00014900740740740742,
      "loss": 4.7985,
      "step": 3450
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.11376135796308517,
      "learning_rate": 0.00014885925925925926,
      "loss": 4.7938,
      "step": 3460
    },
    {
      "epoch": 0.7711111111111111,
      "grad_norm": 0.22976437211036682,
      "learning_rate": 0.0001487111111111111,
      "loss": 4.8302,
      "step": 3470
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.07534478604793549,
      "learning_rate": 0.00014856296296296298,
      "loss": 4.8528,
      "step": 3480
    },
    {
      "epoch": 0.7755555555555556,
      "grad_norm": 3.7993428707122803,
      "learning_rate": 0.00014841481481481483,
      "loss": 5.0331,
      "step": 3490
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.21897488832473755,
      "learning_rate": 0.00014826666666666667,
      "loss": 4.849,
      "step": 3500
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.0291218757629395,
      "learning_rate": 0.00014811851851851852,
      "loss": 4.8051,
      "step": 3510
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 1.901645302772522,
      "learning_rate": 0.0001479703703703704,
      "loss": 4.8031,
      "step": 3520
    },
    {
      "epoch": 0.7844444444444445,
      "grad_norm": 0.045075349509716034,
      "learning_rate": 0.00014782222222222224,
      "loss": 4.8492,
      "step": 3530
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.15294213593006134,
      "learning_rate": 0.0001476740740740741,
      "loss": 4.8198,
      "step": 3540
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.16552050411701202,
      "learning_rate": 0.00014752592592592593,
      "loss": 4.7689,
      "step": 3550
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.5107570290565491,
      "learning_rate": 0.00014737777777777778,
      "loss": 4.7896,
      "step": 3560
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.7163199782371521,
      "learning_rate": 0.00014722962962962963,
      "loss": 4.8114,
      "step": 3570
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.037423014640808105,
      "learning_rate": 0.0001470814814814815,
      "loss": 4.7893,
      "step": 3580
    },
    {
      "epoch": 0.7977777777777778,
      "grad_norm": 0.08286458998918533,
      "learning_rate": 0.00014693333333333335,
      "loss": 4.8119,
      "step": 3590
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.04053176939487457,
      "learning_rate": 0.0001467851851851852,
      "loss": 4.7717,
      "step": 3600
    },
    {
      "epoch": 0.8022222222222222,
      "grad_norm": 0.39993616938591003,
      "learning_rate": 0.00014663703703703704,
      "loss": 4.8255,
      "step": 3610
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.05636029690504074,
      "learning_rate": 0.0001464888888888889,
      "loss": 4.8224,
      "step": 3620
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.03807814419269562,
      "learning_rate": 0.00014634074074074076,
      "loss": 4.76,
      "step": 3630
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.04478556662797928,
      "learning_rate": 0.0001461925925925926,
      "loss": 4.7873,
      "step": 3640
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.036233775317668915,
      "learning_rate": 0.00014604444444444445,
      "loss": 4.7974,
      "step": 3650
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.03000890649855137,
      "learning_rate": 0.0001458962962962963,
      "loss": 4.7535,
      "step": 3660
    },
    {
      "epoch": 0.8155555555555556,
      "grad_norm": 0.02313845418393612,
      "learning_rate": 0.00014574814814814814,
      "loss": 4.8096,
      "step": 3670
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.04449488967657089,
      "learning_rate": 0.00014560000000000002,
      "loss": 4.7773,
      "step": 3680
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.029727432876825333,
      "learning_rate": 0.00014545185185185186,
      "loss": 4.8153,
      "step": 3690
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.02713426947593689,
      "learning_rate": 0.0001453037037037037,
      "loss": 4.7919,
      "step": 3700
    },
    {
      "epoch": 0.8244444444444444,
      "grad_norm": 0.029725555330514908,
      "learning_rate": 0.00014515555555555555,
      "loss": 4.7962,
      "step": 3710
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.1477232426404953,
      "learning_rate": 0.00014500740740740743,
      "loss": 4.8185,
      "step": 3720
    },
    {
      "epoch": 0.8288888888888889,
      "grad_norm": 0.04521794989705086,
      "learning_rate": 0.00014485925925925927,
      "loss": 4.8099,
      "step": 3730
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.02922739088535309,
      "learning_rate": 0.00014471111111111112,
      "loss": 4.7805,
      "step": 3740
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04356227442622185,
      "learning_rate": 0.00014456296296296297,
      "loss": 4.7764,
      "step": 3750
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.032142411917448044,
      "learning_rate": 0.0001444148148148148,
      "loss": 4.8059,
      "step": 3760
    },
    {
      "epoch": 0.8377777777777777,
      "grad_norm": 0.13950306177139282,
      "learning_rate": 0.00014426666666666669,
      "loss": 4.753,
      "step": 3770
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.042966004461050034,
      "learning_rate": 0.00014411851851851853,
      "loss": 4.8356,
      "step": 3780
    },
    {
      "epoch": 0.8422222222222222,
      "grad_norm": 0.04302078112959862,
      "learning_rate": 0.00014397037037037038,
      "loss": 4.776,
      "step": 3790
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.13220183551311493,
      "learning_rate": 0.00014382222222222222,
      "loss": 4.7825,
      "step": 3800
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.07489722222089767,
      "learning_rate": 0.00014367407407407407,
      "loss": 4.7864,
      "step": 3810
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.03115803748369217,
      "learning_rate": 0.00014352592592592592,
      "loss": 4.8118,
      "step": 3820
    },
    {
      "epoch": 0.8511111111111112,
      "grad_norm": 0.03724142909049988,
      "learning_rate": 0.0001433777777777778,
      "loss": 4.7593,
      "step": 3830
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.09584537148475647,
      "learning_rate": 0.00014322962962962964,
      "loss": 4.7389,
      "step": 3840
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.10730807483196259,
      "learning_rate": 0.00014308148148148148,
      "loss": 4.7845,
      "step": 3850
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 0.5785430669784546,
      "learning_rate": 0.00014293333333333333,
      "loss": 4.8164,
      "step": 3860
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.2936428189277649,
      "learning_rate": 0.0001427851851851852,
      "loss": 4.8085,
      "step": 3870
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.03274869546294212,
      "learning_rate": 0.00014263703703703705,
      "loss": 4.796,
      "step": 3880
    },
    {
      "epoch": 0.8644444444444445,
      "grad_norm": 0.031509190797805786,
      "learning_rate": 0.0001424888888888889,
      "loss": 4.8138,
      "step": 3890
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.0474981851875782,
      "learning_rate": 0.00014234074074074074,
      "loss": 4.8202,
      "step": 3900
    },
    {
      "epoch": 0.8688888888888889,
      "grad_norm": 0.02456037513911724,
      "learning_rate": 0.0001421925925925926,
      "loss": 4.7615,
      "step": 3910
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.05099480226635933,
      "learning_rate": 0.00014204444444444443,
      "loss": 4.7777,
      "step": 3920
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.06268300861120224,
      "learning_rate": 0.0001418962962962963,
      "loss": 4.8179,
      "step": 3930
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 0.05342372879385948,
      "learning_rate": 0.00014174814814814815,
      "loss": 4.8284,
      "step": 3940
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.1927410066127777,
      "learning_rate": 0.0001416,
      "loss": 4.785,
      "step": 3950
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.02663915976881981,
      "learning_rate": 0.00014145185185185185,
      "loss": 4.814,
      "step": 3960
    },
    {
      "epoch": 0.8822222222222222,
      "grad_norm": 0.3225458860397339,
      "learning_rate": 0.00014130370370370372,
      "loss": 4.7597,
      "step": 3970
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.09178091585636139,
      "learning_rate": 0.00014115555555555557,
      "loss": 4.7666,
      "step": 3980
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.02773629128932953,
      "learning_rate": 0.0001410074074074074,
      "loss": 4.8184,
      "step": 3990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.4953054189682007,
      "learning_rate": 0.00014085925925925926,
      "loss": 4.7947,
      "step": 4000
    },
    {
      "epoch": 0.8911111111111111,
      "grad_norm": 0.050142280757427216,
      "learning_rate": 0.00014071111111111113,
      "loss": 4.7638,
      "step": 4010
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.10711908340454102,
      "learning_rate": 0.00014056296296296295,
      "loss": 4.7475,
      "step": 4020
    },
    {
      "epoch": 0.8955555555555555,
      "grad_norm": 0.07102419435977936,
      "learning_rate": 0.00014041481481481482,
      "loss": 4.8006,
      "step": 4030
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.024793591350317,
      "learning_rate": 0.00014026666666666667,
      "loss": 4.8652,
      "step": 4040
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.03574278578162193,
      "learning_rate": 0.00014011851851851852,
      "loss": 4.7674,
      "step": 4050
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 0.046670813113451004,
      "learning_rate": 0.00013997037037037036,
      "loss": 4.8236,
      "step": 4060
    },
    {
      "epoch": 0.9044444444444445,
      "grad_norm": 0.05954406037926674,
      "learning_rate": 0.00013982222222222224,
      "loss": 4.8388,
      "step": 4070
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.08055663853883743,
      "learning_rate": 0.00013967407407407408,
      "loss": 4.8023,
      "step": 4080
    },
    {
      "epoch": 0.9088888888888889,
      "grad_norm": 0.046899646520614624,
      "learning_rate": 0.00013952592592592593,
      "loss": 4.7932,
      "step": 4090
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.0761643722653389,
      "learning_rate": 0.00013937777777777777,
      "loss": 4.7828,
      "step": 4100
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.06197596713900566,
      "learning_rate": 0.00013922962962962965,
      "loss": 4.7864,
      "step": 4110
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.055811867117881775,
      "learning_rate": 0.00013908148148148147,
      "loss": 4.7674,
      "step": 4120
    },
    {
      "epoch": 0.9177777777777778,
      "grad_norm": 0.04831898584961891,
      "learning_rate": 0.00013893333333333334,
      "loss": 4.8014,
      "step": 4130
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.07409906387329102,
      "learning_rate": 0.0001387851851851852,
      "loss": 4.7763,
      "step": 4140
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.32686784863471985,
      "learning_rate": 0.00013863703703703706,
      "loss": 4.7619,
      "step": 4150
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.023378504440188408,
      "learning_rate": 0.00013848888888888888,
      "loss": 4.8268,
      "step": 4160
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.062322117388248444,
      "learning_rate": 0.00013834074074074075,
      "loss": 4.8068,
      "step": 4170
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.0839676484465599,
      "learning_rate": 0.0001381925925925926,
      "loss": 4.7955,
      "step": 4180
    },
    {
      "epoch": 0.9311111111111111,
      "grad_norm": 0.04062415659427643,
      "learning_rate": 0.00013804444444444444,
      "loss": 4.7626,
      "step": 4190
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04688778519630432,
      "learning_rate": 0.0001378962962962963,
      "loss": 4.8049,
      "step": 4200
    },
    {
      "epoch": 0.9355555555555556,
      "grad_norm": 0.24838438630104065,
      "learning_rate": 0.00013774814814814816,
      "loss": 4.7733,
      "step": 4210
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 0.07659061998128891,
      "learning_rate": 0.00013759999999999998,
      "loss": 4.8165,
      "step": 4220
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.07646184414625168,
      "learning_rate": 0.00013745185185185186,
      "loss": 4.8201,
      "step": 4230
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.2703281044960022,
      "learning_rate": 0.0001373037037037037,
      "loss": 4.7717,
      "step": 4240
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.06751704216003418,
      "learning_rate": 0.00013715555555555558,
      "loss": 4.7736,
      "step": 4250
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.04046843945980072,
      "learning_rate": 0.0001370074074074074,
      "loss": 4.8091,
      "step": 4260
    },
    {
      "epoch": 0.9488888888888889,
      "grad_norm": 0.16692186892032623,
      "learning_rate": 0.00013685925925925927,
      "loss": 4.732,
      "step": 4270
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.04203793779015541,
      "learning_rate": 0.00013671111111111112,
      "loss": 4.81,
      "step": 4280
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.02649099938571453,
      "learning_rate": 0.00013656296296296296,
      "loss": 4.7595,
      "step": 4290
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.04465778172016144,
      "learning_rate": 0.0001364148148148148,
      "loss": 4.7939,
      "step": 4300
    },
    {
      "epoch": 0.9577777777777777,
      "grad_norm": 0.13342531025409698,
      "learning_rate": 0.00013626666666666668,
      "loss": 4.8107,
      "step": 4310
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06086326390504837,
      "learning_rate": 0.00013611851851851853,
      "loss": 4.7589,
      "step": 4320
    },
    {
      "epoch": 0.9622222222222222,
      "grad_norm": 0.046362631022930145,
      "learning_rate": 0.00013597037037037037,
      "loss": 4.7834,
      "step": 4330
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 0.16470949351787567,
      "learning_rate": 0.00013582222222222222,
      "loss": 4.732,
      "step": 4340
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.019804544746875763,
      "learning_rate": 0.0001356740740740741,
      "loss": 4.8275,
      "step": 4350
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.02984030917286873,
      "learning_rate": 0.0001355259259259259,
      "loss": 4.8063,
      "step": 4360
    },
    {
      "epoch": 0.9711111111111111,
      "grad_norm": 0.194355770945549,
      "learning_rate": 0.00013537777777777779,
      "loss": 4.8323,
      "step": 4370
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.10787688195705414,
      "learning_rate": 0.00013522962962962963,
      "loss": 4.7858,
      "step": 4380
    },
    {
      "epoch": 0.9755555555555555,
      "grad_norm": 0.017161691561341286,
      "learning_rate": 0.0001350814814814815,
      "loss": 4.7597,
      "step": 4390
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.023063194006681442,
      "learning_rate": 0.00013493333333333332,
      "loss": 4.7589,
      "step": 4400
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05855024605989456,
      "learning_rate": 0.0001347851851851852,
      "loss": 4.7853,
      "step": 4410
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.07282644510269165,
      "learning_rate": 0.00013463703703703704,
      "loss": 4.7669,
      "step": 4420
    },
    {
      "epoch": 0.9844444444444445,
      "grad_norm": 0.02776503935456276,
      "learning_rate": 0.0001344888888888889,
      "loss": 4.7556,
      "step": 4430
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08908132463693619,
      "learning_rate": 0.00013434074074074074,
      "loss": 4.7721,
      "step": 4440
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.01535737980157137,
      "learning_rate": 0.0001341925925925926,
      "loss": 4.7564,
      "step": 4450
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.023407433182001114,
      "learning_rate": 0.00013404444444444446,
      "loss": 4.7565,
      "step": 4460
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.05652699992060661,
      "learning_rate": 0.0001338962962962963,
      "loss": 4.7552,
      "step": 4470
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.021533146500587463,
      "learning_rate": 0.00013374814814814815,
      "loss": 4.8032,
      "step": 4480
    },
    {
      "epoch": 0.9977777777777778,
      "grad_norm": 0.08728024363517761,
      "learning_rate": 0.00013360000000000002,
      "loss": 4.813,
      "step": 4490
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04888789355754852,
      "learning_rate": 0.00013345185185185184,
      "loss": 4.8142,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 13500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9550655479808e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
